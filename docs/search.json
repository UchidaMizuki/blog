[
  {
    "objectID": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html",
    "href": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html",
    "title": "Julia JuMPで効用最大化問題を解いてみた（コンストラクタを使って）",
    "section": "",
    "text": "過去の記事で，Juliaの数理最適化ライブラリのJuMPを使って効用最大化問題を解く方法を紹介しました．\n過去の記事では，簡単な事例紹介としてコブ・ダグラス型効用関数を使った効用最大化問題を解く方法を紹介しましたが， 経済モデルでは，コブ・ダグラス型効用関数以外にも，CES型効用関数などの様々な効用関数が使われます． しかし，過去の記事で用いた実装方法では，効用関数を変更するたびに最適化問題の実装を変更する必要がありました．\nそこで，この記事では，Juliaのコンストラクタを使って， 同一のコードで異なる効用関数に対する効用最大化問題を解く方法を紹介します．"
  },
  {
    "objectID": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#コブダグラス型効用関数の実装",
    "href": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#コブダグラス型効用関数の実装",
    "title": "Julia JuMPで効用最大化問題を解いてみた（コンストラクタを使って）",
    "section": "コブ・ダグラス型効用関数の実装",
    "text": "コブ・ダグラス型効用関数の実装\n効用関数は，消費量ベクトルを引数として受け取り，効用（スカラー値）を返す関数と捉えることができます． 一方で，効用関数を設定する際には，消費量ベクトル以外に弾力性パラメータ等のパラメータを事前に設定する必要があります．\nパラメータは効用関数によって異なるため，効用関数を抽象型AbstractEconomicUtilityとして定義し，そのサブタイプとしてコブ・ダグラス型効用関数CobbDouglasUtilityの型を定義します． CobbDouglasUtilityは，弾力性パラメータweightsを持つ型として定義します．\n\nabstract type AbstractEconomicUtility end\n\nstruct CobbDouglasUtility &lt;: AbstractEconomicUtility\n    weights::Vector{Float64}\nend\n\n次に，function-like objectsを定義することで，効用関数の型fに対してf(quantities)と呼ぶことで効用を計算できるようにします． 効用関数のパラメータは，型fから取得することができるため，関数の引数がquantitiesのみとなっていることに注意してください． これにより，様々な効用関数の型fに対してf(quantities)のような同一のコードを呼び出すことができるようになります．\n以上により，コブ・ダグラス型効用関数を定義することができます．\n\nfunction(f::CobbDouglasUtility)(quantities)\n    return prod(quantities .^ f.weights)\nend\n\n\n# 効用関数の定義\ncobb_douglas = CobbDouglasUtility([0.3, 0.4, 0.3])\n\n# 効用の算出\ncobb_douglas([2., 3., 5.])\n\n3.0963389922845703\n\n\nさらに，過去の記事と同様にマーシャルの需要関数demand_marshallian()を定義します．\ndemand_marshallian()により，コブ・ダグラス型効用関数の効用最大化問題の解析的な解を求めることができます．\n\nfunction demand_marshallian(\n  f::CobbDouglasUtility;\n  prices,\n  income\n)\n  return income * f.weights / sum(f.weights) ./ prices\nend\n\ndemand_marshallian (generic function with 1 method)\n\n\n\nquantities_analytical_cobb_douglas = demand_marshallian(\n  cobb_douglas;\n  prices = [1., 2., 3.],\n  income = 100.\n)\n\n3-element Vector{Float64}:\n 30.0\n 20.0\n 10.0"
  },
  {
    "objectID": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#効用最大化問題の数値的な解",
    "href": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#効用最大化問題の数値的な解",
    "title": "Julia JuMPで効用最大化問題を解いてみた（コンストラクタを使って）",
    "section": "効用最大化問題の数値的な解",
    "text": "効用最大化問題の数値的な解\n過去の記事のコードを少し変更することで，効用最大化問題を数値的に解くことができます．\n過去の記事では，コブ・ダグラス型効用関数専用のコードを定義していましたが， 以下の関数demand_marshallian_numericalは，抽象型AbstractEconomicUtilityを引数に取ることで， 様々な効用関数に対して同一のコードを使うことができます．\n\nimport Pkg\nPkg.add(\"JuMP\")\nPkg.add(\"Ipopt\")\n\n\nusing JuMP\nusing Ipopt\n\nfunction demand_marshallian_numerical(\n  f::AbstractEconomicUtility;\n  prices::Vector{Float64},\n  income::Float64\n)\n    n = length(prices)\n    model = Model(Ipopt.Optimizer) \n    set_silent(model)\n    @variable(model, quantities[1:n] &gt;= 0)\n    @objective(model, Max, f(quantities))\n    @constraint(model, sum(prices .* quantities) &lt;= income)\n    optimize!(model)\n    return value.(quantities)\nend\n\ndemand_marshallian_numerical (generic function with 1 method)\n\n\n上で定義したcobb_douglasに対してdemand_marshallian_numerical()を適用することで， 過去の記事と同様に，コブ・ダグラス型効用関数の効用最大化問題の解析的な解と数値的な解がおおよそ一致することを確認できます．\n\nquantities_numerical_cobb_douglas = demand_marshallian_numerical(\n  cobb_douglas;\n  prices = [1., 2., 3.],\n  income = 100.\n)\n\n3-element Vector{Float64}:\n 30.000000297265977\n 20.00000019590169\n 10.000000099089569\n\n\n\nquantities_analytical_cobb_douglas\n\n3-element Vector{Float64}:\n 30.0\n 20.0\n 10.0"
  },
  {
    "objectID": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#ces型効用関数の実装",
    "href": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#ces型効用関数の実装",
    "title": "Julia JuMPで効用最大化問題を解いてみた（コンストラクタを使って）",
    "section": "CES型効用関数の実装",
    "text": "CES型効用関数の実装\nコンストラクタを使うメリットを実感するために， コブ・ダグラス型効用関数以外の効用関数を実装してみます．\n以下では，CES型効用関数を実装してみましょう． CES型効用関数は，コブ・ダグラス型効用関数の一般化であり，代替性の程度を表すパラメータsubstitutionを持ちます． コブ・ダグラス型効用関数と同様に，CES型効用関数は，以下のように定義することができます．\nマーシャルの需要関数の導出は割愛しますが引数の型f::CESUtilityを指定することで， 効用関数に応じてdemand_marshallian()の結果を変化させることができます． このような仕組みは多重ディスパッチと呼ばれています．\n\nstruct CESUtility &lt;: AbstractEconomicUtility\n    substitution::Float64\n    weights::Vector{Float64}\nend\n\nfunction(f::CESUtility)(quantities)\n    return sum(f.weights .* quantities .^ f.substitution) ^ (1 / f.substitution)\nend\n\nfunction demand_marshallian(\n  f::CESUtility;\n  prices,\n  income\n)\n  return f.weights .^ (1 / (1 - f.substitution)) .* prices .^ (1 / (f.substitution - 1)) *\n    income / sum(f.weights .^ (1 / (1 - f.substitution)) .* prices .^ (f.substitution / (f.substitution - 1)))\nend\n\ndemand_marshallian (generic function with 2 methods)\n\n\n\nces = CESUtility(0.5, [0.3, 0.4, 0.3])\nces([2., 3., 5.])\n\n3.196603520188051\n\n\n上で定義したcesに対してdemand_marshallian()とdemand_marshallian_numerical()を適用することで， CES型効用関数の効用最大化問題の解析的な解と数値的な解を求めることができます．\n\nquantities_analytical_ces = demand_marshallian(\n  ces;\n  prices = [1., 2., 3.],\n  income = 100.\n)\n\nquantities_numerical_ces = demand_marshallian_numerical(\n  ces;\n  prices = [1., 2., 3.],\n  income = 100.\n)\n\n3-element Vector{Float64}:\n 45.00000043558223\n 20.000000194987294\n  5.000000053970372\n\n\n\nquantities_analytical_ces\n\n3-element Vector{Float64}:\n 45.0\n 20.000000000000004\n  4.999999999999999"
  },
  {
    "objectID": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#まとめ",
    "href": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#まとめ",
    "title": "Julia JuMPで効用最大化問題を解いてみた（コンストラクタを使って）",
    "section": "まとめ",
    "text": "まとめ\nこの記事では，過去の記事で紹介した効用最大化問題をJuliaのコンストラクタを用いて拡張することで，様々な効用関数に対して同一のコードを使うことができることを示しました．\nこのような仕組みを活用することで効用関数を実装した際に， 自動的に最適化問題の解析的な解と数値的な解の整合性を確認することができ， 効用関数に対するテストを実装するのが容易になることが期待されます1．"
  },
  {
    "objectID": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#footnotes",
    "href": "posts/2025/04/solving-utility-maximisation-problems-with-julia-jump-constructors.html#footnotes",
    "title": "Julia JuMPで効用最大化問題を解いてみた（コンストラクタを使って）",
    "section": "脚注",
    "text": "脚注\n\n\nただし，別で検討したレオンチェフ型効用関数ではうまく数値的な解を求めることができませんでした．特殊な効用関数では，数値的な解を求めることが難しい可能性があるため，別途，テストを実装する必要があるかもしれません．↩︎"
  },
  {
    "objectID": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html",
    "href": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html",
    "title": "Julia JuMPで効用最大化問題を解いてみた（簡易的な事例紹介）",
    "section": "",
    "text": "この記事では，数理最適化ライブラリであるJulia JuMPを用いて， 経済学でよく用いられるコブ・ダグラス型効用関数を用いた効用最大化問題を 解いてみます．\nコブ・ダグラス型効用関数の効用最大化は解析的に解くことができるため， 必ずしもJulia JuMPを使って解く必要はありませんが， ここでは，Julia JuMPの使い方を学ぶために，あえて解析的な解と数値的な解を 比較してみました．"
  },
  {
    "objectID": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#コブダグラス型効用関数",
    "href": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#コブダグラス型効用関数",
    "title": "Julia JuMPで効用最大化問題を解いてみた（簡易的な事例紹介）",
    "section": "コブ・ダグラス型効用関数",
    "text": "コブ・ダグラス型効用関数\nコブ・ダグラス型効用関数は以下のように定義されます． ここで，\\(x_i\\)は生産要素\\(i\\)の消費量， \\(\\alpha_i\\)は生産要素\\(i\\)の弾力性パラメータです．\n\\[\nU = \\prod_{i=1}^{n} x_i^{\\alpha_i}\n\\]\nこの効用関数をJuliaで実装すると以下のようになります．\n\nfunction cobb_douglas(quantities; weights)\n    return prod(quantities .^ weights)\nend\n\ncobb_douglas (generic function with 1 method)\n\n\n実装したcobb_douglas関数に，消費量quantitiesと弾力性パラメータweightsを 渡すことで，以下のように，効用を計算することができます．\n\ncobb_douglas(\n  [2, 3, 5], \n  weights=[0.3, 0.4, 0.3]\n)\n\n3.0963389922845703"
  },
  {
    "objectID": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#効用最大化問題の解析的な解",
    "href": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#効用最大化問題の解析的な解",
    "title": "Julia JuMPで効用最大化問題を解いてみた（簡易的な事例紹介）",
    "section": "効用最大化問題の解析的な解",
    "text": "効用最大化問題の解析的な解\n一般的に，効用最大化問題は，価格と所得が与えられたときに， 効用を最大化するような消費量を求める問題として定式化されます．\nこうした消費量（需要量）は，マーシャルの需要関数として知られています． コブ・ダグラス型効用関数の場合，マーシャルの需要関数は以下のように導出されます1． ここで，\\(p_i\\)は生産要素\\(i\\)の価格，\\(Y\\)は所得です．\n\\[\nx_i = \\frac{1}{p_i}\\frac{w_i}{\\sum_{j=1}^{n} w_j}Y\n\\]\n\nfunction demand_marshallian_cobb_douglas_analytical(prices, income; weights)\n    return income .* weights ./ sum(weights) ./ prices\nend\n\ndemand_marshallian_cobb_douglas_analytical (generic function with 1 method)\n\n\n実装したdemand_marshallian_cobb_douglas_analytical関数に，価格pricesと 所得income，弾力性パラメータweightsを渡すことで，以下のように，需要量を 計算することができます．\n\nquantities_analytical = demand_marshallian_cobb_douglas_analytical(\n  [1, 2, 3], \n  100, \n  weights=[0.3, 0.4, 0.3]\n)\n\n3-element Vector{Float64}:\n 30.0\n 20.0\n 10.0"
  },
  {
    "objectID": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#効用最大化問題の数値的な解",
    "href": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#効用最大化問題の数値的な解",
    "title": "Julia JuMPで効用最大化問題を解いてみた（簡易的な事例紹介）",
    "section": "効用最大化問題の数値的な解",
    "text": "効用最大化問題の数値的な解\n次に，Julia JuMPを用いて，効用最大化問題を数値的に解いてみます． 事前に，JuMPパッケージとIpoptパッケージをインストールしておきましょう． Ipoptは，非線形最適化問題を解くためのソルバーを提供しています．\n\nimport Pkg\nPkg.add(\"JuMP\")\nPkg.add(\"Ipopt\")\n\nコブ・ダグラス型効用関数を用いた効用最大化問題は以下のように定式化されます． JuMPでは，すでに定義したcobb_douglas関数を用いて， 簡単に，効用最大化問題を記述し，数値的に解くことができます．\n\\[\n\\begin{aligned}\n  \\text{maximize} && U = \\prod_{i=1}^{n} x_i^{\\alpha_i} \\\\\n  \\text{subject to} && \\sum_{i=1}^{n} p_i x_i \\leq Y \\\\\n\\end{aligned}\n\\]\n\nusing JuMP\nusing Ipopt\n\nfunction demand_marshallian_cobb_douglas_numerical(prices, income; weights)\n    n = length(prices)\n    model = Model(Ipopt.Optimizer) \n    set_silent(model)\n    @variable(model, quantities[1:n] &gt;= 0) # 消費量を変数として定義\n    @objective(model, Max, cobb_douglas(quantities; weights)) # 効用最大化\n    @constraint(model, sum(prices .* quantities) &lt;= income) # 予算制約式\n    optimize!(model)\n    return value.(quantities)\nend\n\ndemand_marshallian_cobb_douglas_numerical (generic function with 1 method)\n\n\n効用最大化問題の定式化では，以下のことを行っています．\n\n@variableマクロを用いて，消費量quantitiesを変数として定義\n@objectiveマクロを用いて，効用を最大化するような目的関数を定義\n@constraintマクロを用いて，予算制約式を定義\n\nそれでは，実装したdemand_marshallian_cobb_douglas_numerical関数を用いて， 数値的な解を求めてみましょう．\n\nquantities_numerical = demand_marshallian_cobb_douglas_numerical(\n  [1, 2, 3], \n  100, \n  weights=[0.3, 0.4, 0.3]\n)\n\n\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n\n\n3-element Vector{Float64}:\n 30.000000297265977\n 20.00000019590169\n 10.000000099089569\n\n\n最終的な結果は以下のようになり， 数値的な解が解析的な解とほぼ一致していることがわかります．\n\nquantities_numerical\n\n3-element Vector{Float64}:\n 30.000000297265977\n 20.00000019590169\n 10.000000099089569\n\n\n\nquantities_analytical\n\n3-element Vector{Float64}:\n 30.0\n 20.0\n 10.0"
  },
  {
    "objectID": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#まとめ",
    "href": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#まとめ",
    "title": "Julia JuMPで効用最大化問題を解いてみた（簡易的な事例紹介）",
    "section": "まとめ",
    "text": "まとめ\nこの記事では，Julia JuMPを用いて，コブ・ダグラス型効用関数を用いた 効用最大化問題を解いてみました．\nJulia JuMPを用いることで，非線形最適化問題を簡単に定式化し， 数値的に解くことができることを確認しました．\nJulia JuMPでは，シンプルな関数であれば，定義済みの関数をそのまま目的変数等として 扱うことができるため，経済モデルの実装にも有用かもしれません．"
  },
  {
    "objectID": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#footnotes",
    "href": "posts/2025/02/solving-utility-maximisation-problems-with-julia-jump.html#footnotes",
    "title": "Julia JuMPで効用最大化問題を解いてみた（簡易的な事例紹介）",
    "section": "脚注",
    "text": "脚注\n\n\n具体的な解法は割愛しますが，以下の条件から導出できます．\n\n効用最大化条件: \\(MRS_{ij}=\\frac{\\partial U/\\partial x_i}{\\partial U/\\partial x_j}=\\frac{p_i}{p_j}\\)\n予算制約式: \\(\\sum_{i=1}^{n} p_i x_i = Y\\)\n\n↩︎"
  },
  {
    "objectID": "posts/2024/05/visualizing-municipal-mergers-in-japan-with-r-ggraph.html",
    "href": "posts/2024/05/visualizing-municipal-mergers-in-japan-with-r-ggraph.html",
    "title": "R ggraphで全国の市町村合併を可視化する",
    "section": "",
    "text": "昨日，こちらの記事でjpcityパッケージを紹介しました． jpcityパッケージでは，内部的に過去の全国の市町村合併（廃置分合）をネットワーク化することで，異なる時点間での市区町村コードの対応付けを行っています．\nこの記事では，jpcityパッケージが内部で使用する市町村合併ネットワークを以下の方針で可視化しみます．\n\n1970年4月1日～2024年4月1日にかけての市町村合併や市・町制施行を都道府県ごとに可視化\n並び替えにおいて時系列は考慮しない（位置を時点に合わせることも可能だが見えづらいため）\n区の分離・分割は対象外\n\n\n参考記事\n\nRで市区町村データをスマートに整形しよう（市町村合併や政令指定都市の集計処理など）\n\n\n\n都道府県別の市町村合併の可視化結果\n以下に市町村合併の可視化結果を示します．データ上の不備等に気づかれた方はコメント等でお知らせいただければ幸いです． ラベルが重なって見づらい箇所があることをご了承ください．\n\n\nコード\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\n\ndate_end &lt;- ymd(\"2024-04-01\")\n\ngraph_city &lt;- jpcity:::graph_city$graph_city |&gt; \n  \n  activate(nodes) |&gt; \n  filter(!node_is_isolated(),\n         is.infinite(interval) | int_end(interval) &lt;= date_end) |&gt; \n  arrange(desc(interval)) |&gt;\n  \n  replace_na(list(city_desig_name = \"\",\n                  city_name = \"\")) |&gt;\n  mutate(city_name = str_c(city_desig_name, city_name)) |&gt;\n  select(!city_desig_name) |&gt; \n  \n  mutate(group = group_components()) |&gt; \n  \n  activate(edges) |&gt;\n  filter(!type %in% c(\"分離\", \"分割\"))\n\njpcity::parse_pref(1:47) |&gt; \n  walk(\\(pref) {\n    pref_name &lt;- jpcity::pref_name(pref)\n    \n    plot &lt;- graph_city |&gt; \n      activate(nodes) |&gt; \n      filter(pref_name == .env$pref_name) |&gt; \n      \n      create_layout(\"fabric\") |&gt;\n      \n      # ラベル位置を時系列に合わせることも可能だが見えづらいため省略\n      # mutate(y = int_start(interval)) |&gt;\n      \n      ggraph() +\n      geom_edge_diagonal(color = \"gray\",\n                         arrow = arrow(length = unit(2, 'mm')), \n                         end_cap = circle(8, 'mm')) +\n      geom_node_label(aes(label = city_name,\n                          fill = as_factor(group)),\n                      size = 3,\n                      show.legend = FALSE) +\n      scale_fill_hue(l = 100) +\n      coord_flip() +\n      \n      labs(title = pref_name) +\n      theme_void()\n    \n    print(plot)\n  })"
  },
  {
    "objectID": "posts/2024/03/normal-distribution-with-mean-following-a-normal-distribution.html",
    "href": "posts/2024/03/normal-distribution-with-mean-following-a-normal-distribution.html",
    "title": "平均が正規分布に従う正規分布の周辺分布を求める",
    "section": "",
    "text": "平均\\(\\theta\\)・分散\\(\\tau^2\\)の正規分布に従う確率変数\\(Y\\)があり，かつ\\(\\theta\\)が平均\\(\\mu\\)・分散\\(\\sigma^2\\)の正規分布に従うとします．\n\\[\n\\begin{align}\nY \\mid \\theta \\sim \\mathcal{N} ( \\theta, \\tau^2 ) &\\iff\np( y \\mid \\theta ) = \\frac{1}{\\sqrt{2 \\pi} \\tau}\n\\exp \\left[\n{-\\frac{1}{2}} \\left(\n\\frac{y - \\theta}{\\tau}\n\\right) ^2\n\\right] \\\\\n\\theta \\sim \\mathcal{N} (\\mu, \\sigma^2) &\\iff\np( \\theta ) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}\n\\exp \\left[\n{-\\frac{1}{2}} \\left(\n\\frac{\\theta - \\mu}{\\sigma}\n\\right) ^2\n\\right]\n\\end{align}\n\\]\nこのとき，\\(Y\\)の周辺分布は，平均\\(\\mu\\)・分散\\(\\sigma^2 + \\tau^2\\)の正規分布に従うことが知られています． この記事では，この性質を導出する過程を備忘録として記載します（実際にはもっとスマートな導出方法があるかもしれません）．\n\\[\nY \\sim \\mathcal{N} \\left( \\mu, \\sigma^2 + \\tau^2 \\right)\n\\]\n\\(Y\\)の周辺確率密度関数\\(p(y)\\)は以下のような積分で求められます．平方完成を用いることで，積分の対象となる\\(p( y \\mid \\theta ) p ( \\theta )\\)を以下のように変形できます．\n\\[\n\\begin{align}\np(y)\n&= \\int_{-\\infty}^{\\infty}\np( y, \\theta )\nd\\theta \\\\\n&= \\int_{-\\infty}^{\\infty}\np( y \\mid \\theta ) p ( \\theta )\nd\\theta \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\np( y \\mid \\theta ) p ( \\theta )\n&= \\frac{1}{2 \\pi \\tau \\sigma}\n\\exp \\left[\n-\\frac{1}{2}\n\\frac{\n\\sigma^2 \\left( y - \\theta \\right)^2 +\n\\tau^2 \\left( \\theta - \\mu \\right)^2\n}{\\tau^2 \\sigma^2}\n\\right] \\\\\n&= \\frac{1}{2 \\pi \\tau \\sigma}\n\\exp \\left[\n-\\frac{1}{2}\n\\frac{\n\\left( \\sigma^2 + \\tau^2 \\right) \\theta^2 -\n2 \\left( \\sigma^2 y + \\tau^2 \\mu \\right) \\theta +\n\\left( \\sigma^2 y^2 + \\tau^2 \\mu^2 \\right)\n}{\\tau^2 \\sigma^2}\n\\right] \\\\\n&= \\frac{1}{2 \\pi \\tau \\sigma}\n\\exp \\left[\n-\\frac{1}{2} \\left(\n\\frac{\\sqrt{\\sigma^2 + \\tau^2}}{\\tau \\sigma} \\theta -\n\\frac{\\sigma^2 y + \\tau^2 \\mu}{\\tau \\sigma \\sqrt{\\sigma^2 + \\tau^2}}\n\\right)^2 -\n\\frac{1}{2} \\frac{\\left( y - \\mu \\right)^2}{\\sigma^2 + \\tau^2}\n\\right] \\quad \\left( \\because \\thetaに対する平方完成 \\right) \\\\\n&= \\frac{1}{2 \\pi \\tau \\sigma}\n\\exp \\left[\n-\\frac{1}{2} \\left(\n\\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\tau^2}}\n\\right)^2\n\\right]\n\\exp \\left[\n-\\frac{1}{2} \\left(\n\\frac{\\theta - \\frac{\\sigma^2 y + \\tau^2 \\mu}{\\sigma^2 + \\tau^2}}{\\frac{\\tau \\sigma}{\\sqrt{\\sigma^2 + \\tau^2}}}\n\\right)^2\n\\right] \\\\\n\\end{align}\n\\]\n\\(p( y \\mid \\theta ) p ( \\theta )\\)を積分すると，\\(\\theta\\)に関する正規分布のカーネルの積分が現れます．そのため，積分を正規化係数の逆数で置き換えることができます．最終的に，\\(Y\\)の周辺確率密度関数\\(p(y)\\)が，平均\\(\\mu\\)・分散\\(\\sigma^2 + \\tau^2\\)の正規分布に従っていることが導出されます．\n\\[\n\\begin{align}\np(y)\n&= \\int_{-\\infty}^{\\infty}\np( y \\mid \\theta ) p ( \\theta )\nd\\theta \\\\\n&= \\frac{1}{2 \\pi \\tau \\sigma}\n\\exp \\left[\n-\\frac{1}{2} \\left(\n\\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\tau^2}}\n\\right)^2\n\\right]\n\\int_{-\\infty}^{\\infty}\n\\exp \\left[\n-\\frac{1}{2} \\left(\n\\frac{\\theta - \\frac{\\sigma^2 y + \\tau^2 \\mu}{\\sigma^2 + \\tau^2}}{\\frac{\\tau \\sigma}{\\sqrt{\\sigma^2 + \\tau^2}}}\n\\right)^2\n\\right]\nd\\theta \\\\\n&= \\frac{1}{2 \\pi \\tau \\sigma}\n\\exp \\left[\n-\\frac{1}{2} \\left(\n\\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\tau^2}}\n\\right)^2\n\\right]\n\\sqrt{2 \\pi} \\frac{\\tau \\sigma}{\\sqrt{\\sigma^2 + \\tau^2}}\n\\quad \\left( \\because 分散 \\frac{\\tau^2 \\sigma^2}{\\sigma^2 + \\tau^2} の正規分布のカーネルの積分 \\right) \\\\\n&= \\frac{1}{\\sqrt{2 \\pi} \\sqrt{\\sigma^2 + \\tau^2}}\n\\exp \\left[\n-\\frac{1}{2} \\left(\n\\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\tau^2}}\n\\right)^2\n\\right] \\iff Y \\sim \\mathcal{N} \\left( \\mu, \\sigma^2 + \\tau^2 \\right)\\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2024/02/scraping-dynamic-sites-with-rvest.html",
    "href": "posts/2024/02/scraping-dynamic-sites-with-rvest.html",
    "title": "rvestで動的サイトをスクレイピングする（Seleniumを使わずに）",
    "section": "",
    "text": "RにおけるWebスクレイピングの定番パッケージにrvestがありますが，これまでブラウザ上の操作によってコンテンツが変化する動的サイトのスクレイピングにはrvestを用いることができませんでした．そのため，Rで動的サイトのスクレイピングには，RSeleniumなどの他のパッケージと組み合わせて利用する必要があり，以下のような課題が生じていました．\n\nSeleniumを用いる場合には，事前にドライバをダウンロードする必要があるなど環境構築が面倒\n他のパッケージで取得したHTMLに対してrvestの関数をシームレスに適用できない\n\nしかし，rvest 1.0.4では，read_html_live()という新たな関数が追加され，動的サイトのスクレイピングが可能となりました．read_html_live()を用いることで，$click()や$type()などのメソッドを用いたブラウザ上の操作の自動化が可能となるだけでなく，html_elements()やhtml_attr()などの一般的なrvestの関数をシームレスに呼ぶことができるようになります．\nread_html_live()は，Google Chromeの自動化を行うchromoteというパッケージを利用しています．そのため，read_html_live()を使うには，事前にGoogle Chrome（ブラウザ）とchromote（Rパッケージ）をインストールしておく必要があります．"
  },
  {
    "objectID": "posts/2024/02/scraping-dynamic-sites-with-rvest.html#rにおけるwebスクレイピングのこれまで",
    "href": "posts/2024/02/scraping-dynamic-sites-with-rvest.html#rにおけるwebスクレイピングのこれまで",
    "title": "rvestで動的サイトをスクレイピングする（Seleniumを使わずに）",
    "section": "",
    "text": "RにおけるWebスクレイピングの定番パッケージにrvestがありますが，これまでブラウザ上の操作によってコンテンツが変化する動的サイトのスクレイピングにはrvestを用いることができませんでした．そのため，Rで動的サイトのスクレイピングには，RSeleniumなどの他のパッケージと組み合わせて利用する必要があり，以下のような課題が生じていました．\n\nSeleniumを用いる場合には，事前にドライバをダウンロードする必要があるなど環境構築が面倒\n他のパッケージで取得したHTMLに対してrvestの関数をシームレスに適用できない\n\nしかし，rvest 1.0.4では，read_html_live()という新たな関数が追加され，動的サイトのスクレイピングが可能となりました．read_html_live()を用いることで，$click()や$type()などのメソッドを用いたブラウザ上の操作の自動化が可能となるだけでなく，html_elements()やhtml_attr()などの一般的なrvestの関数をシームレスに呼ぶことができるようになります．\nread_html_live()は，Google Chromeの自動化を行うchromoteというパッケージを利用しています．そのため，read_html_live()を使うには，事前にGoogle Chrome（ブラウザ）とchromote（Rパッケージ）をインストールしておく必要があります．"
  },
  {
    "objectID": "posts/2024/02/scraping-dynamic-sites-with-rvest.html#read_html_liveを使ってみよう",
    "href": "posts/2024/02/scraping-dynamic-sites-with-rvest.html#read_html_liveを使ってみよう",
    "title": "rvestで動的サイトをスクレイピングする（Seleniumを使わずに）",
    "section": "read_html_live()を使ってみよう",
    "text": "read_html_live()を使ってみよう\nここからは，こちらのRSeleniumのチュートリアルで紹介されているものと同じ処理をread_html_live()で行ってみましょう．\nこちらのチュートリアルでは，こちらのサイトにアメリカの郵便番号（ZIP code）を入力して地元テレビ局の情報を取得するという一連の処理を自動化しています． read_html_live()を使えば，RSeleniumでのサイトのアクセスを以下のように書き換えられます．\n# RSelenium\n# Source: https://joshuamccrain.com/tutorials/web_scraping_R_selenium.html\nlibrary(RSelenium)\n\nrD &lt;- rsDriver(browser=\"firefox\", port=4545L, verbose=F)\nremDr &lt;- rD[[\"client\"]]\nremDr$navigate(\"https://www.fcc.gov/media/engineering/dtvmaps\")\n⏬\n\n# rvest\nlibrary(rvest)\nlibrary(tidyverse)\n\nhtml &lt;- read_html_live(\"https://www.fcc.gov/media/engineering/dtvmaps\")\n\n読み込まれたオブジェクトは，$view()でブラウザで確認することができます．サイト上のエレメントを選択（Ctrl+Shift+C）したのち，該当箇所を右クリック⏩Copy⏩Copy selectorでCSSセレクタをコピーすれば，$type()や$click()の引数として使うことができます．\n# rvest\nhtml$view()\n\n次に，中央のフォームに郵便番号（ZIP code）を入力⏩Go!ボタンをクリックし地元テレビ局の情報を表示させるコードは以下のように書き換えられます．\n# RSelenium\n# Source: https://joshuamccrain.com/tutorials/web_scraping_R_selenium.html\nzip &lt;- \"30308\"\nremDr$findElement(using = \"id\", value = \"startpoint\")$sendKeysToElement(list(zip))\nremDr$findElements(\"id\", \"btnSub\")[[1]]$clickElement()\n⏬\n\n# rvest\nzip &lt;- \"30308\"\nhtml$type(\"#startpoint\", zip)\nhtml$click(\"#btnSub\")\n\n最後に，上記のRSeleniumのチュートリアルと同じデータが取得できたことを確認しましょう．\n\n# rvest\nhtml |&gt; \n  html_elements(\"table.tbl_mapReception\") |&gt; \n  insistently(chuck)(3) |&gt; \n  html_table() |&gt; \n  select(!c(1, IA)) |&gt; \n  rename_with(str_to_lower) |&gt; \n  rename(ch_num = `ch#`) |&gt; \n  slice_tail(n = -1) |&gt; \n  filter(callsign != \"\")"
  },
  {
    "objectID": "posts/2024/02/scraping-dynamic-sites-with-rvest.html#まとめ",
    "href": "posts/2024/02/scraping-dynamic-sites-with-rvest.html#まとめ",
    "title": "rvestで動的サイトをスクレイピングする（Seleniumを使わずに）",
    "section": "まとめ",
    "text": "まとめ\n以上のようにrvest 1.0.4で追加されたread_html_live()を使うことで，rvestだけでシームレスに静的サイトと動的サイトのスクレイピングが可能となるだけでなく，RSeleniumと比べてシンプルなコードでブラウザ上の操作を再現することができることがわかりました．\nRには，rvestの他にもseleniderというWebスクレイピング用のパッケージも開発されているようです．こういったパッケージの開発が進むことで，RでのWebスクレイピングがさらに便利になることが期待されます．"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm.html",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm.html",
    "title": "Rのdmパッケージでデータ前処理の質を高めよう",
    "section": "",
    "text": "データ分析に要する時間のうち，8割は前処理に費やされているといわれています．前処理は，その後のデータ分析の質を左右しますから非常に重要な一方で，前処理に膨大な時間を要するということは，作業ミスが起こる確率もそれだけ高くなるということを意味しています．\n一般的な（データ構造に対する）前処理には，データフレームの”抽出”・”集約”・”結合”がありますが，なかでも”結合”は，コードが長くなりやすく，作業ミスが起こりやすい作業であると思われます．また，分析で必要となるデータフレームが1つにまとまっていることは稀ですから，データフレームの結合は特に頻出する処理でもあります．\nデータフレームの”結合”で起こりがちな典型的なミスとして，以下のようなものがあります1．\n\n結合対象となるデータフレームのキーが”MECE”（「漏れなく・ダブりなく」）でない\n結合のためのキーを取り違える，または，データ形式が異なる\n\nもしも，これまでデータフレームの結合において，何らかの作業ミスや”ヒヤリ・ハット”を経験したことがあるのであれば，それらを放置せず何らかのパッケージに頼るほうが得策かもしれません．また，仮に作業ミスがなかったとしても，私たちは過去の作業が正しかったかについて疑心暗鬼になりがちです2．\nこの記事では，データフレームの”結合”における作業ミスや疑心暗鬼の解決策として有力なRのdmパッケージを紹介します．"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm.html#データ前処理の作業ミスをなくすために",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm.html#データ前処理の作業ミスをなくすために",
    "title": "Rのdmパッケージでデータ前処理の質を高めよう",
    "section": "",
    "text": "データ分析に要する時間のうち，8割は前処理に費やされているといわれています．前処理は，その後のデータ分析の質を左右しますから非常に重要な一方で，前処理に膨大な時間を要するということは，作業ミスが起こる確率もそれだけ高くなるということを意味しています．\n一般的な（データ構造に対する）前処理には，データフレームの”抽出”・”集約”・”結合”がありますが，なかでも”結合”は，コードが長くなりやすく，作業ミスが起こりやすい作業であると思われます．また，分析で必要となるデータフレームが1つにまとまっていることは稀ですから，データフレームの結合は特に頻出する処理でもあります．\nデータフレームの”結合”で起こりがちな典型的なミスとして，以下のようなものがあります1．\n\n結合対象となるデータフレームのキーが”MECE”（「漏れなく・ダブりなく」）でない\n結合のためのキーを取り違える，または，データ形式が異なる\n\nもしも，これまでデータフレームの結合において，何らかの作業ミスや”ヒヤリ・ハット”を経験したことがあるのであれば，それらを放置せず何らかのパッケージに頼るほうが得策かもしれません．また，仮に作業ミスがなかったとしても，私たちは過去の作業が正しかったかについて疑心暗鬼になりがちです2．\nこの記事では，データフレームの”結合”における作業ミスや疑心暗鬼の解決策として有力なRのdmパッケージを紹介します．"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm.html#dmによるリレーショナルデータモデル",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm.html#dmによるリレーショナルデータモデル",
    "title": "Rのdmパッケージでデータ前処理の質を高めよう",
    "section": "dmによるリレーショナルデータモデル",
    "text": "dmによるリレーショナルデータモデル\ndmの提供するリレーショナルデータモデルは，複数のデータ間の関係性を私たちの代わりに管理してくれます．\nこの記事では，repurrrsiveパッケージで提供されているStar Warsのデータセットを通じて，dmの提供するリレーショナルデータモデルの使い方や利点について説明します．あらかじめ，tidyverse，dm，repurrrsiveの3つのパッケージを読み込んでおきます．\n\nlibrary(tidyverse)\nlibrary(dm)\nlibrary(repurrrsive)\n\nrepurrrsiveで提供されているStar Warsのデータセットには，以下のようなものが含まれています3．\n\n\nrepurrrsiveで提供されているStar Warsのデータセット\ndata(package = \"repurrrsive\") |&gt; \n  chuck(\"results\") |&gt; \n  as_tibble() |&gt; \n  filter(str_starts(Item, \"sw_\")) |&gt; \n  pull(Item)\n\n\n[1] \"sw_films\"     \"sw_people\"    \"sw_planets\"   \"sw_species\"   \"sw_starships\"\n[6] \"sw_vehicles\" \n\n\n複数のデータ間の関係を扱う必要のある，以下の2つの分析事例を通じて，リレーショナルデータモデルを活用する方法を見ていきましょう．\n\n基礎編：dmでリレーショナルデータモデルを構築する\n応用編：dmでモデルを拡張したりキーを検証したりする\n\n\n1. 基礎編：dmでリレーショナルデータモデルを構築する\n基礎編として，Star Wars作品の登場人物の故郷の惑星の構成比を調べるためのリレーショナルデータモデルを構築してみしょう．この分析を行うために，以下のように3つのデータフレームfilms・people・planetsを準備しておきます4．ここでは，分析をシンプルにするため，必要なデータのみをselectしました5．\n\nfilms &lt;- tibble(film = sw_films) |&gt; \n  unnest_wider(film) |&gt; \n  select(url, title, characters)\npeople &lt;- tibble(person = sw_people) |&gt;\n  unnest_wider(person) |&gt; \n  select(url, name, homeworld, species)\nplanets &lt;- tibble(planet = sw_planets) |&gt;\n  unnest_wider(planet) |&gt; \n  select(url, name)\n\n\nfilms\n\n\n  \n\n\npeople\n\n\n  \n\n\nplanets\n\n\n  \n\n\n\n準備したデータフレームを確認すると，各データフレームのurl列がキーとして用いられていることがわかりますので，データフレーム間の関係は以下の図のようにまとめることができます6．特に，1つの映画作品には複数の登場人物が登場することが一般的ですので，filmsのcharacters列が登場人物のリストになっていることに注意が必要です．そのため，このままでは，filmsのcharacters列をpeopleのurl列と対応付けることができません．\n\n\n\n\n\nflowchart TB\n  films.characters --&gt; people.url\n  people.homeworld --&gt; planets.url\n  subgraph films\n    films.url[url]\n    films.title[title]\n    films.characters[List of characters] \n  end\n  subgraph people\n    people.url[url]\n    people.name[name]\n    people.homeworld[homeworld]\n  end\n  subgraph planets\n    planets.url[url]\n    planets.name[name]\n  end\n\n\n\n\n\n\nそこで，filmsとpeopleの関係，すなわち，どの作品にどの登場人物が登場するかを表すデータfilms_x_charactersを新たに作成することを考えます7．films_x_charactersを介すことで，データ間の関係を以下の図のようにまとめることができます．\n\n\n\n\n\nflowchart TB\n  films_x_characters.url --&gt; films.url\n  films_x_characters.characters ---&gt; people.url\n  people.homeworld --&gt; planets.url\n  subgraph films_x_characters\n    films_x_characters.url[url]\n    films_x_characters.characters[characters]\n  end\n  subgraph films\n    films.url[url]\n    films.title[title]\n  end\n  subgraph people\n    people.url[url]\n    people.name[name]\n    people.homeworld[homeworld]\n  end\n  subgraph planets\n    planets.url[url]\n    planets.name[name]\n  end\n\n\n\n\n\n\nそれでは，上のイメージに従って，実際にリレーショナルデータモデルを構築してみましょう．まず，filmsのurl・characters列を使ってfilms_x_charactersを作成します．ついでにfilmsから不要となったcharacters列を削除しておきます．\n\n# Create films_x_characters and remove characters column from films\nfilms_x_characters &lt;- films |&gt; \n  select(url, characters) |&gt; \n  unnest_longer(characters)\nfilms &lt;- films |&gt; \n  select(!characters)\n\nfilms_x_characters\n\n\n  \n\n\n\n最後に，dm()に準備したfilms・people・planets・films_x_charactersを渡した後，主キー（primary keys）と外部キー（foreign keys）を追加することで，リレーショナルデータモデルを構築することができます．\ndmでは，主キーをdm_add_pk()で8，外部キーをdm_add_fk()で設定します9．\n\ndm_starwars_1 &lt;- dm(films, people, planets, films_x_characters) |&gt; \n  \n  # 1. Add primary keys\n  dm_add_pk(films, url) |&gt;\n  dm_add_pk(people, url) |&gt;\n  dm_add_pk(planets, url) |&gt;\n  dm_add_pk(films_x_characters, c(url, characters)) |&gt;\n  \n  # 2. Add foreign keys\n  dm_add_fk(films_x_characters, url, films) |&gt; \n  dm_add_fk(films_x_characters, characters, people) |&gt;\n  dm_add_fk(people, homeworld, planets) \n\ndm_starwars_1\n\n── Metadata ────────────────────────────────────────────────────────────────────\nTables: `films`, `people`, `planets`, `films_x_characters`\nColumns: 10\nPrimary keys: 4\nForeign keys: 3\n\n\ndm_draw()を用いて，リレーショナルデータモデルを描画することもできます．描画してみると，上のイメージと同様の関係が構築されていることがわかります．\n\ndm_draw(dm_starwars_1)\n\n\n\n\n\n以下のようにdm_flatten_to_tbl()を用いることで，films_x_charactersデータにfilms・people・planetsデータを結合したデータフレームを作成することができます10．この際，異なるデータ間で同名の列名が存在する場合には，データ名に応じて自動的に列名が変更されます．このように，リレーショナルデータモデルが私たちの代わりにデータ間の関係を管理してくれるおかげで，他のデータ間との関係に基づいて自動的にデータフレームを結合することができます．\n\ndata_films_x_characters_1 &lt;- dm_starwars_1 |&gt; \n  dm_flatten_to_tbl(films_x_characters,\n                    .recursive = TRUE) \n\nRenaming ambiguous columns: %&gt;%\n  dm_rename(people, name.people = name) %&gt;%\n  dm_rename(planets, name.planets = name)\n\ndata_films_x_characters_1\n\n\n  \n\n\n\n作成したdata_films_x_characters_1を使うことで，以下のように，登場人物の故郷の惑星の構成比をグラフにすることができます． このグラフへの考察はひとまず措くとして，リレーショナルデータモデルを用いることでデータフレームの結合を自動化できることが確認できました．\nしかし，上のような分析であれば，left_join()を用いてデータフレームを結合することも簡単で， あまりリレーショナルデータモデルを用いるメリットが感じられないかもしれません． そこで応用編では，リレーショナルデータモデルが本領を発揮する，より込み入った状況を考えてみます．\n\n\n登場人物の故郷の惑星の構成比のグラフ\ndata_films_x_characters_1 |&gt; \n  mutate(name.planets = fct_lump_n(name.planets, 7,\n                                   ties.method = \"first\") |&gt; \n           fct_relevel(\"Other\", \n                       after = Inf)) |&gt; \n  count(title, name.planets) |&gt; \n  mutate(prop = n / sum(n),\n         .by = title,\n         .keep = \"unused\") |&gt; \n  ggplot(aes(fct_rev(title), prop,\n             fill = name.planets)) +\n  geom_col(position = position_stack(reverse = TRUE)) +\n  geom_text(aes(label = if_else(prop &lt; 5e-2, \n                                \"\",\n                                scales::label_percent(accuracy = 1)(prop))),\n            position = position_stack(vjust = 0.5,\n                                      reverse = TRUE)) +\n  scale_x_discrete(\"作品タイトル\") +\n  scale_y_continuous(\"登場人物の故郷の惑星の構成比\",\n                     labels = scales::percent) +\n  scale_fill_brewer(\"惑星名\",\n                    palette = \"Set2\") +\n  coord_flip() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_legend(nrow = 2,\n                             byrow = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n2. 応用編：dmでモデルを拡張したりキーを検証したりする\n応用編では，Star Wars作品の登場人物の種族の構成比を調べてみます． この分析の難易度は基礎編とさほど変わりませんが，通常，扱うデータが増えるとコードが煩雑化しやすいため，リレーショナルデータモデルを使うメリットが大きくなります． さらに，リレーショナルデータモデルのメリットには，以下のようなものもあります．\n\n既存のリレーショナルデータモデルに新たなデータを追加することが可能\n結合のためのキーの整合性を確認することが可能\n\nあらかじめ，この分析で必要となるspeciesデータを準備しておきます．\n\nspecies &lt;- tibble(species = sw_species) |&gt; \n  unnest_wider(species) |&gt; \n  select(url, name)\n\nspecies\n\n\n  \n\n\n\ndmでは，dm()を用いて，リレーショナルデータモデルに新たにデータを追加することができます． ここでは，dm_starwars_1にspeciesデータを追加して，dm_starwars_2を作成してみましょう． dm_draw()を用いることで，モデルが更新されたことがわかります．\n\ndm_starwars_2 &lt;- dm_starwars_1 |&gt; \n  dm(species) |&gt; \n  dm_add_pk(species, url) |&gt; \n  dm_add_fk(people, species, species)\n\ndm_draw(dm_starwars_2)\n\n\n\n\n\n次に，結合のためのキーの整合性を確認してみましょう．こうした検証は，dm_examine_constraints()を用いることで可能です． ここでは，上でありがちなミスとして挙げた，2種類のミスを含んだモデルを作成して，dm_examine_constraints()の挙動を確認してみましょう． ここで，dm_starwars_2_wrong_dataは，speciesデータの1行目が削除されたデータで，データがMECE（「漏れなく・ダブりなく」）でないものです． また，dm_starwars_2_wrong_pkは，speciesデータの主キーを取り違えたものです．\n\ndm_starwars_2_wrong_data &lt;- dm_starwars_1 |&gt; \n  dm(species = species |&gt; \n       slice(-1)) |&gt; \n  dm_add_pk(species, url) |&gt; \n  dm_add_fk(people, species, species)\n\ndm_starwars_2_wrong_pk &lt;- dm_starwars_1 |&gt; \n  dm(species) |&gt; \n  dm_add_pk(species, name) |&gt; \n  dm_add_fk(people, species, species)\n\ndm_examine_constraints()の結果を見てみましょう． 正しいモデルであるdm_starwars_2では，ℹ All constraints satisfied.というメッセージが表示され，モデルのキーが整合していることがわかります． 一方で，dm_starwars_2_wrong_dataとdm_starwars_2_wrong_pkでは，! Unsatisfied constraints:というメッセージが表示されています． これは，speciesデータの主キーに含まれるはずのデータが含まれていないことに起因します． このように，dm_examine_constraints()を用いることで，簡単にモデルのキーの整合性を確認することができます．\n\nprint(dm_examine_constraints(dm_starwars_2))\n\nℹ All constraints satisfied.\n\nprint(dm_examine_constraints(dm_starwars_2_wrong_data))\n\n! Unsatisfied constraints:\n\n\n• Table `people`: foreign key `species` into table `species`: values of `people$species` not in `species$url`: http://swapi.co/api/species/5/ (1)\n\nprint(dm_examine_constraints(dm_starwars_2_wrong_pk))\n\n! Unsatisfied constraints:\n\n\n• Table `people`: foreign key `species` into table `species`: values of `people$species` not in `species$name`: http://swapi.co/api/species/1/ (35), http://swapi.co/api/species/2/ (5), http://swapi.co/api/species/12/ (3), http://swapi.co/api/species/15/ (2), http://swapi.co/api/species/22/ (2), …\n\n\n以上のように，dm()でリレーショナルデータモデルに新たなデータを追加したり，dm_examine_constraints()で結合のためのキーの整合性を確認したりすることができることがわかりました． 最後に，作成したリレーショナルデータモデルdm_starwars_2を用いて，Star Wars作品の登場人物の種族の構成比をグラフ化したものが以下の図です．ここでも，考察は割愛します．\n\n\n登場人物の種族の構成比のグラフ\ndm_starwars_2 |&gt; \n  dm_flatten_to_tbl(films_x_characters,\n                    .recursive = TRUE) |&gt; \n  mutate(name.species = name.species |&gt; \n           fct_na_value_to_level(\"Other\") |&gt; \n           fct_lump_n(7,\n                      ties.method = \"first\") |&gt; \n           fct_relevel(\"Other\", \n                       after = Inf)) |&gt; \n  count(title, name.species) |&gt; \n  mutate(prop = n / sum(n),\n         .by = title,\n         .keep = \"unused\") |&gt; \n  ggplot(aes(fct_rev(title), prop,\n             fill = name.species)) +\n  geom_col(position = position_stack(reverse = TRUE)) +\n  geom_text(aes(label = if_else(prop &lt; 5e-2, \n                                \"\",\n                                scales::label_percent(accuracy = 1)(prop))),\n            position = position_stack(vjust = 0.5,\n                                      reverse = TRUE)) +\n  scale_x_discrete(\"作品タイトル\") +\n  scale_y_continuous(\"登場人物の種族の構成比\",\n                     labels = scales::percent) +\n  scale_fill_brewer(\"種族名\",\n                    palette = \"Set2\") +\n  coord_flip() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_legend(nrow = 2,\n                             byrow = TRUE))"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm.html#まとめ",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm.html#まとめ",
    "title": "Rのdmパッケージでデータ前処理の質を高めよう",
    "section": "まとめ",
    "text": "まとめ\nこの記事では，dmを使ってリレーショナルデータモデルを構築する方法を紹介しました． dmを使って，ひとたびリレーショナルデータモデルを構築してしまえば， データ間の関係を自ら管理する必要がなくなり，dm_flatten_to_tbl()でデータの結合を自動的に行うことができます． それ以外にも，dmでは，dm()によるモデル拡張やdm_examine_constraints()結合のためのキーの整合性の確認など， データ前処理の質を高めるための便利な機能が提供されています．"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm.html#参考文献",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm.html#参考文献",
    "title": "Rのdmパッケージでデータ前処理の質を高めよう",
    "section": "参考文献",
    "text": "参考文献\n\ndmパッケージのサイト\n前処理大全［データ分析のためのSQL/R/Python実践テクニック］\nstarwarsdb\n\nStar WarsのリレーショナルデータモデルがCRANからダウンロードできます"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm.html#footnotes",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm.html#footnotes",
    "title": "Rのdmパッケージでデータ前処理の質を高めよう",
    "section": "脚注",
    "text": "脚注\n\n\n幸いなことに，最近のdplyrでは，結合先のデータの”ダブり”に対して警告が出るようになりました．↩︎\nその作業を行ったのが過去の自分であっても他の誰かであっても，完全に作業が正しかったという自信は持てないものです．↩︎\nrepurrrsiveのエクスポートするデータのうち，名前がsw_で始まるデータがStar Warsに関するもので，sw_以降の部分がデータの内容を表しています．↩︎\nrepurrrsiveのデータはリスト形式で提供されているため，こちらの記事を参考にデータフレームに変換しました．↩︎\nここでは，sw_peopleのspecies列は不要ですが，次の分析で使用するため選択しておきます．↩︎\nここでは，データフレーム間の関係を図示するために，mermaidを使用しました．↩︎\n作品と登場人物には，明確な上下関係はありませんので，characters_x_filmsという名称でも構いません．↩︎\nfilms・people・planetsについては，url列が主キーとなり，films_x_charactersについては，url・characters列の2列の組合せが主キーとなります．↩︎\n上のイメージの矢印に従って，films_x_charactersのurl・characters列を，それぞれfilms・peopleのurl列に対応付けます．さらに，peopleのhomeworld列をplanetsのurl列に対応付けます．↩︎\ndmでは，各データを”テーブル”と呼びます．↩︎"
  },
  {
    "objectID": "posts/2023/09/presented-at-tokyor-using-quarto.html",
    "href": "posts/2023/09/presented-at-tokyor-using-quarto.html",
    "title": "Quartoを活用してTokyo.Rで発表しました",
    "section": "",
    "text": "第108回R勉強会@東京（#TokyoR）のLTにてe-Stat APIを利用するためのRパッケージであるjpstatパッケージについて紹介しました1．\nLTとは，Lightning Talks（ライトニングトーク）の略で，短時間のプレゼンテーションのことです．Tokyo.Rでは，1人あたりの発表時間は5分でした．\n当日のプレゼンテーション資料はこちらです．"
  },
  {
    "objectID": "posts/2023/09/presented-at-tokyor-using-quarto.html#発表概要",
    "href": "posts/2023/09/presented-at-tokyor-using-quarto.html#発表概要",
    "title": "Quartoを活用してTokyo.Rで発表しました",
    "section": "",
    "text": "第108回R勉強会@東京（#TokyoR）のLTにてe-Stat APIを利用するためのRパッケージであるjpstatパッケージについて紹介しました1．\nLTとは，Lightning Talks（ライトニングトーク）の略で，短時間のプレゼンテーションのことです．Tokyo.Rでは，1人あたりの発表時間は5分でした．\n当日のプレゼンテーション資料はこちらです．"
  },
  {
    "objectID": "posts/2023/09/presented-at-tokyor-using-quarto.html#スライド作成について",
    "href": "posts/2023/09/presented-at-tokyor-using-quarto.html#スライド作成について",
    "title": "Quartoを活用してTokyo.Rで発表しました",
    "section": "スライド作成について",
    "text": "スライド作成について\nこれまでQuartoをGitHubのREADMEやサイト作成などに活用してきましたが，今回，はじめてスライド作成に挑戦してみました．\n以下のページを参考にしながら，自作パッケージのjpstatを紹介するスライドを試しに作ってみたところ，案外，簡単にスライドが作成でき，折角なのでR勉強会で発表してみました．\n\nhttps://quarto.org/docs/presentations/\nhttps://quarto.org/docs/presentations/revealjs/\n\nソースコードはこちらです．Quartoは，こちらのqmd（Quarto Markdown）ファイルで記述されてます．\n以下では，Quartoとreveal.jsによるスライド作成で今回，特に便利と感じた機能を紹介します．\n\n便利機能①：コードのハイライティング機能\n以下のようにcode-line-numbers を指定することでコードの一部の行をハイライトすることができます．\nここでは，code-line-numbers: \"2-3\" で2・3行目をハイライトしています．さらに，code-line-numbers: \"1|2-3\" のように，| で区切ることでハイライト先を推移させることもできるようです．\nハイライト機能は，一部のコードのみに注目を集めたい場合に，非常に便利です．\n```{r}\n#| echo: true\n#| code-line-numbers: \"2-3\"\n\nestat(statsDataId = \"0003343671\") |&gt; \n  activate(cat01) |&gt; \n  filter(str_detect(name, \"チョコレート\"))\n```\n\n\n便利機能②：自動アニメーション・フェードイン機能\n## から始まる各スライドのタイトルの右などに{auto-animate=\"true\"} をつけることで次のページの記述と共通する記述を自動で探し出してアニメーション化してくれます．\n前後のスライドで記述の繰り返しがある場合には，自動アニメーション機能を使うことで文脈のつながりがわかりやすくなるのではないかと感じました．\nまた，::: {.fragment .fade-in} を使用することで，スライドの途中で記述を表示させることもできます．こちらも特定の記述に注目してほしい場合に有用かと思います．\n\n\n便利機能③：タイトル・フッター設定機能\nタイトルスライドや全スライド共通のフッターは，qmdファイルの上部に以下のように記載することで自動でレイアウトされます．\nここでは，タイトル以外にも，サブタイトル・著者・日付も設定していますが，これらのフォントサイズなどを別途指定しなくても大丈夫でした．\nまた，今回はreveal.jsのデフォルトのテーマを使用しましたが，こちらのようにテーマを変更することもできます．\n---\ntitle: \"e-Stat🤝R\"\nsubtitle: \"Tokyo.R #108\"\nauthor: UchidaMizuki\ndate: \"2023-09-02\"\nfooter: &lt;https://github.com/UchidaMizuki/jpstat&gt;\nformat: \n  revealjs\n---"
  },
  {
    "objectID": "posts/2023/09/presented-at-tokyor-using-quarto.html#まとめ",
    "href": "posts/2023/09/presented-at-tokyor-using-quarto.html#まとめ",
    "title": "Quartoを活用してTokyo.Rで発表しました",
    "section": "まとめ",
    "text": "まとめ\nQuartoを使うことでLT発表にちょうどよいプレゼンテーション資料を簡単につくれることが実感できました．\nちなみに，これまでもQuartoで作成したページのGitHub Pagesへの公開を行ってきましたが，今回のreveal.jsのスライドもPublish Commandを利用することで簡単に公開することができました2．"
  },
  {
    "objectID": "posts/2023/09/presented-at-tokyor-using-quarto.html#footnotes",
    "href": "posts/2023/09/presented-at-tokyor-using-quarto.html#footnotes",
    "title": "Quartoを活用してTokyo.Rで発表しました",
    "section": "脚注",
    "text": "脚注\n\n\n第108回Tokyo.Rのイベント申込者はちょうど108人だったようです．↩︎\nGitHub Actionsによる自動更新はうまくいかなったため，今回は，RStudioのTerminalから手動でquarto publish gh-pages を実行しました．↩︎"
  },
  {
    "objectID": "posts/2023/04/why-is-bayes-theorem-important.html",
    "href": "posts/2023/04/why-is-bayes-theorem-important.html",
    "title": "なぜベイズの定理が重要なのかーPCR検査を例にー",
    "section": "",
    "text": "この記事では，コロナ禍で普及したPCR検査を例に，統計学において最も有名な定理の一つであるベイズの定理の重要性について説明したいと思います．\nベイズの定理は， 式 1 のように表すことができます．\n\\[\nP(A|B) \\propto P(B|A) P(A)\n\\tag{1}\\]\n式 1 は，以下のことを示しています．\n\nある結果\\(B\\)が生じた場合に，それが原因\\(A\\)によるものである確率\\(P(A|B)\\)は\nある原因\\(A\\)の生じる確率\\(P(A)\\)とある原因\\(A\\)のもとで結果\\(B\\)が生じる確率\\(P(B|A)\\)の掛け算に比例（\\(\\propto\\)）する\n\nもっと身近なもので例えてみましょう． たとえば，\n\n原因\\(A\\)：新型コロナへの感染の有無\n結果\\(B\\)：PCR検査などでの診断結果\n\nと考えれば， 式 1 を 式 2 のように置き換えることができます．\n\\[\nP(感染の有無|診断結果) \\propto P(診断結果|感染の有無) P(感染の有無)\n\\tag{2}\\]"
  },
  {
    "objectID": "posts/2023/04/why-is-bayes-theorem-important.html#ベイズの定理とは",
    "href": "posts/2023/04/why-is-bayes-theorem-important.html#ベイズの定理とは",
    "title": "なぜベイズの定理が重要なのかーPCR検査を例にー",
    "section": "",
    "text": "この記事では，コロナ禍で普及したPCR検査を例に，統計学において最も有名な定理の一つであるベイズの定理の重要性について説明したいと思います．\nベイズの定理は， 式 1 のように表すことができます．\n\\[\nP(A|B) \\propto P(B|A) P(A)\n\\tag{1}\\]\n式 1 は，以下のことを示しています．\n\nある結果\\(B\\)が生じた場合に，それが原因\\(A\\)によるものである確率\\(P(A|B)\\)は\nある原因\\(A\\)の生じる確率\\(P(A)\\)とある原因\\(A\\)のもとで結果\\(B\\)が生じる確率\\(P(B|A)\\)の掛け算に比例（\\(\\propto\\)）する\n\nもっと身近なもので例えてみましょう． たとえば，\n\n原因\\(A\\)：新型コロナへの感染の有無\n結果\\(B\\)：PCR検査などでの診断結果\n\nと考えれば， 式 1 を 式 2 のように置き換えることができます．\n\\[\nP(感染の有無|診断結果) \\propto P(診断結果|感染の有無) P(感染の有無)\n\\tag{2}\\]"
  },
  {
    "objectID": "posts/2023/04/why-is-bayes-theorem-important.html#なぜベイズの定理が重要なのか",
    "href": "posts/2023/04/why-is-bayes-theorem-important.html#なぜベイズの定理が重要なのか",
    "title": "なぜベイズの定理が重要なのかーPCR検査を例にー",
    "section": "なぜベイズの定理が重要なのか？",
    "text": "なぜベイズの定理が重要なのか？\n式 2 に基づき，ベイズの定理の重要性について考えてみましょう． 実は， 式 2 に示したベイズの定理に出てくる3つの確率は，どれも非常に重要な指標と対応しています（下表参照）． つまり， 式 2 からは，検査の精度（感度・特異度）が高くても，検査前確率が低ければ陽性的中率が低くなることなどがわかります．\n\n\n\n\n\n\n\n確率\n医療分野で対応する指標\n\n\n\n\n\\(P(診断結果|感染の有無)\\)\n検査後確率：統計学では事後確率と呼ばれる\n\n陽性的中率：陽性と診断されたとき実際に感染\n陰性的中率：陰性と診断されたとき実際に未感染\n\n\n\n\\(P(診断結果|感染の有無)\\)\n\n感度：感染者が陽性と診断される確率↔︎偽陰性に関連\n特異度：未感染者が陰性と診断される確率↔︎偽陽性に関連\n\n\n\n\\(P(感染の有無)\\)\n検査前確率：統計学では事前確率と呼ばれる\n\n検査対象者のなかでの感染者の割合\n\n\n\n\nたとえば，以下のような状況を考えてみましょう．\n\n感度・特異度はともに99 %（偽陰性・偽陽性がいずれも1 %の確率で生じる）\n検査前確率は1 %（感染者は100 人中1 人で残りの99 人は未感染者）\n\nこの場合，陽性と診断された場合に感染者である確率\\(P(感染|陽性)\\)と，陽性と診断された場合に未感染者である確率\\(P(未感染|陽性)\\)は， それぞれ 式 3 となります．\n\\[\n\\begin{align}\n  P(感染|陽性) \\propto P(陽性|感染)P(感染) &= 0.99 \\times 0.01 = 0.0099 \\\\\n  P(未感染|陽性) \\propto P(陽性|未感染)P(未感染) &= (1 - 0.99) \\times (1 - 0.01) = 0.0099\n\\end{align}\n\\tag{3}\\]\n式 3 より，\\(P(感染|陽性)\\)と\\(P(未感染|陽性)\\)は等しいことがわかり， 陽性的中率は50 %となります． つまり，陽性と診断された検査対象者のなかで，実際に，感染者である人は2人に1人しかいないことがわかります．\nこのように，ベイズの定理を用いることで 「検査の精度（感度・特異度）が高くても検査前確率が低ければ陽性的中率が低くなってしまう」といった 重要な関係性が明らかとなります．"
  },
  {
    "objectID": "posts/2023/04/why-is-bayes-theorem-important.html#まとめ",
    "href": "posts/2023/04/why-is-bayes-theorem-important.html#まとめ",
    "title": "なぜベイズの定理が重要なのかーPCR検査を例にー",
    "section": "まとめ",
    "text": "まとめ\nコロナ禍で普及したPCR検査を例に，ベイズの定理の重要性について説明しました．\n現実に即した計算事例に興味がある場合は， 統計局の公開しているコラムもご一読されることをおすすめします．"
  },
  {
    "objectID": "posts/2023/02/web-app-for-grid-square-codes-in-japan.html",
    "href": "posts/2023/02/web-app-for-grid-square-codes-in-japan.html",
    "title": "地域メッシュデータのためのWEBアプリをつくりました（R Shiny&jpgrid）",
    "section": "",
    "text": "R Shinyを使って地域メッシュデータを使うためのWEBアプリをつくってみました．\n地域メッシュとは，経度・緯度にもとづいて（日本の）地域をほぼ正方形のメッシュに分割したもので，統計データの集計区分としてよく利用されています．\n今回つくったアプリは，Rパッケージのjpgridパッケージの機能の一部を提供しています． このアプリの提供機能は以下の通りです．\n\n市区町村別の地域メッシュデータ生成\nメッシュ文字列を含む表データから地域メッシュデータ生成\n経度・緯度を含む表データから地域メッシュデータ生成\n\n\n\n\nアプリの外観（クリックするとアプリが開きます）"
  },
  {
    "objectID": "posts/2023/02/web-app-for-grid-square-codes-in-japan.html#このアプリについて",
    "href": "posts/2023/02/web-app-for-grid-square-codes-in-japan.html#このアプリについて",
    "title": "地域メッシュデータのためのWEBアプリをつくりました（R Shiny&jpgrid）",
    "section": "",
    "text": "R Shinyを使って地域メッシュデータを使うためのWEBアプリをつくってみました．\n地域メッシュとは，経度・緯度にもとづいて（日本の）地域をほぼ正方形のメッシュに分割したもので，統計データの集計区分としてよく利用されています．\n今回つくったアプリは，Rパッケージのjpgridパッケージの機能の一部を提供しています． このアプリの提供機能は以下の通りです．\n\n市区町村別の地域メッシュデータ生成\nメッシュ文字列を含む表データから地域メッシュデータ生成\n経度・緯度を含む表データから地域メッシュデータ生成\n\n\n\n\nアプリの外観（クリックするとアプリが開きます）"
  },
  {
    "objectID": "posts/2023/02/web-app-for-grid-square-codes-in-japan.html#アプリの提供機能について",
    "href": "posts/2023/02/web-app-for-grid-square-codes-in-japan.html#アプリの提供機能について",
    "title": "地域メッシュデータのためのWEBアプリをつくりました（R Shiny&jpgrid）",
    "section": "アプリの提供機能について",
    "text": "アプリの提供機能について\n\n市区町村別の地域メッシュデータ生成\n総務省統計局の公開する市区町村別メッシュ・コード一覧から市区町村別のメッシュを取得します．\n以下のような手順で市区町村別のメッシュを生成・保存できます．\n\n都道府県を選択（複数選択可）\n市区町村を選択（複数選択可）\nメッシュサイズ（1 km／10 km／80 kmのいずれか）を選択し「メッシュ表示」を押す\nデータ形式（GeoPackageまたはCSV）を選択し「ダウンロード」を押す\n\n\n\n\n市区町村別メッシュの表示イメージ\n\n\njpgridパッケージでは，grid_city データで市区町村別メッシュデータが提供されています．\n以下のように，市区町村別メッシュデータを図示することができます．\n\nlibrary(jpgrid)\nlibrary(tidyverse)\n\nJGD2011 &lt;- 6668\n\ngrid_city |&gt; \n  filter(city_name_ja %in% c(\"千葉市中央区\", \"千葉市花見川区\", \"千葉市稲毛区\")) |&gt; \n  grid_as_sf(crs = 6668) |&gt; \n  ggplot(aes(fill = city_name_ja)) +\n  geom_sf() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\nメッシュ文字列を含む表データから地域メッシュデータ生成\n以下のような手順でメッシュ文字列を含む表データから地域メッシュを生成・保存できます．\n\nデータを選択（CSVまたはExcel）\nメッシュ文字列の列名を指定（地点IDも指定可能）し「メッシュ表示」を押す\nデータ形式（GeoPackageまたはCSV）を選択し「ダウンロード」を押す\n\njpgridパッケージでは，parse_grid() で文字列から地域メッシュを生成することができます．\n\n\n\n経度・緯度を含む表データから地域メッシュデータ生成\n同様に，以下の手順で経度・緯度を含む表データから地域メッシュを生成・保存できます．\n\nデータを選択（CSVまたはExcel）\n経度（X）・緯度（Y）の列名を指定（地点IDも指定可能）し「メッシュ表示」を押す\nデータ形式（GeoPackageまたはCSV）を選択し「ダウンロード」を押す\n\njpgridパッケージでは，coords_to_grid() で文字列から地域メッシュを生成することができます．"
  },
  {
    "objectID": "posts/2023/02/web-app-for-grid-square-codes-in-japan.html#おわりに",
    "href": "posts/2023/02/web-app-for-grid-square-codes-in-japan.html#おわりに",
    "title": "地域メッシュデータのためのWEBアプリをつくりました（R Shiny&jpgrid）",
    "section": "おわりに",
    "text": "おわりに\nR Shinyで作成した地域メッシュデータのためのWEBアプリについて紹介しました．\nWEBアプリの作成に利用したjpgridパッケージでは，このアプリで提供していない様々な機能が提供されています．詳しくは，こちらをご覧ください．\n例として，ジオメトリをメッシュに変換するgeometry_to_grid() などがあります．\nぜひ地域メッシュデータの分析にjpgridパッケージも活用してみてください．\n\njapan &lt;- rnaturalearth::ne_countries(country = \"japan\",\n                                     scale = \"medium\",\n                                     returnclass = \"sf\")\ngrid_japan &lt;- japan |&gt; \n  geometry_to_grid(\"80km\") |&gt; \n  dplyr::first() |&gt; \n  grid_as_sf(crs = sf::st_crs(japan))\n\njapan |&gt; \n  ggplot() +\n  geom_sf() +\n  geom_sf(data = grid_japan,\n          fill = \"transparent\")"
  },
  {
    "objectID": "posts/2022/12/call-e-stat-api-in-r.html",
    "href": "posts/2022/12/call-e-stat-api-in-r.html",
    "title": "Rで日本の統計データを効率的に取得しよう（e-Stat APIとjpstatパッケージで）",
    "section": "",
    "text": "この記事は「R Advent Calendar 2022」の12日目の記事です．\n昨年，日本で政府統計の整備が始まってから150年を迎えました（平成・令和の統計年表）．最近では，政府統計の総合窓口（e-Stat）で，様々な政府統計データを閲覧・ダウンロードすることができるようになりました．\ne-Statには，便利なAPI機能も提供されています（利用ガイドはこちら．あらかじめ利用規約を確認してください．API機能を利用する際は，事前にユーザ登録を行ってください）．\nこの記事では，Rのjpstatパッケージを使って，e-Stat APIを効率的に用いる方法を紹介します．"
  },
  {
    "objectID": "posts/2022/12/call-e-stat-api-in-r.html#この記事について",
    "href": "posts/2022/12/call-e-stat-api-in-r.html#この記事について",
    "title": "Rで日本の統計データを効率的に取得しよう（e-Stat APIとjpstatパッケージで）",
    "section": "",
    "text": "この記事は「R Advent Calendar 2022」の12日目の記事です．\n昨年，日本で政府統計の整備が始まってから150年を迎えました（平成・令和の統計年表）．最近では，政府統計の総合窓口（e-Stat）で，様々な政府統計データを閲覧・ダウンロードすることができるようになりました．\ne-Statには，便利なAPI機能も提供されています（利用ガイドはこちら．あらかじめ利用規約を確認してください．API機能を利用する際は，事前にユーザ登録を行ってください）．\nこの記事では，Rのjpstatパッケージを使って，e-Stat APIを効率的に用いる方法を紹介します．"
  },
  {
    "objectID": "posts/2022/12/call-e-stat-api-in-r.html#e-statについて",
    "href": "posts/2022/12/call-e-stat-api-in-r.html#e-statについて",
    "title": "Rで日本の統計データを効率的に取得しよう（e-Stat APIとjpstatパッケージで）",
    "section": "e-Statについて",
    "text": "e-Statについて\ne-Statには，様々な政府統計のデータベースが整理されていますが，ここでは，2015年国民健康・栄養調査の調査結果から睡眠時間に関するデータベースを見てみましょう．\nデータベースを開くと以下のように統計表が表示され，右上の「ダウンロード」ボタンからデータをダウンロードすることができます．\n\n\n\ne-Statデータベース：統計表表示画面\n\n\n画面左上の「表示項目選択」ボタンをクリックすると，表示するデータの項目（年齢階級・性別など）を選択することができます．\n\n\n\ne-Statデータベース：表示項目選択画面\n\n\nたとえば，年齢階級を選択したい場合は，年齢階級の「項目を選択」ボタンをクリックすると以下のような画面で年齢階級を選択することができます．\n\n\n\ne-Statデータベース：表示項目の設定画面\n\n\n表示項目を選択した後に，「ダウンロード」ボタンをクリックすると，選択した項目のデータのみをダウンロードすることができます．\nこのように，e-Statでは，簡単にデータを抽出・ダウンロードすることができます．しかし，データ取得作業の再現性を高めたり，プログラムを用いたデータ抽出・取得の効率化を行ったりしたい場合は，e-Stat APIを用いるのがおすすめです．"
  },
  {
    "objectID": "posts/2022/12/call-e-stat-api-in-r.html#jpstatパッケージでe-stat-apiを使う",
    "href": "posts/2022/12/call-e-stat-api-in-r.html#jpstatパッケージでe-stat-apiを使う",
    "title": "Rで日本の統計データを効率的に取得しよう（e-Stat APIとjpstatパッケージで）",
    "section": "jpstatパッケージでe-Stat APIを使う",
    "text": "jpstatパッケージでe-Stat APIを使う\n上で説明したe-Statでのデータの抽出・ダウンロードをe-Stat APIで行うためには，以下のようなステップを踏む必要があります．\n\nメタ情報取得・パラメータ設定：表示項目データを取得・選択，選択項目に対応するAPIパラメータを設定\n統計データ取得：選択したデータを取得・表データに整形する\n\njpstatパッケージは，これらの一連の作業をR上で効率的に行うため開発されたものです1．jpstatパッケージは，CRANからインストールすることができます．\nここでは，男女・年齢階級別の睡眠時間をグラフ化することを目標として，さきほど取り上げた睡眠時間に関するデータベース（2015年）からデータを取得してみましょう．まず，必要なパッケージを読み込みます．\n\ninstall.packages(\"jpstat\")\n\n\nlibrary(tidyverse)\nlibrary(jpstat)\n\n\nステップ1：メタ情報（表示項目）を表示・抽出する\ne-Stat APIを用いるためには，事前にユーザ登録を行い，appId と呼ばれるアプリケーションIDを取得する必要があります2．\nestat() 関数に，appId とデータベースのURL（または統計表ID：statsDataId）を入力することでメタ情報（表示項目）を取得することができます3．\nはじめに，メタ情報のうち「年齢階級（cat01）」のデータを見てみましょう（cat01はAPI上での分類名です）．activate() 関数によりメタ情報を表示することができます．さらに，filter() 関数により項目を選択することができます．ここでは，年齢階級別データのみが必要であるため，「総数」データを削除します4．\nパイプ演算子|&gt; を使うことで，以下のように，cat01以外のメタ情報のデータ抽出を続けて行うことができます．ここでは，男女・年齢階級・睡眠時間別の回答者数データを抽出しています．\n\n# ご自身のappIdに置き換えてください\nSys.setenv(ESTAT_API_KEY = \"Your appId\")\n\n\nestat_sleeptime_2015 &lt;- estat(statsDataId = \"https://www.e-stat.go.jp/dbview?sid=0003224282\")\n\n\n# メタ情報の閲覧・選択\nestat_sleeptime_2015 |&gt; \n  activate(cat01) |&gt; \n  filter(name != \"総数\")\n\n# ☐ tab:   表章項目     [2] &lt;code, name, level, unit&gt;\n# ☒ cat01: 年齢階級     [6] &lt;code, name, level, parentCode&gt;\n# ☐ cat02: 睡眠の質     [8] &lt;code, name, level, parentCode&gt;\n# ☐ cat03: 性別         [3] &lt;code, name, level, parentCode&gt;\n# ☐ cat04: 平均睡眠時間 [6] &lt;code, name, level, parentCode&gt;\n# ☐ time:  時間軸(年次) [1] &lt;code, name, level&gt;\n# \n# A tibble: 6 × 4\n  code  name      level parentCode\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     \n1 160   20歳-29歳 2     100       \n2 170   30歳-39歳 2     100       \n3 180   40歳-49歳 2     100       \n4 190   50歳-59歳 2     100       \n5 210   60歳-69歳 2     100       \n6 220   70歳以上  2     100       \n\n\n\nestat_sleeptime_2015_filtered &lt;- estat_sleeptime_2015 |&gt; \n  \n  # 表章項目\n  activate(tab) |&gt; \n  filter(name == \"人数\") |&gt; \n  \n  # 年齢階級\n  activate(cat01) |&gt; \n  filter(name != \"総数\") |&gt; \n  \n  # 睡眠の質\n  activate(cat02) |&gt; \n  filter(name == \"総数\") |&gt; \n  \n  # 性別\n  activate(cat03) |&gt; \n  filter(name %in% c(\"男性\", \"女性\"))\n\n\n\nステップ2：統計データを取得（ダウンロード）する\nデータの抽出後にcollect() を適用することで統計データを取得することができます．また，collect()のn引数で，取得するデータの列を名付けることができます．ここでは，\"person\"と名付けます．\n取得したデータdata_sleeptime_2015を見ると，（たくさんの列が存在する）分析しづらいデータになっていることがわかります．ステップ2+αで，データ取得とデータ整形を同時に行う方法について説明します．\n\ndata_sleeptime_2015 &lt;- estat_sleeptime_2015_filtered |&gt; \n  \n  # データ取得・数値に変換\n  collect(n = \"person\") |&gt; \n  mutate(person = parse_number(person))\n\nThe total number of data is 72.\n\nknitr::kable(head(data_sleeptime_2015, 10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntab_code\ntab_name\ntab_level\ntab_unit\ncat01_code\ncat01_name\ncat01_level\ncat01_parentCode\ncat02_code\ncat02_name\ncat02_level\ncat02_parentCode\ncat03_code\ncat03_name\ncat03_level\ncat03_parentCode\ncat04_code\ncat04_name\ncat04_level\ncat04_parentCode\ntime_code\ntime_name\ntime_level\nperson\n\n\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n110\n男性\n2\n100\n110\n５時間未満\n2\n100\n2015000000\n2015年\n1\n23\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n110\n男性\n2\n100\n120\n５時間以上６時間未満\n2\n100\n2015000000\n2015年\n1\n86\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n110\n男性\n2\n100\n130\n６時間以上７時間未満\n2\n100\n2015000000\n2015年\n1\n88\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n110\n男性\n2\n100\n140\n７時間以上８時間未満\n2\n100\n2015000000\n2015年\n1\n37\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n110\n男性\n2\n100\n150\n８時間以上９時間未満\n2\n100\n2015000000\n2015年\n1\n19\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n110\n男性\n2\n100\n160\n９時間以上\n2\n100\n2015000000\n2015年\n1\n3\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n120\n女性\n2\n100\n110\n５時間未満\n2\n100\n2015000000\n2015年\n1\n28\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n120\n女性\n2\n100\n120\n５時間以上６時間未満\n2\n100\n2015000000\n2015年\n1\n106\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n120\n女性\n2\n100\n130\n６時間以上７時間未満\n2\n100\n2015000000\n2015年\n1\n94\n\n\n100\n人数\n\n人\n160\n20歳-29歳\n2\n100\n100\n総数\n1\nNA\n120\n女性\n2\n100\n140\n７時間以上８時間未満\n2\n100\n2015000000\n2015年\n1\n55\n\n\n\n\n\n\n\nステップ2+α：データ取得とデータ整形を同時に行う\njpstatでe-Statのデータを取得すると，パラメータ名（cat01など）と各項目の列名（code， nameなど）から列（cat01_code，cat01_nameなど）が作成されます．\njpstatでは，rekey() 関数によりパラメータ名を変更したり，select() 関数で項目別に列を選択したりすることでデータを整理することができます5．以下のように書くことで，すっきりとしたデータを作成することができます．\n\ndata_sleeptime_2015 &lt;- estat_sleeptime_2015 |&gt; \n  activate(tab) |&gt; \n  filter(name == \"人数\") |&gt; \n  select() |&gt; \n  \n  activate(cat01) |&gt; \n  rekey(\"ageclass\") |&gt; \n  filter(name != \"総数\") |&gt; \n  select(name) |&gt; \n  \n  activate(cat02) |&gt; \n  filter(name == \"総数\") |&gt; \n  select() |&gt; \n  \n  activate(cat03) |&gt; \n  rekey(\"sex\") |&gt; \n  filter(name %in% c(\"男性\", \"女性\")) |&gt; \n  select(name) |&gt; \n  \n  activate(cat04) |&gt; \n  rekey(\"sleeptime\") |&gt; \n  select(name) |&gt; \n  \n  activate(time) |&gt; \n  select() |&gt; \n  \n  collect(n = \"person\") |&gt; \n  mutate(person = parse_number(person))\n\nThe total number of data is 72.\n\nknitr::kable(head(data_sleeptime_2015, 10))\n\n\n\n\nageclass_name\nsex_name\nsleeptime_name\nperson\n\n\n\n\n20歳-29歳\n男性\n５時間未満\n23\n\n\n20歳-29歳\n男性\n５時間以上６時間未満\n86\n\n\n20歳-29歳\n男性\n６時間以上７時間未満\n88\n\n\n20歳-29歳\n男性\n７時間以上８時間未満\n37\n\n\n20歳-29歳\n男性\n８時間以上９時間未満\n19\n\n\n20歳-29歳\n男性\n９時間以上\n3\n\n\n20歳-29歳\n女性\n５時間未満\n28\n\n\n20歳-29歳\n女性\n５時間以上６時間未満\n106\n\n\n20歳-29歳\n女性\n６時間以上７時間未満\n94\n\n\n20歳-29歳\n女性\n７時間以上８時間未満\n55\n\n\n\n\n\n\n\nおまけ：取得したデータのグラフ化\n最後に，取得した2015年の男女・年齢階級別の睡眠時間データをグラフ化してみましょう．グラフより，男性と女性では年齢階級別の睡眠時間の傾向が異なることがわかります．\n\ndata_sleeptime_2015 |&gt; \n  mutate(ageclass_name = as_factor(ageclass_name),\n         sex_name = as_factor(sex_name),\n         sleeptime_name = as_factor(sleeptime_name)) |&gt; \n  group_by(ageclass_name, sex_name) |&gt; \n  mutate(prop = person / sum(person)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(ageclass_name, prop,\n             fill = fct_rev(sleeptime_name))) +\n  geom_col() +\n  geom_text(aes(label = if_else(prop &gt; 0.05,\n                                scales::label_percent(accuracy = 1)(prop),\n                                \"\")),\n            position = position_stack(vjust = 0.5)) +\n  scale_x_discrete(\"年齢階級\") +\n  scale_y_continuous(\"割合\", \n                     labels = scales::label_percent(accuracy = 1)) +\n  scale_fill_brewer(\"睡眠時間\",\n                    palette = \"Spectral\") +\n  facet_wrap(~ sex_name) +\n  guides(x = guide_axis(n.dodge = 2))"
  },
  {
    "objectID": "posts/2022/12/call-e-stat-api-in-r.html#まとめ",
    "href": "posts/2022/12/call-e-stat-api-in-r.html#まとめ",
    "title": "Rで日本の統計データを効率的に取得しよう（e-Stat APIとjpstatパッケージで）",
    "section": "まとめ",
    "text": "まとめ\n本記事では，e-Stat APIとjpstatパッケージで日本の統計データを効率的に取得する方法について紹介しました．\nRで統計データを取得することで，作業の再現性や効率性を高めることができます．また，jpstatパッケージを使うことで，データ取得とデータ整形を同時に行うことができるため便利です．みなさんもぜひ使ってみてください．"
  },
  {
    "objectID": "posts/2022/12/call-e-stat-api-in-r.html#footnotes",
    "href": "posts/2022/12/call-e-stat-api-in-r.html#footnotes",
    "title": "Rで日本の統計データを効率的に取得しよう（e-Stat APIとjpstatパッケージで）",
    "section": "脚注",
    "text": "脚注\n\n\ne-Stat APIでは，メタ情報取得・統計データ取得以外にも，様々な機能が提供されています（API仕様）．↩︎\nアプリケーションIDの取得には，URLを登録する必要があります．公開サイトで利用しない場合には，http://test.localhost/などのローカルアドレスを入力することが推奨されています（詳しくは利用ガイドを参照）．↩︎\ne-Statのページの右上の「API」ボタンを押すとAPIのクエリが表示されます．クエリ内のstatsDataId を直接入力することでメタ情報を取得することもできます．↩︎\nただし，各パラメータの項目数には，100件という上限が設定されているため，フィルタリング後の項目数が多くなる場合には，フィルタリングを行わず，全ての項目を選択することをおすすめします．↩︎\nselect() 関数である項目の列を全て削除することもできます．これは，「総数」のみを選択する場合など，分析に不要な項目を削除する場合に便利です．↩︎"
  },
  {
    "objectID": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html",
    "href": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html",
    "title": "Rで日本地図を描いてみよう",
    "section": "",
    "text": "Rでは，ggplot2などのパッケージを利用するだけで，都道府県別の日本地図を描くことができます．ここでは，日本地図をggplot2で描画する方法をいくつか紹介します．"
  },
  {
    "objectID": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#地図描画用のサンプルデータ",
    "href": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#地図描画用のサンプルデータ",
    "title": "Rで日本地図を描いてみよう",
    "section": "地図描画用のサンプルデータ",
    "text": "地図描画用のサンプルデータ\nここでは，こちらからダウンロードできる社会・人口統計体系の2015年の都道府県別外国人人口比率データ（10万人あたり外国人人口）を地図描画用のサンプルデータとしました．\n\n\nパッケージの読み込み\nlibrary(tidyverse)\n\n\n\n\nデータのダウンロード\nlibrary(jpstat)\nlibrary(arrow)\n\nforeigner_ratio_2015 &lt;- estat(statsDataId = \"https://www.e-stat.go.jp/en/dbview?sid=0000010201\",\n                              lang = \"E\")\nforeigner_ratio_2015 &lt;- foreigner_ratio_2015 |&gt; \n  activate(tab) |&gt; \n  select() |&gt; \n  \n  activate(cat01) |&gt; \n  # Ratio of population of foreigners (per 100,000 persons)\n  filter(code == \"#A01601\") |&gt; \n  select() |&gt; \n  \n  activate(area) |&gt; \n  filter(name != \"All Japan\") |&gt; \n  select(code, name) |&gt; \n  rekey(\"pref\") |&gt; \n  \n  activate(time) |&gt; \n  filter(name == \"2015\") |&gt; \n  select()\nforeigner_ratio_2015 &lt;- foreigner_ratio_2015 |&gt; \n  collect(\"foreigners_per_100K\")\n\nwrite_parquet(foreigner_ratio_2015, \"foreigner_ratio_2015.parquet\")\n\n\n\n\nデータの読み込み\nlibrary(arrow)\n\nforeigner_ratio_2015 &lt;- read_parquet(\"foreigner_ratio_2015.parquet\") |&gt; \n  mutate(pref_code = pref_code |&gt; \n           str_extract(\"^\\\\d{2}\") |&gt; \n           parse_integer(),\n         \n         pref_name = pref_name |&gt; \n           str_remove(\"-.+$\"),\n         pref_name = case_when(pref_name == \"Gumma\" ~ \"Gunma\",\n                               TRUE ~ pref_name),\n         \n         foreigner_ratio = parse_number(foreigners_per_100K) / 1e5,\n         .keep = \"unused\")\n\n\n\n# 地図描画用のサンプルデータ\nhead(foreigner_ratio_2015)"
  },
  {
    "objectID": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#geom_map-で日本地図を描く",
    "href": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#geom_map-で日本地図を描く",
    "title": "Rで日本地図を描いてみよう",
    "section": "geom_map() で日本地図を描く",
    "text": "geom_map() で日本地図を描く\nggplot2でmap_data() やgeom_map() を使って世界の国々の地図を描画することができます．これには，あらかじめmapsパッケージとmapdataパッケージをダウンロードしておく必要があります（mapdataパッケージに日本地図が格納されています）．\nmap_data(\"japan\") とすることで，mapsパッケージの地図データがデータフレームに変換されます．このデータフレームのregion 列が都道府県のIDとなるため，aes(map_id = region)を設定した上で，geom_map() することで，描画したいデータのregion 列と都道府県ジオメトリがリンクします．\nただし，map_data(\"japan\") は，以下の点に注意が必要です．\n\nあらかじめlibrary(mapdata) を実行してください（実行しないとデータが読み込めないようです）\nregion 列はすべてアルファベット表記である\n他の都道府県と異なり奈良県だけがNARA と大文字表記になっているなど元データにやや問題があるようです（今回はstr_to_title()で修正しました）\n\nまた，日本地図全体を表示するためには，expand_limits() などで軸を設定すること必要になります．\n\n# pak::pak(\"maps\")\n# pak::pak(\"mapdata\")\nlibrary(tidyverse)\nlibrary(mapdata)\n\nmap_data_japan &lt;- map_data(\"japan\") |&gt; \n  as_tibble() |&gt; \n  mutate(region = str_to_title(region))\nhead(map_data_japan)\n\n\n  \n\n\nggplot(foreigner_ratio_2015 |&gt; \n         rename(region = pref_name),\n       aes(map_id = region)) +\n  geom_map(aes(fill = foreigner_ratio),\n           map = map_data_japan) +\n  expand_limits(x = map_data_japan$long,\n                y = map_data_japan$lat) +\n  scale_fill_viridis_c(\"外国人人口比率\",\n                       limits = c(0, 0.03),\n                       labels = scales::label_percent(),\n                       option = \"turbo\")"
  },
  {
    "objectID": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#sfパッケージで日本地図を描く",
    "href": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#sfパッケージで日本地図を描く",
    "title": "Rで日本地図を描いてみよう",
    "section": "sfパッケージで日本地図を描く",
    "text": "sfパッケージで日本地図を描く\nggplot2のgeom_sf()を使えばsfパッケージのジオメトリを簡単に描画できます．\nsfパッケージのst_as_sf() を使えば，maps・mapdataパッケージの提供する地図データをsfオブジェクトに変換することができます．日本地図データをsfに変換することで，先ほどのコードよりも直感的に地図を描くことができます．\n\nlibrary(sf)\n\nmap_japan &lt;- maps::map(\"japan\", \n                       plot = FALSE,\n                       fill = TRUE) |&gt; \n  st_as_sf() |&gt; \n  rename(pref_name = ID) |&gt; \n  mutate(pref_name = str_to_title(pref_name))\n\nmap_japan |&gt; \n  left_join(foreigner_ratio_2015,\n            by = join_by(pref_name)) |&gt; \n  ggplot(aes(fill = foreigner_ratio)) +\n  geom_sf(color = \"transparent\") +\n  scale_fill_viridis_c(\"外国人人口比率\",\n                       limits = c(0, 0.03),\n                       labels = scales::label_percent(),\n                       option = \"turbo\")\n\n\n\n\n\n\n\n\n\nさらに詳細な地図情報を入手可能なrnaturalearthについて\n世界地図のデータを取得可能なパッケージにはrnaturalearthパッケージもあります．\n都道府県別地図を取得するにはrnaturalearthのne_states(\"japan\") を実行します．ne_states(\"japan\") の出力には英語の都道府県名に加えて日本語の都道府県名も含まれており便利です．\n\nmap_japan &lt;- rnaturalearth::ne_states(\"japan\") |&gt; \n  as_tibble() |&gt; \n  st_as_sf() |&gt; \n  select(iso_3166_2, name_ja, name_en) |&gt; \n  arrange(iso_3166_2)\nhead(map_japan)"
  },
  {
    "objectID": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#日本地図をレイアウトするためのjpmapパッケージ",
    "href": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#日本地図をレイアウトするためのjpmapパッケージ",
    "title": "Rで日本地図を描いてみよう",
    "section": "日本地図をレイアウトするためのjpmapパッケージ",
    "text": "日本地図をレイアウトするためのjpmapパッケージ\n日本地図の描画では，日本列島を大きく描画するために琉球諸島や小笠原諸島を地図上の左上や右下に配置したいケースがあります．ggplot2で作成した日本地図のレイアウトを簡単に行えるようにjpmapを作成しました．\njpmapは，以下の機能を持っています．\n\n琉球諸島・小笠原諸島を再配置したレイアウトを可能にするjpmap::layout_japan()\n\nただし再配置される琉球諸島・小笠原諸島の縮尺は厳密ではありませんのでご注意ください\nryukyu = FALSEやogasawara = FALSE を指定することで琉球諸島・小笠原諸島を非表示にすることが可能です\n\n日本語の都道府県名や都道府県コードが含む都道府県データを提供するjpmap::prefecture（rnaturalearthのデータがベースです）\n\nただし，英語の都道府県名（pref_name）がHokkaidoではなくHokkaidōといった表記なっているためご注意ください．\n\n\njpmap::layout_japan() で地図のレイアウトを変更することで，都道府県ごとの傾向がよりわかりやすくなります．\n\n# pak::pak(\"UchidaMizuki/jpmap\")\njpmap::prefecture\n\n\n  \n\n\nplot &lt;- jpmap::prefecture |&gt; \n  left_join(foreigner_ratio_2015 |&gt; \n              select(!pref_name),\n            by = join_by(pref_code)) |&gt; \n  ggplot(aes(fill = foreigner_ratio)) +\n  geom_sf(color = \"transparent\") +\n  scale_fill_viridis_c(\"外国人人口比率\",\n                       limits = c(0, 0.03),\n                       labels = scales::label_percent(),\n                       option = \"turbo\")\n\njpmap::layout_japan(plot)"
  },
  {
    "objectID": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#まとめ",
    "href": "posts/2022/06/draw-a-map-of-japan-in-r-ja.html#まとめ",
    "title": "Rで日本地図を描いてみよう",
    "section": "まとめ",
    "text": "まとめ\n2015年の都道府県別外国人人口比率に関する日本地図から以下のことがわかりました．\n\n2015年ではどの都道府県でも外国人人口比率が3％以下である\n東京都は外国人人口比率が最も多く，愛知県や群馬県なども外国人口比率が高い．\n\nここまで，ggplot2などのパッケージを活用した日本地図の描画を試してみました．\nその結果，Rを使えば，自前でデータを整備しなくても，簡単に日本地図を描けることがわかりました．みなさんもぜひ，ggplot2を使って，色々な地図を使ってみてください！"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UchidaMizuki",
    "section": "",
    "text": "Julia JuMPで効用最大化問題を解いてみた（コンストラクタを使って）\n\n\n\n\n\n\nJuMP\n\n\nJulia\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n割合データ（割り算値）から観測ノイズを除去する\n\n\n\n\n\n\nJapanese\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nJulia JuMPで効用最大化問題を解いてみた（簡易的な事例紹介）\n\n\n\n\n\n\nJuMP\n\n\nJulia\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRでExcelファイルを整形する際のTips\n\n\n\n\n\n\nJapanese\n\n\nR\n\n\nExcel\n\n\nTidy Data\n\n\nData Wrangling\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR ggraphで全国の市町村合併を可視化する\n\n\n\n\n\n\nJapanese\n\n\njpcity\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRで市区町村データをスマートに整形しよう（市町村合併や政令指定都市の集計処理など）\n\n\n\n\n\n\nJapanese\n\n\njpcity\n\n\n\n\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n平均が正規分布に従う正規分布の周辺分布を求める\n\n\n\n\n\n\nJapanese\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nMar 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLiNGAMによる因果探索では関数形に注意しよう\n\n\n\n\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nScraping dynamic sites with rvest (without Selenium)\n\n\n\n\n\n\nrvest\n\n\nR\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nrvestで動的サイトをスクレイピングする（Seleniumを使わずに）\n\n\n\n\n\n\nrvest\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImprove the quality of data preprocessing with R’s dm package\n\n\n\n\n\n\ndm\n\n\nR\n\n\nEnglish\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRのdmパッケージでデータ前処理の質を高めよう\n\n\n\n\n\n\ndm\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRの数値積分で円の面積を求める\n\n\n\n\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nQuartoを活用してTokyo.Rで発表しました\n\n\n\n\n\n\ne-Stat\n\n\nLT\n\n\nTokyo.R\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nなぜベイズの定理が重要なのかーPCR検査を例にー\n\n\n\n\n\n\nBayes\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n世代間の「1票の格差」を定量化してみる\n\n\n\n\n\n\nPolitics\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n地域メッシュデータのためのWEBアプリをつくりました（R Shiny&jpgrid）\n\n\n\n\n\n\nShiny\n\n\njpgrid\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRで人口ピラミッドのアニメーションを作る\n\n\n\n\n\n\nggplot2\n\n\ngganimate\n\n\ne-Stat\n\n\njpstat\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRで日本の統計データを効率的に取得しよう（e-Stat APIとjpstatパッケージで）\n\n\n\n\n\n\ne-Stat\n\n\njpstat\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nDec 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCSVの代わりにParquetを使ってみよう\n\n\n\n\n\n\nparquet\n\n\narrow\n\n\nR\n\n\nPython\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nJun 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRで日本地図を描いてみよう\n\n\n\n\n\n\nggplot2\n\n\nsf\n\n\njpmap\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nJun 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRで産業連関分析\n\n\n\n\n\n\nioanalysis\n\n\ndibble\n\n\nR\n\n\nJapanese\n\n\n\n\n\n\n\n\n\nMay 31, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Uchida Mizuki.\nI am a classical music lover."
  },
  {
    "objectID": "about.html#games",
    "href": "about.html#games",
    "title": "About",
    "section": "Games",
    "text": "Games\n\nMakeMultiples\n\n\n\nAvoidCrowds (PC only)"
  },
  {
    "objectID": "about.html#apps",
    "href": "about.html#apps",
    "title": "About",
    "section": "Apps",
    "text": "Apps\n\nUrayasu Accessibility App (in Japanese)\n\n\njpgrid App (in Japanese)"
  },
  {
    "objectID": "posts/2022/05/ioanalysis-in-r-ja.html",
    "href": "posts/2022/05/ioanalysis-in-r-ja.html",
    "title": "Rで産業連関分析",
    "section": "",
    "text": "産業連関分析は，経済波及効果の算出に広く用いられている分析手法です． 日本では，国や都道府県によって，約5年に1度，産業連関表と呼ばれる統計データが 作成・公開されており，産業連関分析における基礎データとなっています．\nこれまで，産業連関分析では，Excel・VBAが用いられることが多かったようです．\n一方で，近年は，Python・R・Juliaなどのプログラミング言語の普及が進んでいます． これらのプログラミング言語は以下のような特長を持っています．\nそのため今後は，産業連関分析においても，これらのプログラミング言語の利用が 進むのではないかと思われます．\nここでは，Rを用いて産業連関分析を行います． Rでは近年，tidyverseなどモダンなデータ分析を行うためのパッケージが 多く提供されており，プログラミング初心者でも習得しやすい言語であると思います．\n産業連関表として，e-Statのデータベースで公開されている日本（国）の 2013年・13部門産業連関表を用います． ここで使用するデータは， こちら からダウンロードできます．"
  },
  {
    "objectID": "posts/2022/05/ioanalysis-in-r-ja.html#産業連関分析の基礎",
    "href": "posts/2022/05/ioanalysis-in-r-ja.html#産業連関分析の基礎",
    "title": "Rで産業連関分析",
    "section": "産業連関分析の基礎",
    "text": "産業連関分析の基礎\n産業連関分析は一般的に以下の流れに従って行われます．\n\n産業連関表の整形\n投入係数行列の算出\nレオンチェフ逆行列の算出\n経済波及効果の算出\n\nまず，産業連関分析において重要な投入係数行列・レオンチェフ逆行列・経済波及効果の算出方法について解説します．\n\n投入係数行列とは\n投入係数は，産業の「クッキングレシピ」として呼ばれており，産業\\(j\\)の生産物を1単位生産するのに必要な産業\\(i\\)の生産物の量を表すものです．具体的には，以下のように中間投入\\(x_{ij}\\)を生産額\\(X_j\\)（産出額）で割ることで算出できます．\n\\[\na_{ij}=\\frac{x_{ij}}{X_j}\n\\]\n産業連関分析では，「クッキングレシピ」に相当する投入係数\\(a_{ij}\\)に基づく生産額のバランス式（行方向）を連立方程式として解きます．そこで，以下のように，投入係数行列（通常，\\(A\\)と表される）と呼ばれる行列を作成することで，連立方程式が簡単に解けるようになります．\n\\[\nA = \\begin{pmatrix}\n  a_{11} & \\cdots & a_{1n} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  a_{n1} & \\cdots & a_{nn} \\\\\n\\end{pmatrix}\n\\]\n\n\nレオンチェフ逆行列による経済波及効果の推計について\n生産額のバランス式（行方向）は，行列を用いて以下のように表せます．変数の意味は以下の表の通りです．\n\\[\nAX + F + E - M = X\n\\]\n\n\n\n変数\n意味\n\n\n\n\n\\(A\\)\n投入係数行列\n\n\n\\(X = (X_1, \\cdots, X_n) ^ \\top\\)\n生産額ベクトル\n\n\n\\(F = (F_1, \\cdots, F_n) ^ \\top\\)\n最終需要ベクトル\n\n\n\\(E = (E_1, \\cdots, E_n) ^ \\top\\)\n移輸出ベクトル\n\n\n\\(M = (M_1, \\cdots, M_n) ^ \\top\\)\n移輸入ベクトル\n\n\n\n経済波及効果の推計では，最終需要の変化が生産額に与える波及効果を算出します．\n特に，日本の産業連関表での経済波及効果の推計では，移輸入\\(M\\)の扱いに注意が必要です（これは，日本表の多くが競争移輸入型表と呼ばれる形式を採用しており，投入係数に移輸入分が含まれているためです）．\n最終需要による経済波及効果は，域内の生産額だけでなく域外からの移輸入を誘発すると考えられます．この効果を無視すると経済波及効果を過大評価することにつながるため，通常，投入係数から移輸入相当分を差し引くという処理が行われます．\n移輸入は域内需要におおよそ比例すると考えられるため，以下のように，移輸入係数\\(\\hat{M_i}\\)が算出できます．\n\\[\n\\hat{M_i} = \\frac{M_i}{\\sum_{j}a_{ij}X_j + F_i}\n\\]\nさらに，行列での計算に適した移輸入係数行列\\(\\hat{M}\\)が，以下のように定義されます．\n\\[\n\\hat{M} =\n\\begin{pmatrix}\n  \\hat{M_1} & & 0 \\\\\n  & \\ddots & \\\\\n  0 & & \\hat{M_n} \\\\\n\\end{pmatrix}\n\\]\n以上より，生産額のバランス式（行方向）は，移輸入係数行列\\(\\hat{M}\\)を用いて，以下のように変形されます．ただし，\\(I\\)は単位行列（対角成分が1，それ以外が0の正方行列）です．\n\\[\n\\begin{align}\n  AX + F + E - \\hat{M} (AX + F) &= X \\\\\n  (I - \\hat{M}) (AX + F) + E &= X\n\\end{align}\n\\]\n上のバランス式より，経済波及効果の算出式が，以下のように導出されます．ここで，\\(\\Delta X\\)，\\(\\Delta F\\)は，それぞれ，生産額の変化量，最終需要の変化量です．\n\\[\n\\begin{align}\n  X &= (I - \\hat{M}) (AX + F) + E \\\\\n  [I - (I - \\hat{M}) A] X &= (I - \\hat{M}) F + E \\\\\n  X &= [I - (I - \\hat{M}) A] ^ {-1} [(I - \\hat{M}) F + E] \\\\\n  \\Delta X &= [I - (I - \\hat{M}) A] ^ {-1} (I - \\hat{M}) \\Delta F\n\\end{align}\n\\]\n生産額の変化量\\(\\Delta X\\)の式の右辺の\\((I - \\hat{M}) \\Delta F\\)は，最終需要の変化量に自給率\\(I - \\hat{M}\\)を掛けた値となっています．\nまた，\\([I - (I - \\hat{M}) A] ^ {-1}\\)は，最終需要の変化による直接・間接の波及効果を表す行列であり（開放型または競争移輸入型の）レオンチェフ逆行列と呼ばれています．\n以上のように，最終需要の変化量\\(\\Delta F\\)から生産額の変化量\\(\\Delta X\\)を推計するというのが，最も一般的な産業連関分析の方法となっています．"
  },
  {
    "objectID": "posts/2022/05/ioanalysis-in-r-ja.html#rによる産業連関分析",
    "href": "posts/2022/05/ioanalysis-in-r-ja.html#rによる産業連関分析",
    "title": "Rで産業連関分析",
    "section": "Rによる産業連関分析",
    "text": "Rによる産業連関分析\n\n産業連関表の整形\nここでは， こちら からダウンロードできる日本の2011年の3部門表（iotable_3sector_2011_wider.csv）を使用します．\nこちらの表は，以下のように，日本の2011年の13部門表より作成したもので，単位は「百万円」です．\n\n13部門の産業分類を第1次・第2次・第3次産業に集計（注：「分類不明」を第3次産業に分類）\n付加価値部門を1部門に集計\n最終需要部門を域内最終需要（finaldemand）・輸出（export）・輸入（import）の3部門に集計\n\n産業連関表のデータ形式は，e-Statのデータベースで提供されている表などを除いて， 行に投入部門（input）・列に産出部門（output）を持つ「横長データ」であることが多いと思われます．\nここでも，以下の通り，まずは横長の産業連関表データを読み込みます．\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\niotable_wider &lt;- read_csv(\"iotable_3sector_2011_wider.csv\",\n                          col_types = cols(.default = \"c\")) |&gt; \n  \n  # input (投入) 列以外を数値に変換\n  mutate(across(!input, parse_number))\n\nknitr::kable(iotable_wider)\n\n\n\n\n\n\n\n\n\n\n\n\n\ninput\nindustry/01_primary\nindustry/02_secondary\nindustry/03_tertiary\nfinaldemand/04_finaldemand\nexport/05_export\nimport/06_import\n\n\n\n\nindustry/01_primary\n1456611\n7850628\n1373767\n3869875\n47890\n-2562809\n\n\nindustry/02_secondary\n2715710\n161897553\n62841827\n132924323\n54473273\n-71673715\n\n\nindustry/03_tertiary\n2025270\n66811645\n155796589\n352324555\n16423417\n-8921553\n\n\nvalueadded/04_valueadded\n5838371\n106619145\n364447740\nNA\nNA\nNA\n\n\n\n\n\nデータ分析においては，「横長データ」よりも，以下のような「縦長データ」のほうが， 分析しやすい場合が多くあります． ここでも，横長の産業連関表を「縦長データ」に変換します．\n\niotable &lt;- iotable_wider |&gt;\n  \n  # input (投入) 列を分類・名称に分割\n  separate(input, c(\"input_type\", \"input_name\"),\n           sep = \"/\") |&gt;\n  \n  # input (投入) と同様にoutput (産出) の分類・名称列を追加し縦長データに\n  pivot_longer(!c(input_type, input_name),\n               names_to = c(\"output_type\", \"output_name\"),\n               names_sep = \"/\",\n               values_to = \"value_M\") |&gt;\n  \n  # 数値が存在しない行を削除\n  drop_na(value_M)\n\nknitr::kable(iotable)\n\n\n\n\ninput_type\ninput_name\noutput_type\noutput_name\nvalue_M\n\n\n\n\nindustry\n01_primary\nindustry\n01_primary\n1456611\n\n\nindustry\n01_primary\nindustry\n02_secondary\n7850628\n\n\nindustry\n01_primary\nindustry\n03_tertiary\n1373767\n\n\nindustry\n01_primary\nfinaldemand\n04_finaldemand\n3869875\n\n\nindustry\n01_primary\nexport\n05_export\n47890\n\n\nindustry\n01_primary\nimport\n06_import\n-2562809\n\n\nindustry\n02_secondary\nindustry\n01_primary\n2715710\n\n\nindustry\n02_secondary\nindustry\n02_secondary\n161897553\n\n\nindustry\n02_secondary\nindustry\n03_tertiary\n62841827\n\n\nindustry\n02_secondary\nfinaldemand\n04_finaldemand\n132924323\n\n\nindustry\n02_secondary\nexport\n05_export\n54473273\n\n\nindustry\n02_secondary\nimport\n06_import\n-71673715\n\n\nindustry\n03_tertiary\nindustry\n01_primary\n2025270\n\n\nindustry\n03_tertiary\nindustry\n02_secondary\n66811645\n\n\nindustry\n03_tertiary\nindustry\n03_tertiary\n155796589\n\n\nindustry\n03_tertiary\nfinaldemand\n04_finaldemand\n352324555\n\n\nindustry\n03_tertiary\nexport\n05_export\n16423417\n\n\nindustry\n03_tertiary\nimport\n06_import\n-8921553\n\n\nvalueadded\n04_valueadded\nindustry\n01_primary\n5838371\n\n\nvalueadded\n04_valueadded\nindustry\n02_secondary\n106619145\n\n\nvalueadded\n04_valueadded\nindustry\n03_tertiary\n364447740\n\n\n\n\n\n上で構築した表データは，各行のフィルタリングなどが容易にできる一方で， 産業連関分析に用いられる行列計算などに適していません．\nそこで，表データの基本的な演算と行列計算を同時に行えるdibbleパッケージを用います． 以下のように，産業連関表をdibbleに変換します．\n\n# pak::pak(\"UchidaMizuki/dibble\")\nlibrary(dibble)\n\niotable &lt;- iotable |&gt;\n  dibble_by(input = c(input_type, input_name),\n            output = c(output_type, output_name),\n            \n            # \"_\"で列名を分割してinput (投入)・output (産出) 軸を設定\n            .names_sep = \"_\")\n\niotable\n\n# A dibble:   24 x 1\n# Dimensions: input [4], output [6]\n# Measures:   value_M\n   input$type $name        output$type $name            value_M\n   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n 1 industry   01_primary   industry    01_primary       1456611\n 2 industry   01_primary   industry    02_secondary     7850628\n 3 industry   01_primary   industry    03_tertiary      1373767\n 4 industry   01_primary   finaldemand 04_finaldemand   3869875\n 5 industry   01_primary   export      05_export          47890\n 6 industry   01_primary   import      06_import       -2562809\n 7 industry   02_secondary industry    01_primary       2715710\n 8 industry   02_secondary industry    02_secondary   161897553\n 9 industry   02_secondary industry    03_tertiary     62841827\n10 industry   02_secondary finaldemand 04_finaldemand 132924323\n# ℹ 14 more rows\n\n\n\n\n投入係数行列の算出\n産業の「クッキングレシピ」と呼ばれる投入係数行列\\(A\\)を以下のように中間投入を生産額で割って算出します．\n注：dibbleではブロードキャストが自動で行われますが，安全のため，ブロードキャストを行う際に，警告を発するように設計されています．そのため，broadcast()でブロードキャスト後の軸名c(\"input\", \"output\")を与えて警告が出ないようにする必要があります．\n\n# 生産額\ntotal_input &lt;- iotable |&gt;\n  filter(output$type == \"industry\") |&gt;\n  apply(\"output\", sum)\n\n# 中間投入\ninterindustry &lt;- iotable |&gt;\n  filter(input$type == \"industry\",\n         output$type == \"industry\")\n\n# 投入係数\ninputcoeff &lt;- broadcast(interindustry / total_input,\n                        c(\"input\", \"output\"))\n\ninputcoeff\n\n# A dibble:   9\n# Dimensions: input [3], output [3]\n  input$type $name        output$type $name              .\n  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 industry   01_primary   industry    01_primary   0.121  \n2 industry   01_primary   industry    02_secondary 0.0229 \n3 industry   01_primary   industry    03_tertiary  0.00235\n4 industry   02_secondary industry    01_primary   0.226  \n5 industry   02_secondary industry    02_secondary 0.472  \n6 industry   02_secondary industry    03_tertiary  0.108  \n7 industry   03_tertiary  industry    01_primary   0.168  \n8 industry   03_tertiary  industry    02_secondary 0.195  \n9 industry   03_tertiary  industry    03_tertiary  0.267  \n\n\n\n\nレオンチェフ逆行列の算出\n経済波及効果を表すレオンチェフ逆行列は以下のように，移輸入係数と投入係数を用いて算出できます．\n注：solve()で逆行列を算出すると行列の軸名が入れ替わるため注意してください．\n\n# 域内需要\nlocaldemand &lt;- iotable |&gt;\n  filter(input$type == \"industry\",\n         !output$type %in% c(\"export\", \"import\")) |&gt;\n  apply(\"input\", sum)\n\n# (移) 輸入\nimport &lt;- iotable |&gt;\n  filter(input$type == \"industry\",\n         output$type == \"import\") |&gt;\n  apply(\"input\", sum)\n# 符号を正に\nimport &lt;- -import\n\n# (移) 輸入係数\nimportcoeff &lt;- import / localdemand\n\nI &lt;- eye(inputcoeff) # 単位行列\nM &lt;- importcoeff     # 移輸入係数ベクトル (broadcastが行われるため行列でなくてよい)\nA &lt;- inputcoeff      # 投入係数行列\n\n# レオンチェフ逆行列\nleontiefinv &lt;- broadcast(I - (1 - M) * A,\n                         c(\"input\", \"output\")) |&gt;\n  solve()\n\nleontiefinv\n\n# A dibble:   9\n# Dimensions: output [3], input [3]\n  output$type $name        input$type $name              .\n  &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;\n1 industry    01_primary   industry   01_primary   1.12   \n2 industry    01_primary   industry   02_secondary 0.0361 \n3 industry    01_primary   industry   03_tertiary  0.00716\n4 industry    02_secondary industry   01_primary   0.374  \n5 industry    02_secondary industry   02_secondary 1.68   \n6 industry    02_secondary industry   03_tertiary  0.197  \n7 industry    03_tertiary  industry   01_primary   0.348  \n8 industry    03_tertiary  industry   02_secondary 0.445  \n9 industry    03_tertiary  industry   03_tertiary  1.41   \n\n\n\n\n経済波及効果の算出\nこちら からダウンロードできる最終需要がそれぞれ百万円ずつ増加する（finaldemand_change_3sector.csv）ケースで経済波及効果を算出しています．\n\n# 最終需要変化量\nfinaldemand_change &lt;- read_csv(\"finaldemand_change_3sector.csv\",\n                               col_types = cols(.default = \"c\",\n                                                value_M = \"n\")) |&gt; \n  dibble_by(input = c(input_type, input_name),\n            .names_sep = \"_\")\n\nL &lt;- leontiefinv         # レオンチェフ逆行列\nM &lt;- importcoeff         # 移輸入係数\nFD &lt;- finaldemand_change # 最終需要変化量\n\n# 経済波及効果\nspillover &lt;- L %*% ((1 - M) * FD)\n\nspillover\n\n# A dibble:   3\n# Dimensions: output [3]\n  output$type $name            .\n  &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;\n1 industry    01_primary   0.958\n2 industry    02_secondary 1.85 \n3 industry    03_tertiary  2.03 \n\n\n\ntheme_set(theme_light())\n\nspillover |&gt; \n  as_tibble(n = \"value_M\") |&gt; \n  unpack(output, \n         names_sep = \"_\") |&gt; \n  ggplot(aes(output_name, value_M,\n             fill = output_name)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "posts/2022/05/ioanalysis-in-r-ja.html#まとめ",
    "href": "posts/2022/05/ioanalysis-in-r-ja.html#まとめ",
    "title": "Rで産業連関分析",
    "section": "まとめ",
    "text": "まとめ\nRを用いた産業連関分析の方法について紹介しました．\nここまでの計算を，jpioにパッケージ形式でまとめました．以下のように，ここまでの計算と同様の計算を行うことができます．\n\n# pak::pak(\"UchidaMizuki/jpio\")\n\n# 産業連関表\niotable &lt;- read_csv(\"iotable_3sector_2011.csv\",\n                    col_types = cols(.default = \"c\",\n                                     value_M = \"n\")) |&gt; \n  jpio::as_iotable()\n\niotable\n\n# A dibble:   24\n# Dimensions: input [4], output [6]\n   input$type $name        output$type $name                  .\n   &lt;fct&gt;      &lt;chr&gt;        &lt;fct&gt;       &lt;chr&gt;              &lt;dbl&gt;\n 1 industry   01_primary   industry    01_primary       1456611\n 2 industry   01_primary   industry    02_secondary     7850628\n 3 industry   01_primary   industry    03_tertiary      1373767\n 4 industry   01_primary   finaldemand 04_finaldemand   3869875\n 5 industry   01_primary   export      05_export          47890\n 6 industry   01_primary   import      06_import       -2562809\n 7 industry   02_secondary industry    01_primary       2715710\n 8 industry   02_secondary industry    02_secondary   161897553\n 9 industry   02_secondary industry    03_tertiary     62841827\n10 industry   02_secondary finaldemand 04_finaldemand 132924323\n# ℹ 14 more rows\n\n# 投入係数\njpio::input_coef(iotable)\n\n# A dibble:   9\n# Dimensions: input [3], output [3]\n  input$type $name        output$type $name              .\n  &lt;fct&gt;      &lt;chr&gt;        &lt;fct&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 industry   01_primary   industry    01_primary   0.121  \n2 industry   01_primary   industry    02_secondary 0.0229 \n3 industry   01_primary   industry    03_tertiary  0.00235\n4 industry   02_secondary industry    01_primary   0.226  \n5 industry   02_secondary industry    02_secondary 0.472  \n6 industry   02_secondary industry    03_tertiary  0.108  \n7 industry   03_tertiary  industry    01_primary   0.168  \n8 industry   03_tertiary  industry    02_secondary 0.195  \n9 industry   03_tertiary  industry    03_tertiary  0.267  \n\n# レオンチェフ逆行列\njpio::leontief_inv(iotable)\n\n# A dibble:   9\n# Dimensions: output [3], input [3]\n  output$type $name        input$type $name              .\n  &lt;fct&gt;       &lt;chr&gt;        &lt;fct&gt;      &lt;chr&gt;          &lt;dbl&gt;\n1 industry    01_primary   industry   01_primary   1.12   \n2 industry    01_primary   industry   02_secondary 0.0361 \n3 industry    01_primary   industry   03_tertiary  0.00716\n4 industry    02_secondary industry   01_primary   0.374  \n5 industry    02_secondary industry   02_secondary 1.68   \n6 industry    02_secondary industry   03_tertiary  0.197  \n7 industry    03_tertiary  industry   01_primary   0.348  \n8 industry    03_tertiary  industry   02_secondary 0.445  \n9 industry    03_tertiary  industry   03_tertiary  1.41   \n\n# 経済波及効果\njpio::spillover_effect(iotable,\n                       list(`01_primary` = 1,\n                            `02_secondary` = 1,\n                            `03_tertiary` = 1))\n\n# A dibble:   3\n# Dimensions: output [3]\n  output$type $name            .\n  &lt;fct&gt;       &lt;chr&gt;        &lt;dbl&gt;\n1 industry    01_primary   0.958\n2 industry    02_secondary 1.85 \n3 industry    03_tertiary  2.03 \n\n# スカイラインチャート\njpio::skyline_chart(iotable, \n                    ylim = c(-0.5, NA))"
  },
  {
    "objectID": "posts/2022/06/use-parquet-instead-of-csv-ja.html",
    "href": "posts/2022/06/use-parquet-instead-of-csv-ja.html",
    "title": "CSVの代わりにParquetを使ってみよう",
    "section": "",
    "text": "本記事では，CSVの代替として有望かつビッグデータ分析にも適しているParquetを紹介します．\nさて，データフレーム（Data Frames）は，データ分析において最も基本的なデータ構造の1つです．Rのtibble・dplyrやPythonのpandasなどのデータフレーム操作のためのパッケージを使えば，これまでExcelなどの表計算ソフトで行っていたデータ分析をさらに効率的に行うことができます．\nこのようにデータ分析ツールが充実している一方で，データの保存にはExcelなどとの互換性が高いCSVが未だに広く使われています．しかし，CSVは，必ずしもデータ分析に適したファイル形式とは言えません．そこで，CSVの代替として使われることが多くなっているParquetをCSVと比較してみましょう．"
  },
  {
    "objectID": "posts/2022/06/use-parquet-instead-of-csv-ja.html#サンプルデータの準備",
    "href": "posts/2022/06/use-parquet-instead-of-csv-ja.html#サンプルデータの準備",
    "title": "CSVの代わりにParquetを使ってみよう",
    "section": "サンプルデータの準備",
    "text": "サンプルデータの準備\nCSVとParquetを比較するため，まずは，データ分析にありがちなサンプルデータを用意しましょう．今回は，tidyrパッケージで提供されているwho （世界保健機関（WHO）結核データ）からサンプルデータをつくります．\n近年，データ分析では，整然データ（tidy data）の概念が普及しています．tidy dataは，個々の変数が1つの列をなし，個々の観測（値）が1つの行をなすようなデータです．\nそれでは，whoは，tidy dataと言えるでしょうか？whoには，\"new_sp_m014\" ～\"newrel_f65\" といったたくさんの列が存在しますが，これらには，1列ごとに，診断結果（spやsel）・性別（mとf）・年齢階級（014や65）といった複数の変数が含まれています．そのため，who は，tidy dataでないといえます．そこで，こちらに従ってtidy dataであるwho_longerに変形します．\nデータ分析では，who よりtidy dataであるwho_longer のほうを分析が行いやすい一方で，行数はwho（約7,000行）よりwho_longer （約400,000行）のほうが約50倍多いことがわかります．そのため，tidy dataであるwho_longerのようなデータをテキストファイルであるCSVで保存すると容量が増大してしまいます．\nこのように，tidy dataはデータ分析に適している一方で，CSVのようなテキストファイルでの保存に適していないことがわかります．しかし，このようなデータ保存上の課題はParquetを使えば解決することができます．\nここで，tidy dataでないwho とtidy dataであるwho_longer を見比べてみましょう．\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\nlibrary(fs)\n\n\n\nコード\nlevels_gender &lt;- c(\"f\", \"m\")\nlevels_age &lt;- c(\"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"65\")\n\nwho_longer &lt;- who |&gt; \n  pivot_longer(cols = new_sp_m014:newrel_f65,\n               names_to = c(\"diagnosis\", \"gender\", \"age\"), \n               names_pattern = \"new_?(.*)_(.)(.*)\",\n               names_transform = list(gender = ~ .x |&gt; \n                                        readr::parse_factor(levels = levels_gender),\n                                      age = ~ .x |&gt; \n                                        readr::parse_factor(levels = levels_age,\n                                                            ordered = TRUE)),\n               values_to = \"count\")\n\n\n\n# データ整形前\nprint(who, n = 5)\n\n# A tibble: 7,240 × 60\n  country   iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Afghanis… AF    AFG    1980          NA           NA           NA           NA\n2 Afghanis… AF    AFG    1981          NA           NA           NA           NA\n3 Afghanis… AF    AFG    1982          NA           NA           NA           NA\n4 Afghanis… AF    AFG    1983          NA           NA           NA           NA\n5 Afghanis… AF    AFG    1984          NA           NA           NA           NA\n# ℹ 7,235 more rows\n# ℹ 52 more variables: new_sp_m4554 &lt;dbl&gt;, new_sp_m5564 &lt;dbl&gt;,\n#   new_sp_m65 &lt;dbl&gt;, new_sp_f014 &lt;dbl&gt;, new_sp_f1524 &lt;dbl&gt;,\n#   new_sp_f2534 &lt;dbl&gt;, new_sp_f3544 &lt;dbl&gt;, new_sp_f4554 &lt;dbl&gt;,\n#   new_sp_f5564 &lt;dbl&gt;, new_sp_f65 &lt;dbl&gt;, new_sn_m014 &lt;dbl&gt;,\n#   new_sn_m1524 &lt;dbl&gt;, new_sn_m2534 &lt;dbl&gt;, new_sn_m3544 &lt;dbl&gt;,\n#   new_sn_m4554 &lt;dbl&gt;, new_sn_m5564 &lt;dbl&gt;, new_sn_m65 &lt;dbl&gt;, …\n\n# データ整形後\nprint(who_longer, n = 5)\n\n# A tibble: 405,440 × 8\n  country     iso2  iso3   year diagnosis gender age   count\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;fct&gt;  &lt;ord&gt; &lt;dbl&gt;\n1 Afghanistan AF    AFG    1980 sp        m      014      NA\n2 Afghanistan AF    AFG    1980 sp        m      1524     NA\n3 Afghanistan AF    AFG    1980 sp        m      2534     NA\n4 Afghanistan AF    AFG    1980 sp        m      3544     NA\n5 Afghanistan AF    AFG    1980 sp        m      4554     NA\n# ℹ 405,435 more rows"
  },
  {
    "objectID": "posts/2022/06/use-parquet-instead-of-csv-ja.html#csvparquetの保存方法",
    "href": "posts/2022/06/use-parquet-instead-of-csv-ja.html#csvparquetの保存方法",
    "title": "CSVの代わりにParquetを使ってみよう",
    "section": "CSV・Parquetの保存方法",
    "text": "CSV・Parquetの保存方法\nRでは，write_csv() でCSVを保存できます．同様に，arrowパッケージのwrite_parquet() でParquetを保存することができます．who_longerをCSVとParquetで保存してみましょう．\nCSVとParquetでは，どちらも簡単にデータ保存ができることがわかります．\n\nlibrary(arrow)\n\nWarning: package 'arrow' was built under R version 4.2.3\n\n# CSVを保存\nwrite_csv(who_longer, \"who_longer.csv\")\n\n# Parquetを保存\nwrite_parquet(who_longer, \"who_longer.parquet\")"
  },
  {
    "objectID": "posts/2022/06/use-parquet-instead-of-csv-ja.html#parquetのメリットcsvとの比較",
    "href": "posts/2022/06/use-parquet-instead-of-csv-ja.html#parquetのメリットcsvとの比較",
    "title": "CSVの代わりにParquetを使ってみよう",
    "section": "Parquetのメリット・CSVとの比較",
    "text": "Parquetのメリット・CSVとの比較\nここからは，保存したwho_longer のCSV・Parquetファイルを比較して，CSVに対するParquetのメリットを紹介していきます．\n\nメリット1：CSVよりデータ容量が軽い\ntidy dataは行数が多くなるため，CSVでの保存に適しておらず，Parquetを使ったほうがよいことを既に述べました．\n実際に，who_longer のCSV・Parquetのデータ容量は，それぞれ，14.1 MBと154 KBとなり，ParquetはCSVの約1 %のデータ容量しかないことがわかります．\nどのようなケースでもこのようなデータ容量の削減が見込めるわけではありませんが，Parquetは列指向でデータ圧縮を行うため，Rなどでよく用いられるtidy dataの保存に適したデータ形式であるといえます．\n\n# CSV\nfile_size(\"who_longer.csv\")\n\n14.1M\n\n# Parquet\nfile_size(\"who_longer.parquet\")\n\n156K\n\nunits::set_units(file_size(\"who_longer.parquet\") / file_size(\"who_longer.csv\")) |&gt; \n  units::set_units(`%`)\n\n1.080502 [%]\n\n\n\n\nメリット2：CSVより読み込みが簡単\nwrite_csv()・write_parquet() でデータを書き込めるのと同様に，read_csv()・read_parquet() でCSV・Parquetデータを読み込むことができます．\nCSVはテキスト形式であるため，読み込み時にcol_typesで各列の型を指定する必要があります（デフォルトでは自動で型を推測）．\n一方，Parquetは，書き込み時に各列の型情報も保存されているため読み込み時に型を指定する必要がありません．\n\n# CSVの読み込み\nread_csv(\"who_longer.csv\",\n         col_types = cols(.default = \"c\",\n                          year = \"i\",\n                          count = \"i\"))\n\n# Parquetの読み込み\nread_parquet(\"who_longer.parquet\")\n\n\n\nメリット3：CSVよりビッグデータの読み込み・集計に適している\nCSVはビッグデータの保存に適しておらず，これまでは，ビッグデータの保存にはSQLを用いるなどの使い分けが必要でした．\nRでは，dplyr（dbplyr）・DBIなどのパッケージで簡単にSQLが使えますが，データベースへの接続・切断などが必要なSQLは，CSVと使い勝手が異なり，初学者にとってはハードルがあるかもしれません．\nまた，（ほとんどの？）SQLは行指向であるため，データの追加・更新・削除などに適していますが，データ分析に用いられるデータの保存・集計には列指向であるParquetのほうが適していると思われます．\nCSVファイルを用いてビッグデータを集計する場合には，一度，全データをメモリに移す必要があります．そのため，データの読み込みでメモリが逼迫するおそれがあります．\nParquetでは，読み込み時にas_data_frame = FALSEとすることで，SQLと同様にメモリにデータを移すことなくデータのフィルタリング・集計などが可能です．\nここでは，日本の年・症例別の患者数を計算してみましょう．dplyrのfilter() ・group_by() ・summarise() などを使って効率的にクエリを作成することができます．最後にcollect() を行えばデータフレームを出力することができます．\n\nread_parquet(\"who_longer.parquet\",\n             as_data_frame = FALSE) |&gt; \n  filter(country == \"Japan\",\n         !is.na(count)) |&gt; \n  group_by(country, year, diagnosis) |&gt; \n  summarise(count = sum(count),\n            .groups = \"drop\") |&gt; \n  collect()\n\n# A tibble: 33 × 4\n   country  year diagnosis count\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 Japan    1995 sp        14367\n 2 Japan    1996 sp        12867\n 3 Japan    1997 sp        13571\n 4 Japan    1998 sp        11935\n 5 Japan    2001 sp        11408\n 6 Japan    2002 sp        10807\n 7 Japan    2003 sp        10843\n 8 Japan    2004 sp        10471\n 9 Japan    1999 sp        12909\n10 Japan    2000 sp        11853\n# ℹ 23 more rows\n\n\n\n\nメリット4：複数のデータからなるデータセットを扱える\nParquetは列指向であるため，行指向であるSQLと違い，データの追加・更新・削除などに適していません．しかし，Parquetでは，複数のデータからなるデータセットの読み込みが簡単に行えるため，このようなデメリットを簡単に解決することができます．\nここでは，who_longerを年齢階級別に分割したParquetファイルを格納した\"who_longer_byage\" フォルダをデータセットのサンプルとして用いましょう．\nopen_dataset(\"who_longer_byage\") とすることで，複数のParquetファイルを含むにもかかわらず，さきほどと同様のデータ集計を簡単に行うことができます．\n\n\nコード\ndir_create(\"who_longer_byage\")\nwho_longer |&gt; \n  group_by(age) |&gt; \n  group_walk(~ .x |&gt; \n               write_parquet(str_glue(\"who_longer_byage/who_longer_{.y$age}.parquet\")),\n  .keep = TRUE)\n\n\n\nopen_dataset(\"who_longer_byage\") |&gt; \n  filter(country == \"Japan\",\n         !is.na(count)) |&gt; \n  group_by(country, year, diagnosis) |&gt; \n  summarise(count = sum(count),\n            .groups = \"drop\") |&gt; \n  collect()\n\n# A tibble: 33 × 4\n   country  year diagnosis count\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 Japan    1995 sp        14367\n 2 Japan    1996 sp        12867\n 3 Japan    1997 sp        13571\n 4 Japan    1998 sp        11935\n 5 Japan    2001 sp        11408\n 6 Japan    2002 sp        10807\n 7 Japan    2003 sp        10843\n 8 Japan    2004 sp        10471\n 9 Japan    2005 sp        10931\n10 Japan    2006 sp        10159\n# ℹ 23 more rows\n\n\n\n\nメリット5：R・Python間でのデータのやり取りに適している\nPythonのpandasパッケージはParquetの読み書きに対応しているため，Parquetは，R・Python間でのデータのやり取りにも適しています．\nRで作成した'who_longer.parquet' をpandasで読み込んでみましょう．\n\nimport pandas as pd\n\npd.read_parquet('who_longer.parquet')\n\n            country iso2 iso3    year diagnosis gender   age   count\n0       Afghanistan   AF  AFG  1980.0        sp      m   014     NaN\n1       Afghanistan   AF  AFG  1980.0        sp      m  1524     NaN\n2       Afghanistan   AF  AFG  1980.0        sp      m  2534     NaN\n3       Afghanistan   AF  AFG  1980.0        sp      m  3544     NaN\n4       Afghanistan   AF  AFG  1980.0        sp      m  4554     NaN\n...             ...  ...  ...     ...       ...    ...   ...     ...\n405435     Zimbabwe   ZW  ZWE  2013.0       rel      f  2534  4649.0\n405436     Zimbabwe   ZW  ZWE  2013.0       rel      f  3544  3526.0\n405437     Zimbabwe   ZW  ZWE  2013.0       rel      f  4554  1453.0\n405438     Zimbabwe   ZW  ZWE  2013.0       rel      f  5564   811.0\n405439     Zimbabwe   ZW  ZWE  2013.0       rel      f    65   725.0\n\n[405440 rows x 8 columns]"
  },
  {
    "objectID": "posts/2022/06/use-parquet-instead-of-csv-ja.html#まとめ",
    "href": "posts/2022/06/use-parquet-instead-of-csv-ja.html#まとめ",
    "title": "CSVの代わりにParquetを使ってみよう",
    "section": "まとめ",
    "text": "まとめ\nここまで，R・Pythonで利用可能なParquetのメリットを紹介しました．Parquetは，近年，データ分析で普及しているtidy dataの保存・集計に適しています．\nまた，最近では，地理データを扱えるsfパッケージのデータをparquetとして保存できるsfarrowなども登場しています．\nCSVの代わりにParquetを用いることでデータ分析がさらに簡単になることが期待されます．"
  },
  {
    "objectID": "posts/2023/01/create-an-animation-of-a-population-pyramid-in-r.html",
    "href": "posts/2023/01/create-an-animation-of-a-population-pyramid-in-r.html",
    "title": "Rで人口ピラミッドのアニメーションを作る",
    "section": "",
    "text": "国立社会保障・人口問題研究所（社人研）の公開している人口ピラミッドの推移アニメーションを参考に，以下のようなアニメーションをRのggplot2で作成してみました．"
  },
  {
    "objectID": "posts/2023/01/create-an-animation-of-a-population-pyramid-in-r.html#この記事について",
    "href": "posts/2023/01/create-an-animation-of-a-population-pyramid-in-r.html#この記事について",
    "title": "Rで人口ピラミッドのアニメーションを作る",
    "section": "",
    "text": "国立社会保障・人口問題研究所（社人研）の公開している人口ピラミッドの推移アニメーションを参考に，以下のようなアニメーションをRのggplot2で作成してみました．"
  },
  {
    "objectID": "posts/2023/01/create-an-animation-of-a-population-pyramid-in-r.html#国勢調査の時系列データの取得",
    "href": "posts/2023/01/create-an-animation-of-a-population-pyramid-in-r.html#国勢調査の時系列データの取得",
    "title": "Rで人口ピラミッドのアニメーションを作る",
    "section": "国勢調査の時系列データの取得",
    "text": "国勢調査の時系列データの取得\nまず，e-Statで公開されている国勢調査の男女・5最階級別人口の時系列データをRのjpstatパッケージを用いて取得します．\njpstatパッケージからe-Stat APIを用いるためには，アプリケーションID（appId）を取得する必要があります．\n\nlibrary(tidyverse)\nlibrary(jpstat)\n\n\n# ご自身のappIdに置き換えてください\nSys.setenv(ESTAT_API_KEY = \"Your appId\")\n\n\ncensus &lt;- estat(statsDataId = \"https://www.e-stat.go.jp/dbview?sid=0003410380\")\n\npop &lt;- census |&gt; \n  activate(tab) |&gt; \n  filter(name == \"人口\") |&gt; \n  select() |&gt; \n  \n  # 性別の抽出\n  activate(cat01) |&gt; \n  rekey(\"sex\") |&gt; \n  filter(name %in% c(\"男\", \"女\")) |&gt; \n  select(name) |&gt; \n  \n  # 年齢階級の抽出\n  activate(cat02) |&gt; \n  rekey(\"ageclass\") |&gt; \n  filter(str_detect(name, \"^\\\\d+～\\\\d+歳$\") |\n           name == \"85歳以上\" |\n           name == \"110歳以上\") |&gt; \n  select(name) |&gt; \n  \n  # 年の抽出\n  activate(time) |&gt; \n  rekey(\"year\") |&gt; \n  filter(str_detect(name, \"^\\\\d+年$\")) |&gt; \n  select(name) |&gt; \n  \n  # e-Statデータの取得\n  collect(n = \"pop\") |&gt; \n  \n  rename_with(~ .x |&gt; \n                str_remove(\"_name$\")) |&gt; \n  mutate(sex = as_factor(sex),\n         year = parse_number(year),\n         \n         # 各年齢階級の最低年齢を取得\n         age_from = ageclass |&gt; \n           str_extract(\"^\\\\d+\") |&gt; \n           stringi::stri_trans_nfkc() |&gt; \n           as.integer(),\n         \n         # 最高の年齢階級を「85歳以上」とする\n         ageclass = case_when(age_from &gt;= 85 ~ \"85歳以上\",\n                              TRUE ~ ageclass) |&gt; \n           as_factor(),\n         \n         # 年齢層を追加\n         agegroup = case_when(between(age_from, 0, 10) ~ \"年少人口\",\n                              between(age_from, 15, 60) ~ \"生産年齢人口\",\n                              between(age_from, 65, 70) ~ \"前期老年人口\",\n                              age_from &gt;= 75 ~ \"後期老年人口\") |&gt; \n           as_factor(),\n         pop = parse_number(pop)) |&gt; \n  \n  # 「85歳以上」人口を集計\n  group_by(year, sex, ageclass, agegroup) |&gt; \n  summarise(pop = sum(pop),\n            .groups = \"drop\")\n\nThe total number of data is 796.\n\nknitr::kable(head(pop))\n\n\n\n\nyear\nsex\nageclass\nagegroup\npop\n\n\n\n\n1920\n男\n０～４歳\n年少人口\n3752627\n\n\n1920\n男\n５～９歳\n年少人口\n3467156\n\n\n1920\n男\n10～14歳\n年少人口\n3089225\n\n\n1920\n男\n15～19歳\n生産年齢人口\n2749022\n\n\n1920\n男\n20～24歳\n生産年齢人口\n2316479\n\n\n1920\n男\n25～29歳\n生産年齢人口\n2008005"
  },
  {
    "objectID": "posts/2023/01/create-an-animation-of-a-population-pyramid-in-r.html#人口ピラミッドアニメーションの作成",
    "href": "posts/2023/01/create-an-animation-of-a-population-pyramid-in-r.html#人口ピラミッドアニメーションの作成",
    "title": "Rで人口ピラミッドのアニメーションを作る",
    "section": "人口ピラミッドアニメーションの作成",
    "text": "人口ピラミッドアニメーションの作成\nRのgganimateパッケージを用いることでggplot2のグラフをアニメーションにすることができます．\n\nlibrary(gganimate)\n\nWarning: package 'gganimate' was built under R version 4.3.1\n\n# 年齢層を表示するためのデータ\nagegroup &lt;- pop |&gt; \n  group_by(sex, agegroup) |&gt;\n  summarise(ageclass = mean(as.integer(ageclass)),\n            .groups = \"drop\") |&gt; \n  mutate(hjust = case_when(sex == \"男\" ~ 1.25,\n                           sex == \"女\" ~ -0.25))\n\npoppyramid &lt;- pop |&gt; \n  \n  # 人口ピラミッドを作成するため男性人口をマイナスに変換\n  mutate(pop = if_else(sex == \"男\", -pop, pop)) |&gt; \n  \n  ggplot(aes(ageclass, pop,\n             group = sex,\n             fill = agegroup)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(data = agegroup,\n            aes(label = agegroup,\n                hjust = hjust),\n            y = 0) +\n  scale_x_discrete(NULL) +\n  scale_y_continuous(\"人口［千人］\",\n                     \n                     # ラベルを絶対値・千人単位に変換\n                     labels = purrr::compose(scales::label_comma(scale = 1e-3), abs)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  coord_flip() +\n  facet_wrap(~ sex,\n             scales = \"free_x\") +\n  labs(title = \"{frame_time %/% 5 * 5}年\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  \n  # アニメーションに変換\n  transition_time(year)\n\n# 幅・高さを変更\nanimate(poppyramid, \n        width = 800,\n        height = 600,\n        res = 150,\n        renderer = gifski_renderer())"
  },
  {
    "objectID": "posts/2023/04/quantify-vote-disparity-between-generations.html",
    "href": "posts/2023/04/quantify-vote-disparity-between-generations.html",
    "title": "世代間の「1票の格差」を定量化してみる",
    "section": "",
    "text": "ちょうど選挙シーズンですので1，オープンデータを使って，しばしば問題になる「1票の格差」を定量化してみたいと思います．\n「1票の格差」は，通常，議員1人あたりの有権者数が地域（選挙区）によって異なる（≒議員1人を当選させるために必要な票数が地域によって異なる）ことを指します．たとえば，2022年の参議院選挙では，「1票の格差」が最大で3.03倍であったとされています2．\n現在，日本の都市部と地方は以下の表のような関係にあるとされ，「1票の格差」が「地方で新幹線が整備される一方で東京の通勤ラッシュが解消されない」といった経済効率性の低下などを引き起こすことが問題視されています3．\n\n\n\n\n\n\n\n\n地域\nお金の流れ（地方交付税など）\n政治的平等（「1票の格差」）\n\n\n\n\n都市部\n地方にお金を「支払う」\n「1票の価値」が軽く\n都市部「冷遇」\n\n\n地方\n都市部からお金を「受け取る」\n「1票の価値」が重く\n地方「優遇」\n\n\n\n一方で，今後，東京一極集中などにより地方の社会課題が一層深刻になることが予想されます．そのため，「1票の価値」を完全に等しくすれば，むしろ地方の「冷遇」につながるといった懸念もあります．\nこのように，「1票の格差」の是正は，決して簡単な道のりとはいえませんが，引き続き是正に向けた取り組みを進めていく必要があるといえるでしょう．\n\n\nここまで紹介した地域間の「1票の格差」に加えて，世代間にも「1票の格差」が存在するといわれています4．これは，少子高齢化により，若年層の有権者数に対して高齢者層の有権者数が多くなることにより引き起こされるものです5．\nさきほど示した都市・地方部の関係と同様に，若年層と高齢者層は，以下の表のような関係にあると考えられます．地方における人口減少・人手不足が深刻となった最近になってようやく，国は，少子化対策を国の最重要課題に位置づけましたが，これまで，こうした若年層向けの政策が十分に行われなかった6のは世代間の「1票の格差」の影響によるものかもしれません．\n\n\n\n\n\n\n\n\n世代\nお金の流れ（社会保障費など）\n政治的平等（「1票の格差」）\n\n\n\n\n若年層\n高齢者層にお金を「支払う」\n「1票の価値」が軽く\n若年層「冷遇」\n\n\n高齢者層\n若年層からお金を「受け取る」\n「1票の価値」が重く\n高齢者層「優遇」"
  },
  {
    "objectID": "posts/2023/04/quantify-vote-disparity-between-generations.html#票の格差とは",
    "href": "posts/2023/04/quantify-vote-disparity-between-generations.html#票の格差とは",
    "title": "世代間の「1票の格差」を定量化してみる",
    "section": "",
    "text": "ちょうど選挙シーズンですので1，オープンデータを使って，しばしば問題になる「1票の格差」を定量化してみたいと思います．\n「1票の格差」は，通常，議員1人あたりの有権者数が地域（選挙区）によって異なる（≒議員1人を当選させるために必要な票数が地域によって異なる）ことを指します．たとえば，2022年の参議院選挙では，「1票の格差」が最大で3.03倍であったとされています2．\n現在，日本の都市部と地方は以下の表のような関係にあるとされ，「1票の格差」が「地方で新幹線が整備される一方で東京の通勤ラッシュが解消されない」といった経済効率性の低下などを引き起こすことが問題視されています3．\n\n\n\n\n\n\n\n\n地域\nお金の流れ（地方交付税など）\n政治的平等（「1票の格差」）\n\n\n\n\n都市部\n地方にお金を「支払う」\n「1票の価値」が軽く\n都市部「冷遇」\n\n\n地方\n都市部からお金を「受け取る」\n「1票の価値」が重く\n地方「優遇」\n\n\n\n一方で，今後，東京一極集中などにより地方の社会課題が一層深刻になることが予想されます．そのため，「1票の価値」を完全に等しくすれば，むしろ地方の「冷遇」につながるといった懸念もあります．\nこのように，「1票の格差」の是正は，決して簡単な道のりとはいえませんが，引き続き是正に向けた取り組みを進めていく必要があるといえるでしょう．\n\n\nここまで紹介した地域間の「1票の格差」に加えて，世代間にも「1票の格差」が存在するといわれています4．これは，少子高齢化により，若年層の有権者数に対して高齢者層の有権者数が多くなることにより引き起こされるものです5．\nさきほど示した都市・地方部の関係と同様に，若年層と高齢者層は，以下の表のような関係にあると考えられます．地方における人口減少・人手不足が深刻となった最近になってようやく，国は，少子化対策を国の最重要課題に位置づけましたが，これまで，こうした若年層向けの政策が十分に行われなかった6のは世代間の「1票の格差」の影響によるものかもしれません．\n\n\n\n\n\n\n\n\n世代\nお金の流れ（社会保障費など）\n政治的平等（「1票の格差」）\n\n\n\n\n若年層\n高齢者層にお金を「支払う」\n「1票の価値」が軽く\n若年層「冷遇」\n\n\n高齢者層\n若年層からお金を「受け取る」\n「1票の価値」が重く\n高齢者層「優遇」"
  },
  {
    "objectID": "posts/2023/04/quantify-vote-disparity-between-generations.html#世代間の1票の格差を定量化してみる",
    "href": "posts/2023/04/quantify-vote-disparity-between-generations.html#世代間の1票の格差を定量化してみる",
    "title": "世代間の「1票の格差」を定量化してみる",
    "section": "世代間の「1票の格差」を定量化してみる",
    "text": "世代間の「1票の格差」を定量化してみる\n地域間の「1票の格差」は，（都市部基準で）約〇倍といった定量化がなされることが一般的ですが，世代間の「1票の格差」については，これまで，あまり定量化されていないようです．\n単純な定量化の方法としては，年齢層別の人口を何らかの基準人口で割るといったものが考えられますが，このような方法では，加齢により亡くなる方がいることが考慮されず，「人口が少ないので100歳以上の1票の価値が軽い」といった評価をすることになってしまいます．\nそこで，2020年国勢調査人口と2020年生命表を用いて，世代間の「1票の格差」を定量化してみようと思います．生命表とは，「ある期間における死亡状況が今後変化しないとの仮定のもとで各年齢の者が1年以内に死亡する確率などの指標を表したもの」です7．\n以下で示す世代間の「1票の格差」は，あくまで，オープンデータに基づく簡易的な試算であることにご注意ください．\n\n使用データについて\n今回は，以下の表に示すデータを使用します．ここで，表中の定常人口（nLx）とは，「一定の出生・（生命表上の）死亡率のもとで得られる年齢階級別人口」のことです8．\nデータ入手・整形方法については，下に折りたたまれているコードも参考にしてください．\n\n\n\n内容\n出典\n入手方法\n\n\n\n\n男女・年齢別人口\n2020年国勢調査\njpstatパッケージで\ne-Stat APIを使用\n\n\n男女・年齢別定常人口（nLx）\n2020年生命表\nURLからエクセルファイルを\nダウンロード\n\n\n\n\n\nパッケージのローディングなど\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(arrow)\nlibrary(readxl)\nlibrary(jpstat)\n\ntheme_set(theme_light())\n\ndir_create(\"quantify-vote-disparity-between-generations\")\n\n\n\n\n2020年国勢調査人口のダウンロード・データ整形\n# jpstatパッケージを使って2020年国勢調査データをダウンロードしています\nif (!file_exists(\"quantify-vote-disparity-between-generations/pop_2020.parquet\")) {\n  # 注意: jpstatパッケージはe-Stat APIを利用するためAPIキーが必要です\n  census_2020 &lt;- estat(keyring::key_get(\"estat-api\"),\n                       \"https://www.e-stat.go.jp/dbview?sid=0003445139\")\n\n  pop_2020 &lt;- census_2020 |&gt;\n    activate(tab) |&gt;\n    select() |&gt;\n  \n    activate(cat01) |&gt;\n    filter(name == \"うち日本人\") |&gt;\n    select() |&gt;\n  \n    activate(cat02) |&gt;\n    rekey(\"sex\") |&gt;\n    filter(name %in% c(\"男\", \"女\")) |&gt;\n    select(name) |&gt;\n  \n    activate(cat03) |&gt;\n    rekey(\"age\") |&gt;\n    select(name) |&gt;\n  \n    activate(area) |&gt;\n    filter(name == \"全国\") |&gt;\n    select() |&gt;\n  \n    activate(time) |&gt;\n    select() |&gt;\n  \n    collect(n = \"pop\")\n  \n  pop_2020 &lt;- pop_2020 |&gt; \n    filter(str_detect(age_name, \"^\\\\d+歳$\") | age_name == \"100歳以上\") |&gt; \n    mutate(sex = as_factor(sex_name),\n           age = case_when(str_detect(age_name, \"^\\\\d+歳$\") ~ age_name |&gt; \n                             str_extract(\"\\\\d+\"),\n                           age_name == \"100歳以上\" ~ \"100--Inf\") |&gt; \n             as_factor(),\n           pop = parse_number(pop),\n           .keep = \"unused\") |&gt; \n    relocate(sex, age, pop)\n  \n  write_parquet(pop_2020, \"quantify-vote-disparity-between-generations/pop_2020.parquet\") \n}\n\n\n\n\n2020年生命表のダウンロード\n# 第23回生命表（男）\nfile_lifetable_male_2020 &lt;- \"quantify-vote-disparity-between-generations/lifetable_male_2020.xlsx\"\nif (!file_exists(file_lifetable_male_2020)) {\n  curl::curl_download(\"https://www.e-stat.go.jp/stat-search/file-download?statInfId=000032173232&fileKind=0\", file_lifetable_male_2020) \n}\n\n# 第23回生命表（女）\nfile_lifetable_female_2020 &lt;- \"quantify-vote-disparity-between-generations/lifetable_female_2020.xlsx\"\nif (!file_exists(file_lifetable_female_2020)) {\n  curl::curl_download(\"https://www.e-stat.go.jp/stat-search/file-download?statInfId=000032173233&fileKind=0\", file_lifetable_female_2020)\n}\n\n\n\n\n2020年生命表の定常人口のデータ整形\ncol_names_lifetable &lt;- read_excel(file_lifetable_male_2020, \n                        range = \"B3:J5\",\n                        col_names = as.character(1:9)) |&gt; \n  t() |&gt; \n  as_tibble(.name_repair = \\(x) c(\"col_name_1\", \"col_name_2\", \"col_name_3\")) |&gt; \n  unite(\"col_name\", starts_with(\"col_name\"),\n        na.rm = TRUE) |&gt; \n  mutate(col_name = col_name |&gt; \n           str_remove_all(\"\\\\s\")) |&gt; \n  pull(col_name)\n\nrange_lifetable &lt;- \"B6:J127\"\n\nlifetable_male_2020 &lt;- read_excel(file_lifetable_male_2020,\n                                  range = range_lifetable,\n                                  col_names = col_names_lifetable)\n\nlifetable_female_2020 &lt;- read_excel(file_lifetable_female_2020,\n                                    range = range_lifetable,\n                                    col_names = col_names_lifetable)\n\nstatic_pop_2020 &lt;- list(男 = lifetable_male_2020,\n                        女 = lifetable_female_2020) |&gt; \n  bind_rows(.id = \"sex\") |&gt; \n  rename(age = 年齢_x,\n         static_pop = 定常人口_nLx_人) |&gt; \n  select(sex, age, static_pop) |&gt; \n  filter(str_ends(age, \"年\")) |&gt; \n  mutate(sex = as_factor(sex),\n         age = age |&gt; \n           str_extract(\"^\\\\d+\") |&gt; \n           parse_integer(),\n         # 100歳以上はまとめる\n         age = if_else(age &gt;= 100,\n                       \"100--Inf\",\n                       as.character(age)) |&gt; \n           as_factor()) |&gt; \n  summarise(static_pop = sum(static_pop),\n            .by = c(sex, age))\n\nwrite_parquet(static_pop_2020, \"quantify-vote-disparity-between-generations/static_pop_2020.parquet\")\n\n\n\n\n国勢調査の人口・生命表の定常人口の「人口ピラミッド」の比較\n入手・整形したデータを使って人口ピラミッドを作成してみましょう．以下のグラフは，2020年国勢調査の人口に生命表の定常人口（赤線）を重ねたものです9．ただし，定常人口は，0歳定常人口が2020年国勢調査の0歳人口と等しくなるように基準化しています10．また，100歳以上の人口・定常人口については，100歳に集計して表示しています．\n\n\n人口ピラミッドの図示\n# 100歳以上を数値100で置き換える\nas_integer_age &lt;- function(age) {\n  if_else(age == \"100--Inf\",\n          100,\n          parse_integer(as.character(age),\n                        na = \"100--Inf\"))\n}\n\npop_2020 &lt;- read_parquet(\"quantify-vote-disparity-between-generations/pop_2020.parquet\") |&gt; \n  mutate(age = as_integer_age(age))\nstatic_pop_2020 &lt;- read_parquet(\"quantify-vote-disparity-between-generations/static_pop_2020.parquet\") |&gt; \n  mutate(age = as_integer_age(age))\n\nratio_pop_static_pop &lt;- static_pop_2020 |&gt; \n  filter(age == 0) |&gt; \n  select(!age) |&gt; \n  left_join(pop_2020 |&gt; \n              filter(age == 0) |&gt; \n              select(!age),\n            by = join_by(sex)) |&gt; \n  mutate(ratio_pop_static_pop = pop / static_pop,\n         .keep = \"unused\")\n\nstatic_pop_2020 &lt;- static_pop_2020 |&gt; \n  left_join(ratio_pop_static_pop,\n            by = join_by(sex)) |&gt; \n  mutate(pop = static_pop * ratio_pop_static_pop,\n         .keep = \"unused\")\n\npop_2020 |&gt;\n  mutate(pop = if_else(sex == \"男\",\n                       -pop,\n                       pop)) |&gt; \n  ggplot(aes(age, pop,\n             fill = sex)) +\n  geom_col() +\n  geom_line(data = static_pop_2020 |&gt; \n               mutate(pop = if_else(sex == \"男\",\n                                    -pop,\n                                    pop)),\n            aes(group = sex,\n                color = \"定常人口\\n（参考値）\")) +\n  scale_x_continuous(\"年齢\",\n                     breaks = seq(0, 100, 10)) +\n  scale_y_continuous(\"人口［千人］\",\n                     labels = \\(x) {\n                       scales::label_comma(scale = 1e-3)(abs(x))\n                     }) +\n  scale_fill_manual(\"性別\",\n                    values = c(男 = \"cornflowerblue\",\n                               女 = \"lightcoral\")) +\n  scale_color_manual(NULL,\n                     values = c(`定常人口\\n（参考値）` = \"red\")) +\n  coord_flip() +\n  guides(fill = guide_legend(order = 1),\n         color = guide_legend(order = 2))\n\n\n\n\n\n\n\n\n\n2020年の人口ピラミッドより，70歳前半および40歳後半において人口のピークがあることがわかります．これらは，それぞれ「団塊世代」「団塊ジュニア」と呼ばれる世代にあたります11．一方で，若年層の人口は，「団塊世代」「団塊ジュニア」の人口を大きく下回っていることがわかり，近年の少子化の深刻さがみてとれます．\n\n\n「人口ピラミッド」に基づく世代間の「1票の格差」の定量化\n赤線の定常人口（参考値）に対する国勢調査人口の比を用いて，世代間の「1票の格差」を定量化してみましょう．\n以下のグラフは，選挙権を有する18歳以上を対象に，おおよそ10歳間隔で世代間の「1票の格差」を定量化したものです．ただし，ここでは18～29歳の「1票の価値」を1として基準化しています．\nグラフより，40代～60代の1票は，18～29歳の約1.5倍，70代の1票は，18～29歳の約1.7倍の価値を有するという試算が得られました．また，（生命表の性質によるものかもしれませんが）「団塊世代」「団塊ジュニア」ではない30代や80代以上の1票の価値は，40代～60代・70代と比べると低くなっていることがわかりました．\n\n\n世代間の「1票の格差」の定量化\nvote_disparity &lt;- pop_2020 |&gt; \n  left_join(static_pop_2020 |&gt; \n              rename(static_pop = pop),\n            by = join_by(sex, age)) |&gt;\n  filter(age &gt;= 18) |&gt; \n  mutate(ageclass = case_when(between(age, 18, 29) ~ \"18～29歳\",\n                              between(age, 30, 39) ~ \"30代\",\n                              between(age, 40, 49) ~ \"40代\",\n                              between(age, 50, 59) ~ \"50代\",\n                              between(age, 60, 69) ~ \"60代\",\n                              between(age, 70, 79) ~ \"70代\",\n                              between(age, 80, 89) ~ \"80代\",\n                              90 &lt;= age ~ \"90代以上\")) |&gt; \n  summarise(across(c(pop, static_pop),\n                   sum),\n            .by = ageclass) |&gt; \n  mutate(vote_disparity = pop / static_pop,\n         .keep = \"unused\")\n\nvote_disparity &lt;- vote_disparity |&gt; \n  bind_cols(vote_disparity |&gt; \n              filter(ageclass == \"18～29歳\") |&gt; \n              select(!ageclass) |&gt; \n              rename(vote_disparity_18to29 = vote_disparity)) |&gt; \n  mutate(vote_disparity = vote_disparity / vote_disparity_18to29,\n         .keep = \"unused\")\n\nplot_vote_disparity &lt;- vote_disparity |&gt; \n  ggplot(aes(ageclass, vote_disparity)) +\n  geom_col(fill = \"lightblue\") +\n  geom_text(aes(label = scales::label_comma(accuracy = 1e-2)(vote_disparity)),\n            y = 1,\n            vjust = -0.5,\n            color = \"red\") +\n  geom_hline(yintercept = 1,\n             color = \"red\",\n             linetype = \"dashed\") +\n  scale_x_discrete(\"年齢層\") +\n  scale_y_continuous(\"世代間の「1票の格差」（18～29歳基準）\")\n\n\nggsave(\"quantify-vote-disparity-between-generations/plot_vote_disparity.png\",\n       plot = plot_vote_disparity)\n\nplot_vote_disparity"
  },
  {
    "objectID": "posts/2023/04/quantify-vote-disparity-between-generations.html#まとめ",
    "href": "posts/2023/04/quantify-vote-disparity-between-generations.html#まとめ",
    "title": "世代間の「1票の格差」を定量化してみる",
    "section": "まとめ",
    "text": "まとめ\nオープンデータである国勢調査と生命表を用いて，世代間の「1票の格差」を定量化してみました．\n実は，若者の投票率の低さも世代間の「1票の格差」の大きな要因となっているといわれています．今後は，世代間の「1票の格差」に対する議論を深めつつ，若者の投票率が上がるような魅力的な政策を多く打ち出していくことが必要であると考えられます．"
  },
  {
    "objectID": "posts/2023/04/quantify-vote-disparity-between-generations.html#footnotes",
    "href": "posts/2023/04/quantify-vote-disparity-between-generations.html#footnotes",
    "title": "世代間の「1票の格差」を定量化してみる",
    "section": "脚注",
    "text": "脚注\n\n\n地方公共団体の議員・長を決める第20回統一地方選挙が，2023年4月9日（日）と2023年4月23日（日）に行われました．↩︎\n参院選 “1票の格差 最大3.03倍” 最高裁が大法廷で審理へ↩︎\n「一票の格差」の問題点とは？↩︎\n世代間における「1票の格差」↩︎\nこういった考え方は，東京一極集中に伴う都市部への人口集積のもとで，「1票の価値」を完全に等しくすれば，むしろ地方「冷遇」につながるといった考え方とも類似します．↩︎\n選択する未来 （4）少子化対策↩︎\n生命表について↩︎\n参考資料１生命表諸関数の定義↩︎\n赤線で示した定常人口（参考値）は，2020年の0歳の男女が，死亡率一定の仮定のもとで，ある年齢で何人生存しているかを表しています．↩︎\n0歳人口の男女比は，おおよそ105：100となっています．↩︎\n「人口ピラミッド」から日本の未来が見えてくる！？～高齢化と「団塊世代」、少子化と「団塊ジュニア」～↩︎"
  },
  {
    "objectID": "posts/2023/09/circle-area-by-numerical-integration-in-r.html",
    "href": "posts/2023/09/circle-area-by-numerical-integration-in-r.html",
    "title": "Rの数値積分で円の面積を求める",
    "section": "",
    "text": "最近，R statsパッケージ1のintegrate() 関数で一次元関数の数値積分ができることを知りました．そこで，この記事では，こちらの記事を参考に積分で円の面積を計算してみました．"
  },
  {
    "objectID": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#はじめに",
    "href": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#はじめに",
    "title": "Rの数値積分で円の面積を求める",
    "section": "",
    "text": "最近，R statsパッケージ1のintegrate() 関数で一次元関数の数値積分ができることを知りました．そこで，この記事では，こちらの記事を参考に積分で円の面積を計算してみました．"
  },
  {
    "objectID": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#半円を描く関数",
    "href": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#半円を描く関数",
    "title": "Rの数値積分で円の面積を求める",
    "section": "半円を描く関数",
    "text": "半円を描く関数\n半径\\(r\\)の半円を描く関数は 式 1 と表せます．Rで書くとsemicircle() 関数のようになります．\n\\[\ny = \\sqrt{r^2 - x^2} \\qquad (-r \\le x \\le r)\n\\tag{1}\\]\n\nsemicircle &lt;- function(x, radius) {\n  sqrt(radius^2 - x^2)\n}\n\n実装したsemicircle() 関数で半径\\(r = 5\\)の半円を描いてみましょう．\n\nradius &lt;- 5\ncurve(semicircle(x, radius), -radius, radius,\n      asp = 1) # アスペクト比を1:1にする"
  },
  {
    "objectID": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#数値積分で円の面積を求める",
    "href": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#数値積分で円の面積を求める",
    "title": "Rの数値積分で円の面積を求める",
    "section": "数値積分で円の面積を求める",
    "text": "数値積分で円の面積を求める\n次に，stats::integrate() 関数を用いて，さきほど書いたsemicircle() を数値積分してみましょう．半径\\(r = 5\\)の場合，積分すると半円の面積\\(\\frac{1}{2}\\pi r^2\\)とほぼ等しくなることが確認できます．\n\n# 半径を5とする\nradius &lt;- 5\nintegrate(semicircle, -radius, radius, # -radiusからradiusまでの範囲で積分する\n          radius = radius)\n\n39.26991 with absolute error &lt; 2.5e-08\n\npi * 5^2 / 2\n\n[1] 39.26991\n\n\nそのため，円の面積を近似的に求めるcircle_area_approx()が以下のように書けます（数値積分の結果は，stats::integrate() 関数の戻り値のvalue に格納されています）．\n\ncircle_area_approx &lt;- function(radius) {\n  out &lt;- integrate(semicircle, -radius, radius,\n                   radius = radius)\n  out$value * 2\n}\n\ncircle_area_approx(5)\n\n[1] 78.53982\n\n\n最後に，半径\\(1 \\le r \\le 10\\)の範囲で近似値circle_area_approx()と理論値circle_area_true()（\\(\\pi r^2\\)）を比較してみましょう．\n計算の結果，近似値と理論値で円の面積がほぼ等しいことが確認できました．\n\ncircle_area_true &lt;- function(radius) {\n  pi * radius ^ 2\n}\n\ndata &lt;- data.frame(radius = 1:10)\ndata$circle_area_approx &lt;- sapply(data$radius, circle_area_approx)\ndata$circle_area_true &lt;- sapply(data$radius, circle_area_true)\nknitr::kable(data)\n\n\n\n\nradius\ncircle_area_approx\ncircle_area_true\n\n\n\n\n1\n3.141593\n3.141593\n\n\n2\n12.566371\n12.566371\n\n\n3\n28.274334\n28.274334\n\n\n4\n50.265482\n50.265482\n\n\n5\n78.539816\n78.539816\n\n\n6\n113.097335\n113.097335\n\n\n7\n153.938040\n153.938040\n\n\n8\n201.061930\n201.061930\n\n\n9\n254.469005\n254.469005\n\n\n10\n314.159265\n314.159265"
  },
  {
    "objectID": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#おわりに",
    "href": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#おわりに",
    "title": "Rの数値積分で円の面積を求める",
    "section": "おわりに",
    "text": "おわりに\nstats::integrate() 関数を使えば，簡単に一次元関数の数値積分ができることがわかりました．\n\n注意点\nstats::integrate() 関数では，\\(-\\infty \\le x \\le \\infty\\)の範囲での数値積分も可能ですが，\\(10^{-6} \\le x \\le 10^6\\)のような大きな値を代入すると数値積分が適切に行われませんのでご注意ください．\n\n# OK\nintegrate(dnorm, -Inf, Inf)\n\n1 with absolute error &lt; 9.4e-05\n\n# NG\nintegrate(dnorm, -1e6, 1e6)\n\n0 with absolute error &lt; 0"
  },
  {
    "objectID": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#footnotes",
    "href": "posts/2023/09/circle-area-by-numerical-integration-in-r.html#footnotes",
    "title": "Rの数値積分で円の面積を求める",
    "section": "脚注",
    "text": "脚注\n\n\nstatsパッケージは，Rにデフォルトで入っているパッケージの一つです．↩︎"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm-en.html",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm-en.html",
    "title": "Improve the quality of data preprocessing with R’s dm package",
    "section": "",
    "text": "Note: This article is a machine-translated translation of my Japanese article with simple modifications."
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#how-do-we-reduce-errors-in-data-preprocessing",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#how-do-we-reduce-errors-in-data-preprocessing",
    "title": "Improve the quality of data preprocessing with R’s dm package",
    "section": "How do we reduce errors in data preprocessing?",
    "text": "How do we reduce errors in data preprocessing?\nIt is said that 80% of the time required for data analysis is spent on preprocessing. Even though preprocessing determines the quality of subsequent data analysis, the more time spent on pre-processing means the probability of making mistakes is also high.\nPreprocessing tasks include “extraction,” “aggregation,” and “joining (merging)” of data frames. Among these, data joining is a task that tends to make the code longer and more prone to errors. And since the data frames needed for analysis are rarely combined into one, joining data frames is a widespread process.\nTypical mistakes that tend to occur when joining data frames are as follows1,\n\nThe keys of the data frame to be joined are not “MECE”\nMistaken keys to join data frames\n\nIf you have experienced any mistakes or near-misses in joining data frames, you may be better off relying on some sort of package instead of letting them go unattended. Besides, even if there were no mistakes, we tend to be skeptical about whether we did the right thing in the past.\nThis article introduces the R dm package, a powerful solution to the problems of errors and skepticism in joining data frames."
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#relational-data-model-with-dm",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#relational-data-model-with-dm",
    "title": "Improve the quality of data preprocessing with R’s dm package",
    "section": "Relational data model with dm",
    "text": "Relational data model with dm\nThe relational data model provided by dm manages the relationships among multiple data on our behalf.\nThis article describes the use and advantages of the relational data model provided by dm. the Star Wars data set provided by the repurrrsive package. For this purpose, let’s use the Star Wars dataset provided by the repurrrsive package. tidyverse, dm, and repurrrsive packages must be pre-loaded.\n\nlibrary(tidyverse)\nlibrary(dm)\nlibrary(repurrrsive)\n\nThe Star Wars dataset provided by repurrrsive includes the followings2.\n\n\nStarWars dataset provided by repurrrsive\ndata(package = \"repurrrsive\") |&gt; \n  chuck(\"results\") |&gt; \n  as_tibble() |&gt; \n  filter(str_starts(Item, \"sw_\")) |&gt; \n  pull(Item)\n\n\n[1] \"sw_films\"     \"sw_people\"    \"sw_planets\"   \"sw_species\"   \"sw_starships\"\n[6] \"sw_vehicles\" \n\n\nThis article will illustrate the use of the relational data model through the following two analytical examples. These examples require dealing with relationships among multiple data sets.\n\nBasic: Building a relational data model with dm\nApplication: Extending the model and validating keys with dm\n\n\n1. Basic: Building a relational data model with dm\nAs a basic example, let us build a relational data model to determine the composition ratio of the home planets of the characters in Star Wars movies. To perform this analysis, three data frames, films, people, and planets, are prepared as shown below. Here, only the necessary data are selected to simplify the analysis3．\n\nfilms &lt;- tibble(film = sw_films) |&gt; \n  unnest_wider(film) |&gt; \n  select(url, title, characters)\npeople &lt;- tibble(person = sw_people) |&gt;\n  unnest_wider(person) |&gt; \n  select(url, name, homeworld, species)\nplanets &lt;- tibble(planet = sw_planets) |&gt;\n  unnest_wider(planet) |&gt; \n  select(url, name)\n\n\nfilms\n\n\n  \n\n\npeople\n\n\n  \n\n\nplanets\n\n\n  \n\n\n\nChecking the prepared data frames, we can see that url column of each data frame is used as a key, so the relationship between the data frames can be summarized as shown in the following figure4.\nSince it is common for a film to have multiple characters, it is important to note that characters column of films is a list of characters, so it isn’t possible to join characters column of films to url column of people.\n\n\n\n\n\nflowchart TB\n  films.characters --&gt; people.url\n  people.homeworld --&gt; planets.url\n  subgraph films\n    films.url[url]\n    films.title[title]\n    films.characters[List of characters] \n  end\n  subgraph people\n    people.url[url]\n    people.name[name]\n    people.homeworld[homeworld]\n  end\n  subgraph planets\n    planets.url[url]\n    planets.name[name]\n  end\n\n\n\n\n\n\nTherefore, we consider creating a new data file, films_x_characters, that represents the relationship between films and people, i.e., which characters appear in which films5．Through films_x_characters, the relationship between data can be summarized as shown in the following figure.\n\n\n\n\n\nflowchart TB\n  films_x_characters.url --&gt; films.url\n  films_x_characters.characters ---&gt; people.url\n  people.homeworld --&gt; planets.url\n  subgraph films_x_characters\n    films_x_characters.url[url]\n    films_x_characters.characters[characters]\n  end\n  subgraph films\n    films.url[url]\n    films.title[title]\n  end\n  subgraph people\n    people.url[url]\n    people.name[name]\n    people.homeworld[homeworld]\n  end\n  subgraph planets\n    planets.url[url]\n    planets.name[name]\n  end\n\n\n\n\n\n\nNow, let’s build a relational data model according to the above image. First, create films_x_characters using url and characters columns of films. In addition, I delete the unnecessary characters column from films.\n\n# Create films_x_characters and remove characters column from films\nfilms_x_characters &lt;- films |&gt; \n  select(url, characters) |&gt; \n  unnest_longer(characters)\nfilms &lt;- films |&gt; \n  select(!characters)\n\nfilms_x_characters\n\n\n  \n\n\n\nFinally, after passing the prepared films, people, planets, and films_x_characters to dm(), a relational data model can be built by adding primary keys and foreign keys.\nIn dm, primary keys are set with dm_add_pk()6 and foreign keys with dm_add_fk()7.\n\ndm_starwars_1 &lt;- dm(films, people, planets, films_x_characters) |&gt; \n  \n  # 1. Add primary keys\n  dm_add_pk(films, url) |&gt;\n  dm_add_pk(people, url) |&gt;\n  dm_add_pk(planets, url) |&gt;\n  dm_add_pk(films_x_characters, c(url, characters)) |&gt;\n  \n  # 2. Add foreign keys\n  dm_add_fk(films_x_characters, url, films) |&gt; \n  dm_add_fk(films_x_characters, characters, people) |&gt;\n  dm_add_fk(people, homeworld, planets) \n\ndm_starwars_1\n\n── Metadata ────────────────────────────────────────────────────────────────────\nTables: `films`, `people`, `planets`, `films_x_characters`\nColumns: 10\nPrimary keys: 4\nForeign keys: 3\n\n\nThe relational data model can be drawn using dm_draw(). The drawing reveals that the same relationships are built as in the image above.\n\ndm_draw(dm_starwars_1)\n\n\n\n\n\nBy using dm_flatten_to_tbl() as shown below, a data frame can be created by joining films_x_characters data with films/people/planets data. In this case, if the same column names exist between different data, the column names are automatically renamed according to the data names. In this way, the relational data model manages the relationships between data on our behalf, allowing us to automatically join data frames based on the relationships among other data.\n\ndata_films_x_characters_1 &lt;- dm_starwars_1 |&gt; \n  dm_flatten_to_tbl(films_x_characters,\n                    .recursive = TRUE) \n\nRenaming ambiguous columns: %&gt;%\n  dm_rename(people, name.people = name) %&gt;%\n  dm_rename(planets, name.planets = name)\n\ndata_films_x_characters_1\n\n\n  \n\n\n\nThe data_films_x_characters_1 can be used to plot the composition ratio of the characters’ home planets, as shown below. We will not discuss this plot further. However, we have confirmed that the relational data model can be used to automate the joining of data frames.\nIn the above analysis, it is easy to join data frames using left_join(), and you may not see the advantage of using a relational data model. In the application section, we will consider a more complex situation where a relational data model prevails.\n\n\nPlot of the composition of the characters’ home planets\ndata_films_x_characters_1 |&gt; \n  mutate(name.planets = fct_lump_n(name.planets, 7,\n                                   ties.method = \"first\") |&gt; \n           fct_relevel(\"Other\", \n                       after = Inf)) |&gt; \n  count(title, name.planets) |&gt; \n  mutate(prop = n / sum(n),\n         .by = title,\n         .keep = \"unused\") |&gt; \n  ggplot(aes(fct_rev(title), prop,\n             fill = name.planets)) +\n  geom_col(position = position_stack(reverse = TRUE)) +\n  geom_text(aes(label = if_else(prop &lt; 5e-2, \n                                \"\",\n                                scales::label_percent(accuracy = 1)(prop))),\n            position = position_stack(vjust = 0.5,\n                                      reverse = TRUE)) +\n  scale_x_discrete(\"Title\") +\n  scale_y_continuous(\"Composition of the characters' home planets\",\n                     labels = scales::percent) +\n  scale_fill_brewer(\"Planet Name\",\n                    palette = \"Set2\") +\n  coord_flip() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_legend(nrow = 2,\n                             byrow = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n2. Application: Extending the model and validating keys with dm\nIn the application, we will examine the composition ratio of the species of the characters in Star Wars movies. The difficulty level of this analysis is not much different from that of the basic analysis. However, as the amount of data to be handled increases, the code tends to become more complicated, and the advantages of using a relational data model are significant. In addition, the advantages of the relational data model include the following,\n\nNew data can be added to the existing relational data model\nPossible to check key constraints defined to join data frames\n\nWe prepare in advance the species data needed for this analysis.\n\nspecies &lt;- tibble(species = sw_species) |&gt; \n  unnest_wider(species) |&gt; \n  select(url, name)\n\nspecies\n\n\n  \n\n\n\nIn dm, we can add new data to the relational data model using dm(). In this example, let’s build dm_starwars_2 by adding the species data to dm_starwars_1. By using dm_draw(), we can see that the model has been updated.\n\ndm_starwars_2 &lt;- dm_starwars_1 |&gt; \n  dm(species) |&gt; \n  dm_add_pk(species, url) |&gt; \n  dm_add_fk(people, species, species)\n\ndm_draw(dm_starwars_2)\n\n\n\n\n\nNext, let us check key constraints to join data frames. This can be done using dm_examine_constraints(). Let’s check the behavior of dm_examine_constraints() by building a model that contains the two types of mistakes mentioned above as common mistakes. Here, dm_starwars_2_strong_data is data in which the first row of the species data has been deleted and the data is not “MECE”. dm_starwars_2_wrong_pk is the data with the wrong primary key in the species data.\n\ndm_starwars_2_wrong_data &lt;- dm_starwars_1 |&gt; \n  dm(species = species |&gt; \n       slice(-1)) |&gt; \n  dm_add_pk(species, url) |&gt; \n  dm_add_fk(people, species, species)\n\ndm_starwars_2_wrong_pk &lt;- dm_starwars_1 |&gt; \n  dm(species) |&gt; \n  dm_add_pk(species, name) |&gt; \n  dm_add_fk(people, species, species)\n\nLet’s look at the result of dm_examine_constraints(). For the correct model, dm_starwars_2, the message ℹ All constraints satisfied. is displayed. On the other hand, dm_starwars_2_strong_data and dm_starwars_2_strong_pk show the message ! Unsatisfied constraints: is displayed. This is because the primary key of the species data does not contain data that should be included. Thus, we see that dm_examine_constraints() can be used to easily check the key constraints.\n\nprint(dm_examine_constraints(dm_starwars_2))\n\nℹ All constraints satisfied.\n\nprint(dm_examine_constraints(dm_starwars_2_wrong_data))\n\n! Unsatisfied constraints:\n\n\n• Table `people`: foreign key `species` into table `species`: values of `people$species` not in `species$url`: http://swapi.co/api/species/5/ (1)\n\nprint(dm_examine_constraints(dm_starwars_2_wrong_pk))\n\n! Unsatisfied constraints:\n\n\n• Table `people`: foreign key `species` into table `species`: values of `people$species` not in `species$name`: http://swapi.co/api/species/1/ (35), http://swapi.co/api/species/2/ (5), http://swapi.co/api/species/12/ (3), http://swapi.co/api/species/15/ (2), http://swapi.co/api/species/22/ (2), …\n\n\nAs described above, we have seen that we can add new data to the relational data model with dm() and check key constraints with dm_examine_constraints(). Finally, the following figure shows a composition of the species of the characters in the Star Wars films using dm_starwars_2. Again, we omit the discussion of the plots.\n\n\nPlot of the composition of the characters’ races\ndm_starwars_2 |&gt; \n  dm_flatten_to_tbl(films_x_characters,\n                    .recursive = TRUE) |&gt; \n  mutate(name.species = name.species |&gt; \n           fct_na_value_to_level(\"Other\") |&gt; \n           fct_lump_n(7,\n                      ties.method = \"first\") |&gt; \n           fct_relevel(\"Other\", \n                       after = Inf)) |&gt; \n  count(title, name.species) |&gt; \n  mutate(prop = n / sum(n),\n         .by = title,\n         .keep = \"unused\") |&gt; \n  ggplot(aes(fct_rev(title), prop,\n             fill = name.species)) +\n  geom_col(position = position_stack(reverse = TRUE)) +\n  geom_text(aes(label = if_else(prop &lt; 5e-2, \n                                \"\",\n                                scales::label_percent(accuracy = 1)(prop))),\n            position = position_stack(vjust = 0.5,\n                                      reverse = TRUE)) +\n  scale_x_discrete(\"Title\") +\n  scale_y_continuous(\"Composition of the characters' races\",\n                     labels = scales::percent) +\n  scale_fill_brewer(\"Species name\",\n                    palette = \"Set2\") +\n  coord_flip() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_legend(nrow = 2,\n                             byrow = TRUE))"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#summary",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#summary",
    "title": "Improve the quality of data preprocessing with R’s dm package",
    "section": "Summary",
    "text": "Summary\nIn this article, we have shown how to build a relational data model using dm. Once a relational data model is built using dm, it is no longer necessary to manage relationships among data by oneself, and data can be automatically joined using dm_flatten_to_tbl(). In addition, dm provides other useful functions to enhance the quality of data preprocessing, such as adding data to the model with dm() and checking key constraints with dm_examine_constraints()."
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#references",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#references",
    "title": "Improve the quality of data preprocessing with R’s dm package",
    "section": "References",
    "text": "References\n\ndm package site\nPreprocessing Compendium [Practical SQL/R/Python techniques for data analysis] (in Japanese)\nstarwarsdb\n\nStar Wars relational data model available for download from CRAN"
  },
  {
    "objectID": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#footnotes",
    "href": "posts/2024/01/relational-data-models-with-r-s-dm-en.html#footnotes",
    "title": "Improve the quality of data preprocessing with R’s dm package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFortunately, recent versions of dplyr warn against duplicate keys when joining data frames.↩︎\nAmong the data exported by repurrrsive, the data whose name begins with sw_ are related to StarWars, and the part after sw_ represents the content of the data.↩︎\nThe species column of sw_people is not needed here, but is selected for use in the next analysis.↩︎\nHere, I used mermaid to illustrate the relationship between data frames.↩︎\nThere is no clear hierarchical relationship between works and characters, so the name characters_x_films is acceptable.↩︎\nFor films, people, and planets, the url column is the primary key, and for films_x_characters, url and characters are the primary keys.↩︎\nFollowing the arrows in the image above, I set foreign keys.↩︎"
  },
  {
    "objectID": "posts/2024/02/scraping-dynamic-sites-with-rvest-en.html",
    "href": "posts/2024/02/scraping-dynamic-sites-with-rvest-en.html",
    "title": "Scraping dynamic sites with rvest (without Selenium)",
    "section": "",
    "text": "Note: This article is translated from my Japanese article."
  },
  {
    "objectID": "posts/2024/02/scraping-dynamic-sites-with-rvest-en.html#web-scraping-for-dynamic-sites-in-r",
    "href": "posts/2024/02/scraping-dynamic-sites-with-rvest-en.html#web-scraping-for-dynamic-sites-in-r",
    "title": "Scraping dynamic sites with rvest (without Selenium)",
    "section": "Web scraping for dynamic sites in R",
    "text": "Web scraping for dynamic sites in R\nrvest is a popular package for web scraping in R, bur it could not be used for dynamic sites where the contents changes with the operations on the browser.\nTherefore, for scraping dynamic sites in R, you needed to use other packages such as RSelenium with rvest and had the following issues.\n\nWhen using Selenium, it is troublesome to set up the environment (e.g., you need to download the driver in advance, etc.).\nWe couldn’t seamlessly apply rvest functions to HTML from other packages.\n\nIn rvest 1.0.4, however, a new read_html_live() function has been added to allow dynamic site scraping with rvest alone. By using read_html_live(), you can automate browser operations with methods such as $click() and $type(). Not only that, but you can seamlessly call rvest functions such as html_elements() and html_attr() with read_html_live().\nread_html_live() uses chromote package for Google Chrome automation internally. Therefore to use the function, you need to install Google Chrome (browser) and chromote (R package) beforehand."
  },
  {
    "objectID": "posts/2024/02/scraping-dynamic-sites-with-rvest-en.html#lets-use-read_html_live",
    "href": "posts/2024/02/scraping-dynamic-sites-with-rvest-en.html#lets-use-read_html_live",
    "title": "Scraping dynamic sites with rvest (without Selenium)",
    "section": "Let’s use read_html_live()",
    "text": "Let’s use read_html_live()\nFrom here, let’s use read_html_live() to perform the same process in this RSelenium tutorial.\nThis tutorial automates the process of collecting information from local TV stations in this site. This process requires a search for U.S. zip codes.\nBy using read_html_live(), the code to access the site in RSelenium can be rewritten in rvest as follows.\n# RSelenium\n# Source: https://joshuamccrain.com/tutorials/web_scraping_R_selenium.html\nlibrary(RSelenium)\n\nrD &lt;- rsDriver(browser=\"firefox\", port=4545L, verbose=F)\nremDr &lt;- rD[[\"client\"]]\nremDr$navigate(\"https://www.fcc.gov/media/engineering/dtvmaps\")\n⏬\n\n# rvest\nlibrary(rvest)\nlibrary(tidyverse)\n\nhtml &lt;- read_html_live(\"https://www.fcc.gov/media/engineering/dtvmaps\")\n\nYou can use the $view() method to see the LiveHTML object. You can also select elements on the site by Ctrl+Shift+C and then right-click⏩Copy⏩Copy selector to copy the CSS selector, and use it as an argument to $type() or $click().\n# rvest\nhtml$view()\n\nNext, enter the zip code in the center form and click the Go! button. Here, the code in RSelenium can be rewritten in rvest as follows.\n# RSelenium\n# Source: https://joshuamccrain.com/tutorials/web_scraping_R_selenium.html\nzip &lt;- \"30308\"\nremDr$findElement(using = \"id\", value = \"startpoint\")$sendKeysToElement(list(zip))\nremDr$findElements(\"id\", \"btnSub\")[[1]]$clickElement()\n⏬\n\n# rvest\nzip &lt;- \"30308\"\nhtml$type(\"#startpoint\", zip)\nhtml$click(\"#btnSub\")\n\nFinally, let’s check we got the same data as in the RSelenium tutorial above.\n\n# rvest\nhtml |&gt; \n  html_elements(\"table.tbl_mapReception\") |&gt; \n  insistently(chuck)(3) |&gt; \n  html_table() |&gt; \n  select(!c(1, IA)) |&gt; \n  rename_with(str_to_lower) |&gt; \n  rename(ch_num = `ch#`) |&gt; \n  slice_tail(n = -1) |&gt; \n  filter(callsign != \"\")"
  },
  {
    "objectID": "posts/2024/02/scraping-dynamic-sites-with-rvest-en.html#summary",
    "href": "posts/2024/02/scraping-dynamic-sites-with-rvest-en.html#summary",
    "title": "Scraping dynamic sites with rvest (without Selenium)",
    "section": "Summary",
    "text": "Summary\nAs above, by using read_html_live() that was added in rvest 1.0.4, we found that seamless scraping of static and dynamic sites is possible with rvest alone. In addition, it should be noted that rvest’s code was simpler than RSelenium’s.\nIn addition to rvest, a web scraping package called selenider is also being developed for R. It is expected that the development of such packages will make web scraping in R even more convenient!"
  },
  {
    "objectID": "posts/2024/03/be-careful-with-function-forms-in-lingam.html",
    "href": "posts/2024/03/be-careful-with-function-forms-in-lingam.html",
    "title": "LiNGAMによる因果探索では関数形に注意しよう",
    "section": "",
    "text": "LiNGAM（Linear Non-Gaussian Acyclic Model）は代表的な因果探索手法の一つで，近年は市販のソフトウェア等にも実装されるなど実務での活用が進んでいるようです．LiNGAMは，その名の通り，関数形が線形かつ誤差項がガウス分布（正規分布）以外に従う場合に，データから因果関係を明らかにすることができる手法です．\nLiNGAMは，PythonのlingamパッケージやRのpcalgパッケージで利用することができ，これらのパッケージで提供される関数にデータフレームを入力すれば，簡単に因果グラフを推定することができます．因果グラフとは，観測変数をノード，因果関係を矢印に見立てて構築されるネットワーク構造のことです．特に，因果探索では，因果グラフとしてDAG（Directed Acyclic Graph）と呼ばれる有向非巡回グラフを推定することが一般的です．\n因果グラフの例として，以下にレストラン経営に関する因果グラフを作成してみました．実務においては，曜日・天気・出店地域などの様々な要因によって利益が 変動すると考えられ，さらに複雑な因果関係が分析対象となることが想定されます．\n\n\n\n\n\nflowchart LR\n  食材の質 --&gt; 料理の美味しさ\n  食材の質 --&gt; 原価\n  料理人の腕 --&gt; 料理の美味しさ\n  料理人の腕 --&gt; 給与\n  料理の美味しさ --&gt; 来客数\n  料理の美味しさ --&gt; 客単価\n  来客数 --&gt; 売上\n  客単価 --&gt; 売上\n  売上 --&gt; 利益\n  原価 --&gt; 費用\n  給与 --&gt; 費用\n  固定費用 --&gt; 費用\n  費用 --&gt; 利益"
  },
  {
    "objectID": "posts/2024/03/be-careful-with-function-forms-in-lingam.html#lingamによる因果探索",
    "href": "posts/2024/03/be-careful-with-function-forms-in-lingam.html#lingamによる因果探索",
    "title": "LiNGAMによる因果探索では関数形に注意しよう",
    "section": "",
    "text": "LiNGAM（Linear Non-Gaussian Acyclic Model）は代表的な因果探索手法の一つで，近年は市販のソフトウェア等にも実装されるなど実務での活用が進んでいるようです．LiNGAMは，その名の通り，関数形が線形かつ誤差項がガウス分布（正規分布）以外に従う場合に，データから因果関係を明らかにすることができる手法です．\nLiNGAMは，PythonのlingamパッケージやRのpcalgパッケージで利用することができ，これらのパッケージで提供される関数にデータフレームを入力すれば，簡単に因果グラフを推定することができます．因果グラフとは，観測変数をノード，因果関係を矢印に見立てて構築されるネットワーク構造のことです．特に，因果探索では，因果グラフとしてDAG（Directed Acyclic Graph）と呼ばれる有向非巡回グラフを推定することが一般的です．\n因果グラフの例として，以下にレストラン経営に関する因果グラフを作成してみました．実務においては，曜日・天気・出店地域などの様々な要因によって利益が 変動すると考えられ，さらに複雑な因果関係が分析対象となることが想定されます．\n\n\n\n\n\nflowchart LR\n  食材の質 --&gt; 料理の美味しさ\n  食材の質 --&gt; 原価\n  料理人の腕 --&gt; 料理の美味しさ\n  料理人の腕 --&gt; 給与\n  料理の美味しさ --&gt; 来客数\n  料理の美味しさ --&gt; 客単価\n  来客数 --&gt; 売上\n  客単価 --&gt; 売上\n  売上 --&gt; 利益\n  原価 --&gt; 費用\n  給与 --&gt; 費用\n  固定費用 --&gt; 費用\n  費用 --&gt; 利益"
  },
  {
    "objectID": "posts/2024/03/be-careful-with-function-forms-in-lingam.html#関数形がlingamの推定に与える影響について",
    "href": "posts/2024/03/be-careful-with-function-forms-in-lingam.html#関数形がlingamの推定に与える影響について",
    "title": "LiNGAMによる因果探索では関数形に注意しよう",
    "section": "関数形がLiNGAMの推定に与える影響について",
    "text": "関数形がLiNGAMの推定に与える影響について\nLiNGAMは，関数形が線形かつ誤差項がガウス分布以外に従うという前提のもとで因果関係を推定する手法です．そのため，関数形が線形でない（線形近似が難しい）場合には，誤った因果関係が推定される可能性があります．パッケージや市販のソフトウェアを使えばLiNGAMを簡単に実行することができてしまいますが，利用者はこうした前提があることを理解してLiNGAMを使う必要があると思われます．\nここでは，上に挙げた因果グラフよりも簡単な以下のような因果関係をもつデータに対してLiNGAMを適用してみましょう．具体的には， 以下のフローチャートのfunに，非線形な関数が入ったときにLiNGAMの結果がどのように変わるかを確認します．\nただし，この記事では最も一般的なLiNGAMであるDirect LiNGAM，誤差項として非ガウス分布の一様分布を用いています． また，係数の絶対値が0.001未満の因果関係は無視するものとしました．\n\n\n\n\n\nflowchart LR\n  x11 --&gt; add1(+)\n  x12 --&gt; add1\n  add1 --&gt; x21\n  x21 --&gt; fun2(fun)\n  x22 --&gt; fun2\n  fun2 --&gt; x31\nstyle fun2 stroke:red,color:red\n\n\n\n\n\n\n因果グラフの推定に先立ってx11・x12・x22列をもつ1,000行のデータフレームおよび因果探索用の関数を用意しておきましょう．\n\n\nコード\nimport numpy as np\nimport os\nimport pandas as pd\nimport lingam\n\nrng = np.random.default_rng(1234)\n\nn = 1000\ndata = pd.DataFrame({\n  'x11': 1 + rng.random(n),\n  'x12': 2 + rng.random(n),\n  'x22': 3 + rng.random(n)\n})\n\n# DirectLiNGAMの結果をデータフレームとして返す関数\ndef discover_causality(data):\n  model = lingam.DirectLiNGAM()\n  model.fit(data)\n\n  return pd.DataFrame(\n    model.adjacency_matrix_,\n    columns=data.columns,\n    index=data.columns\n  )\\\n  .reset_index(names = 'node_to')\\\n  .melt(\n    id_vars='node_to',\n    var_name='node_from'\n  )\\\n  .pipe(lambda df: df[np.logical_not(np.isclose(df.value, 0, rtol=0, atol=1e-3))])\\\n  .reindex(columns=['node_from', 'node_to', 'value'])\n\n# mermaidファイルを出力する関数\ndef write_mermaid(df, file):\n  with open(file, 'w') as f:\n    f.write('flowchart LR\\n')\n    for row in df.itertuples():\n      f.write('  {}--&gt;|{:.3f}|{}\\n'.format(row.node_from, row.value, row.node_to))\n\n# 出力先のフォルダ\ndir = 'be-careful-with-function-forms-in-lingam'\nif not os.path.exists(dir):\n  os.makedirs(dir)\n\n\n\n因果グラフに掛け算が含まれるケース（funが*）\nfunが掛け算（*）のときにDirect LiNGAMの推定結果がどのようになるかを見てみましょう． 推定された因果グラフは真の因果グラフと同等の構造をもっており，今回のケースでは，因果グラフが正しく推定されていることがわかります． このように，足し算（線形）でなく掛け算のケースでも因果関係の推定がうまくいくケースがあるようです． ただし，観測変数の数やデータ数によっては状況が大きく異なるかもしれません．\n\ndata_nonlinear_prod = data\\\n .assign(\n    x21=lambda df: df.x11 + df.x12 + rng.random(n),\n    x31=lambda df: df.x21 * df.x22 + rng.random(n),\n  )\n\ncausality_nonlinear_prod = discover_causality(data_nonlinear_prod)\nprint(causality_nonlinear_prod)\n\n   node_from node_to     value\n3        x11     x21  0.975641\n8        x12     x21  0.999905\n14       x22     x31  4.524066\n19       x21     x31  3.477884\n\n\n\n\nコード\nwrite_mermaid(causality_nonlinear_prod, os.path.join(dir, 'dag_lingam_nonlinear_prod.mmd'))\n\n\n\n\n\n\n\nflowchart LR\n  x11--&gt;|0.976|x21\n  x12--&gt;|1.000|x21\n  x22--&gt;|4.524|x31\n  x21--&gt;|3.478|x31\n\n\n\n\n\n\n\n\n因果グラフにべき乗が含まれるケース（funが**）\n次に，funがべき乗（**）のときにDirect LiNGAMの推定結果がどのようになるかを見てみましょう． 推定された因果グラフは真の因果グラフと異なる構造をもっており，もともと因果関係が存在しない上流の観測変数間にも誤った因果関係が推定されていることがわかります．このように，Direct LiNGAMに非線形な関数が含まれる場合には，その関数と直接関係しない観測変数間においても，誤った因果関係が推定されてしまうリスクがあることがわかります．\n\ndata_nonlinear_power = data\\\n .assign(\n    x21=lambda df: df.x11 + df.x12 + rng.random(n),\n    x31=lambda df: df.x21 ** df.x22 + rng.random(n),\n  )\n\ncausality_nonlinear_power = discover_causality(data_nonlinear_power)\nprint(causality_nonlinear_power)\n\n   node_from node_to     value\n1        x11     x12 -0.147973\n3        x11     x21  0.832946\n8        x12     x21  0.798157\n17       x21     x22 -0.400541\n22       x31     x22  0.002306\n\n\n\n\nコード\nwrite_mermaid(causality_nonlinear_power, os.path.join(dir, 'dag_lingam_nonlinear_power.mmd'))\n\n\n\n\n\n\n\nflowchart LR\n  x11--&gt;|-0.148|x12\n  x11--&gt;|0.833|x21\n  x12--&gt;|0.798|x21\n  x21--&gt;|-0.401|x22\n  x31--&gt;|0.002|x22"
  },
  {
    "objectID": "posts/2024/03/be-careful-with-function-forms-in-lingam.html#まとめ",
    "href": "posts/2024/03/be-careful-with-function-forms-in-lingam.html#まとめ",
    "title": "LiNGAMによる因果探索では関数形に注意しよう",
    "section": "まとめ",
    "text": "まとめ\nこの記事では，非線形な関数をもつデータに対してLiNGAMを適用すると，誤った因果関係が推定される可能性があることを示しました． そのため，LiNGAMの適用にあたっては，因果関係が線形で表せることを確認することが重要です．\n一方で，因果関係がわからない状況なのに，因果関係が線形で表せることがわかるという状況はまれであると思われます． そのため，事後的な線形性の確認や実験などを通じて，慎重に因果関係を特定することが求められます．"
  },
  {
    "objectID": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html",
    "href": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html",
    "title": "Rで市区町村データをスマートに整形しよう（市町村合併や政令指定都市の集計処理など）",
    "section": "",
    "text": "この記事は，はじめに市区町村データの分析で注意すべき点について説明し， 次に，Rのjpcityパッケージを用いて市区町村データをスマートに整形する方法について紹介します．"
  },
  {
    "objectID": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html#市区町村データの分析で注意すべきことは",
    "href": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html#市区町村データの分析で注意すべきことは",
    "title": "Rで市区町村データをスマートに整形しよう（市町村合併や政令指定都市の集計処理など）",
    "section": "市区町村データの分析で注意すべきことは？",
    "text": "市区町村データの分析で注意すべきことは？\n市区町村別の統計データを分析する際には，どのようなことに気をつけるべきでしょうか． 以下では，主な注意点を3つに絞って紹介します．\n\n①市区町村名の重複に注意する\nまず気をつけるべきことは，同じ名称をもつ市区町村名が複数存在する場合があることです． 実際に，2020年1月1日時点で漢字表記が重複する市町村は，以下のように60市町村弱存在します1．\nここで，表の一番左列city_codeに記載されている5桁の数字は， 市区町村コード（全国地方公共団体コード）と呼ばれるもので市区町村を一意に識別するための番号です． そのため，市区町村データを分析する際には，市区町村名ではなく市区町村コードを用いて市区町村を識別することが望ましいと言えるでしょう．\nちなみに，市区町村コードの上2桁の数字は都道府県の識別番号であり，都道府県コードと呼ばれます．\n\n\n市区町村名の重複箇所の抽出\nlibrary(tidyverse)\nlibrary(jpcity)\n\nget_city(\"2020-01-01\") |&gt; \n  jpcity:::city_data() |&gt; \n  mutate(n = n(),\n         .by = c(city_desig_name, city_name)) |&gt; \n  filter(n &gt; 1) |&gt; \n  arrange(desc(n), city_desig_name, city_name) |&gt; \n  select(!c(starts_with(\"city_desig_name\"), n)) |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\n\n②市町村合併（廃置分合）による市区町村コードの変更に注意する\n市区町村コードを使ってさえいれば問題ないのでしょうか？ 残念ながら，市区町村コードによる分析にはもう一つの落とし穴があります． それは，市町村合併（廃置分合）によって市区町村コードが変更されることがあるという点です．\n市区町村コードは1970年4月1日に利用が開始されましたが，その後の平成の大合併（1999年～2010年）などによって市区町村数は大きく減少しました． そのため，特に1999年～2010年ごろの市区町村データを分析する際には，別途，面倒な集計・按分処理（いわゆる廃置分合処理）が必要となることがあります．\n例として，1970年4月1日から2020年1月1日にかけて最も多くの市区町村が合併してできた市区町村を抽出してみましょう． 抽出の結果（下表），これまでに最も多くの市町村合併を経た市区町村は15市町村が合併してできた新潟県上越市であることがわかりました．\nまた，市町村合併だけでなく町制変更や市制施行などによって市区町村コードが変更されることもあります． そのため，市区町村データを分析する際には，こうした市区町村コードの変更事由や変更時期を考慮したデータ整形が必要となることがあります．\n\n\n最も多くの市区町村が合併してできた市区町村の抽出\ntibble(city_after = get_city(\"2020-01-01\"),\n       city_before = city_after |&gt; \n         city_convert(\"2020-01-01\", \"1970-04-01\")) |&gt; \n  mutate(size_city_before = vctrs::list_sizes(city_before)) |&gt; \n  slice_max(size_city_before, \n            n = 1) |&gt; \n  select(!size_city_before) |&gt; \n  unnest(city_before) |&gt; \n  mutate(city_code_before = city_code(city_before),\n         city_name_before = city_name(city_before),\n         city_code_after = city_code(city_after),\n         city_name_after = city_name(city_after),\n         .keep = \"unused\") |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\n\n③政令指定都市の市区の扱いに注意する\n上記の注意点を押さえれば，市区町村データの整形の山場を乗り越えたと言えるでしょう． 最後に，細かい点ではありますが，政令指定都市の市区の扱いについて取り上げておきます．\n統計データによっては，政令指定都市の区レベルのデータが入手可能な市区町村データではなく， 市レベルで集計した市町村データしか入手できないことがあります．\n政令指定都市の市と区には，それぞれに異なる市区町村コードが割り振られているため， こうした地域区分の異なるデータを統合する際には，市区町村コードの対応付けが必要となります．\n例として，横浜市における市と区の対応付けを下表に示します．\n\n\n政令指定都市の市と区の対応付け\ntibble(city = find_city(\"横浜市\", \"2020-01-01\") |&gt; \n         city_desig_merge() |&gt; \n         vctrs::vec_unique(),\n       city_desig = city |&gt; \n         city_desig_split()) |&gt; \n  unnest(city_desig) |&gt; \n  mutate(city_code = city_code(city),\n         city_name = city_name(city),\n         city_desig_code = city_code(city_desig),\n         city_desig_name = city_name(city_desig),\n         .keep = \"unused\") |&gt; \n  rmarkdown::paged_table()"
  },
  {
    "objectID": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html#rで市区町村データをスマートに整形する",
    "href": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html#rで市区町村データをスマートに整形する",
    "title": "Rで市区町村データをスマートに整形しよう（市町村合併や政令指定都市の集計処理など）",
    "section": "Rで市区町村データをスマートに整形する",
    "text": "Rで市区町村データをスマートに整形する\nここまでの注意点を踏まえて，Rを用いて市区町村データをスマートに整形する方法を紹介します． 実は，上に示した事例は，いずれもe-Statの 「市区町村を探す」と 「廃置分合等情報を探す」で 公開されている情報に基づいています．\nしかし，「廃置分合等情報を探す」では，市町村合併等による市区町村コードの変更内容がテキスト形式で記載されているため， そのままではデータ整形に利用しにくいという問題があります．\nそこで，こうした複雑なデータ整形を行うことなく，簡単に廃置分合処理や政令指定都市の市区の対応付けを行うために， 新たに，jpcityパッケージを作成しました2． ちなみに，上に示した表はすべてjpcityパッケージを用いて作成したものです． jpcityは，CRANに登録済みであるため，以下のコードを実行することでインストールすることができます． ここからは，jpcityの使い方について簡単に説明していきます．\n\ninstall.packages(\"jpcity\")\n\n\n①市区町村コードや市区町村名の読込み\njpcityでは，parse_city()関数を用いて市区町村コードから市区町村オブジェクトを構築することができます3． また，find_city()関数を用いて市区町村名から市区町村オブジェクトを構築することができます． ただし，find_city()は市区町村名が重複する場合にすべての市区町村が返されてしまうため，基本的にはparse_city()を用いることが推奨されます． さらに，parse_city()・find_city()は，ともに第2引数whenに日付を指定することで， その時点での市区町村を取得することができます4．\nまた，市区町村オブジェクトの市区町村コードや市区町村名への（文字列）変換には，city_code()関数やcity_name()関数を用いることができます．\n\nlibrary(tidyverse)\nlibrary(jpcity)\n\ncity &lt;- tibble(city_parsed = parse_city(\"15222\", \"2020-01-01\"),\n               city_found = find_city(\"上越市\", \"2020-01-01\"))\n\ncity\n\n# A tibble: 1 × 2\n  city_parsed          city_found          \n  &lt;city&gt;               &lt;city&gt;              \n1 15222 [新潟県上越市] 15222 [新潟県上越市]\n\n\n\n\n②市町村合併（廃置分合）処理\ncity_convert()関数を用いて市区町村合併（廃置分合）処理を行うことができます． ここでは，さきほど読み込んだ上越市を例に，2000年1月1日時点での市区町村に変換してみましょう5． city_convert()の第2引数fromは変換前の日付\"2020-01-01\"，第3引数toは変換後の日付\"2000-01-01\"を指定します．\n\ncity |&gt; \n  select(city_parsed) |&gt; \n  mutate(city_converted = city_parsed |&gt; \n           city_convert(\"2020-01-01\", \"2000-01-01\")) |&gt; \n  unnest(city_converted)\n\n# A tibble: 14 × 2\n   city_parsed          city_converted        \n   &lt;city&gt;               &lt;city&gt;                \n 1 15222 [新潟県上越市] 15521 [新潟県安塚町]  \n 2 15222 [新潟県上越市] 15522 [新潟県浦川原村]\n 3 15222 [新潟県上越市] 15525 [新潟県大島村]  \n 4 15222 [新潟県上越市] 15526 [新潟県牧村]    \n 5 15222 [新潟県上越市] 15541 [新潟県柿崎町]  \n 6 15222 [新潟県上越市] 15542 [新潟県大潟町]  \n 7 15222 [新潟県上越市] 15543 [新潟県頸城村]  \n 8 15222 [新潟県上越市] 15544 [新潟県吉川町]  \n 9 15222 [新潟県上越市] 15546 [新潟県中郷村]  \n10 15222 [新潟県上越市] 15548 [新潟県板倉町]  \n11 15222 [新潟県上越市] 15549 [新潟県清里村]  \n12 15222 [新潟県上越市] 15550 [新潟県三和村]  \n13 15222 [新潟県上越市] 15561 [新潟県名立町]  \n14 15222 [新潟県上越市] 15222 [新潟県上越市]  \n\n\n\n\n③政令指定都市の市区の対応付け\ncity_desig_split()関数やcity_desig_merge()関数を用いて政令指定都市の市区の対応付けを行うことができます． ここでは，さきほど事例として挙げた横浜市を対象に，市区の対応付けを行ってみましょう．\ncity_desig_split()関数は，政令指定都市の市を区に分割する関数です．city_desig_split()により，横浜市に14の区が存在することがわかります． city_desig_merge()関数は，政令指定都市の区を市に統合する関数です．city_desig_merge()により，さきほど分割した横浜市の区を市に戻すことができます．\n\n# 政令指定都市の市を区に分割\ncity_desig &lt;- tibble(city = parse_city(\"14100\", \"2020-01-01\"),\n                     city_desig = city_desig_split(city)) |&gt; \n  unnest(city_desig)\ncity_desig\n\n# A tibble: 14 × 2\n   city                   city_desig                      \n   &lt;city&gt;                 &lt;city&gt;                          \n 1 14100 [神奈川県横浜市] 14101 [神奈川県横浜市鶴見区]    \n 2 14100 [神奈川県横浜市] 14102 [神奈川県横浜市神奈川区]  \n 3 14100 [神奈川県横浜市] 14103 [神奈川県横浜市西区]      \n 4 14100 [神奈川県横浜市] 14104 [神奈川県横浜市中区]      \n 5 14100 [神奈川県横浜市] 14105 [神奈川県横浜市南区]      \n 6 14100 [神奈川県横浜市] 14106 [神奈川県横浜市保土ケ谷区]\n 7 14100 [神奈川県横浜市] 14107 [神奈川県横浜市磯子区]    \n 8 14100 [神奈川県横浜市] 14108 [神奈川県横浜市金沢区]    \n 9 14100 [神奈川県横浜市] 14109 [神奈川県横浜市港北区]    \n10 14100 [神奈川県横浜市] 14110 [神奈川県横浜市戸塚区]    \n11 14100 [神奈川県横浜市] 14111 [神奈川県横浜市港南区]    \n12 14100 [神奈川県横浜市] 14112 [神奈川県横浜市旭区]      \n13 14100 [神奈川県横浜市] 14113 [神奈川県横浜市緑区]      \n14 14100 [神奈川県横浜市] 14114 [神奈川県横浜市瀬谷区]    \n\n# 政令指定都市の区を市に統合 (最初の数行だけ表示)\ncity_desig |&gt;\n  select(city_desig) |&gt; \n  mutate(city = city_desig_merge(city_desig)) |&gt; \n  head()\n\n# A tibble: 6 × 2\n  city_desig                       city                  \n  &lt;city&gt;                           &lt;city&gt;                \n1 14101 [神奈川県横浜市鶴見区]     14100 [神奈川県横浜市]\n2 14102 [神奈川県横浜市神奈川区]   14100 [神奈川県横浜市]\n3 14103 [神奈川県横浜市西区]       14100 [神奈川県横浜市]\n4 14104 [神奈川県横浜市中区]       14100 [神奈川県横浜市]\n5 14105 [神奈川県横浜市南区]       14100 [神奈川県横浜市]\n6 14106 [神奈川県横浜市保土ケ谷区] 14100 [神奈川県横浜市]"
  },
  {
    "objectID": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html#まとめ",
    "href": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html#まとめ",
    "title": "Rで市区町村データをスマートに整形しよう（市町村合併や政令指定都市の集計処理など）",
    "section": "まとめ",
    "text": "まとめ\nこの記事では，市区町村データの分析で注意すべき点と，Rのjpcityパッケージを用いて市区町村データをスマートに整形する方法について紹介しました． jpcityパッケージに対する要望・バグ報告や質問などがあれば，GitHubか以下のコメント欄からお気軽にご意見をお寄せください．"
  },
  {
    "objectID": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html#footnotes",
    "href": "posts/2024/05/lets-smartly-format-municipal-data-in-r.html#footnotes",
    "title": "Rで市区町村データをスマートに整形しよう（市町村合併や政令指定都市の集計処理など）",
    "section": "脚注",
    "text": "脚注\n\n\nここでは，ひらがな表記（読み方）が異なっていても漢字表記が同じであれば「重複」としてカウントしています．また，政令指定都市の区名は重複が多いので対象外としています．↩︎\njpcityは，e-Statの公開に公開されている「市区町村を探す」と「廃置分合等情報を探す」のデータに基づいて構築されています．↩︎\nparse_city()は，チェックデジット付きの6桁の市区町村コードにも対応しています．↩︎\nwhen 引数を指定しない場合には，いつの期間の市区町村かを推測して読込みを行います．また，期間が整合しない市区町村が含まれる場合には，読込みエラーとなります．↩︎\n上越市は，1971年4月29日に高田市と直江津市が合併して新設され，その後，2005年1月1日に13町村が上越市に合併されました．↩︎"
  },
  {
    "objectID": "posts/2024/12/tips-for-tidying-excel-file-with-r.html",
    "href": "posts/2024/12/tips-for-tidying-excel-file-with-r.html",
    "title": "RでExcelファイルを整形する際のTips",
    "section": "",
    "text": "この記事はR言語 Advent Calendar 2024の23日目の記事です．\nデータ分析において，面倒だけれどもやらなくては始まらないのがデータの前処理です．最近では，機械判読しやすいデータ作成を心掛けることが重視されるようになってきていますが， 人と機械の両方にとって読みやすいデータ形式を作成することはそもそも容易ではありません．\nそこで，この記事では，人が見ることを前提に作成されることの多いExcelファイルを Rで整形する際のTipsを紹介します．この記事では，主にExcelファイル向けのTipsを紹介しますが，CSVファイル等のデータ形式にも応用できると思います．\nExcelファイルを整形するアプローチには主に以下の2つがあります． この記事では，Excelファイルのテーブル構造の読み解き方を紹介した後に， 以下の2つのアプローチによるデータ整形の方法を紹介します．\nまた，Excelファイルのデータ整形については，すでにいくつかの日本語記事がありますので， 以下の記事も参考になると思います．"
  },
  {
    "objectID": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#excelファイルを整形する前に",
    "href": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#excelファイルを整形する前に",
    "title": "RでExcelファイルを整形する際のTips",
    "section": "Excelファイルを整形する前に",
    "text": "Excelファイルを整形する前に\n本題に入る前に，一点注意しておきたいことがあります． それは，分析に使用したいデータがさらに機械判読に適したデータ形式で提供されていないかを確認することです．\nExcelファイルはデータ閲覧には便利ですが，機械判読しにくいデータ形式であることが多いです． そのため，他の形式でもっと機械判読しやすいデータが提供されていないかを確認しておいたほうが良いでしょう． たとえば，政府統計の総合窓口のe-Statでは，API機能を通じて，一般的なExcelファイルより機械判読しやすいデータを入手できることがあります．"
  },
  {
    "objectID": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#データ形式を巡る人と機械の溝",
    "href": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#データ形式を巡る人と機械の溝",
    "title": "RでExcelファイルを整形する際のTips",
    "section": "データ形式を巡る「人と機械の溝」",
    "text": "データ形式を巡る「人と機械の溝」\n私たちにとって読みやすいデータが機械にとっても読みやすいとは限りません． そういったデータの典型的な例として，週間天気予報のデータが挙げられます1．\nたとえば，以下のようなダミーの週間天気予報があるとしましょう． このようなデータを見たとき，すぐに「地域・日付別の天気」であることがわかると思います． しかし，機械的に以下のようなことを理解することは意外に難しいと思われます．\n\n2列目以降の列名が日付を表していること\n2列目以降の列の値が天気を表していること\n\n\n\nコード\nlibrary(tidyverse)\n\nregion &lt;- c(\"札幌\", \"東京\", \"名古屋\", \"大阪\", \"福岡\", \"那覇\")\ndate &lt;- seq(ymd(\"2024-12-20\"), ymd(\"2024-12-26\"),\n            by = \"day\") |&gt; \n  format(\"%m/%d\")\nweather &lt;- c(\"🌞\", \"⛅\",  \"☔\")\n\nset.seed(1234)\nweather_forecast &lt;- expand_grid(region = region,\n                                date = date) |&gt; \n  mutate(weather = sample(weather, n(),\n                          replace = TRUE))\n\nweather_forecast_wider &lt;- weather_forecast |&gt; \n  pivot_wider(names_from = date,\n              values_from = weather)\nweather_forecast_wider\n\n\n\n  \n\n\n\n上のデータが「地域・日付別の天気」であることを明確にするためには，以下のようなデータのほうが適しています． 以下のデータでは，軸（次元）である地域・日付に，観測値である天気が列名と対応しており，こうした縦長のデータはtidy data（整然データ）と呼ばれています． しかし，このような縦長のデータがニュースで流れたら，多くの人は読みづらいと感じるでしょう（週間天気予報を確認するのに縦長のテレビが必要になってしまいますね）．\n\nweather_forecast\n\n\n  \n\n\n\nなぜ，このような「人と機械の溝」が生じるのでしょうか？それは，多くの人が2次元でデータを捉えることに慣れていることが一因かもしれません．\n以下の図は，上の縦長データをggplot2で散布図として表したものです．この散布図をはじめに示した表と見比べると同じような見た目になっていることがわかります．私たちにとっては，以下の散布図のように縦方向だけでなく横方向にも軸を持つ表のほうが見やすいのかもしれません．\n\nweather_forecast |&gt; \n  ggplot(aes(date, region, \n             label = weather)) +\n  geom_text() +\n  scale_x_discrete(position = \"top\") +\n  scale_y_discrete(limits = rev)"
  },
  {
    "objectID": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#excelファイルのテーブル構造を読み解く",
    "href": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#excelファイルのテーブル構造を読み解く",
    "title": "RでExcelファイルを整形する際のTips",
    "section": "Excelファイルのテーブル構造を読み解く",
    "text": "Excelファイルのテーブル構造を読み解く\nこのような「人と機械の溝」を踏まえて，実際のExcelファイルのテーブル構造を読み解いてみましょう．\n「地域・日付別の天気」のようなシンプルな事例であれば，テーブル構造はそこまで難しくなりませんが，たとえば，「西暦・地域・性別・年齢別の人口」のようなデータの場合はどうでしょうか？ 実際に，e-Statに公開されているこちらのExcelファイルを見てみましょう． 以下の画像はExcelファイルの上部を一部抜粋したものです．\n\nデータ上部の説明を無視すると，以下のようなデータ形式であることがわかります．そのため，データの軸（次元）にあたる西暦・地域・性別・年齢を水色（■），観測値の種別・単位にあたる人口・人口割合・人口性比を青色（■）で塗りつぶすと以下の図のようになります．データ形式の詳細をまとめると以下のようになります．\n\n1・2列目に，それぞれ地域・年齢階級の軸（次元）の情報が格納されている\n3列目以降の列名にあたる部分には，軸（次元）と観測値の情報が混在している\n\n1・3行目に，それぞれ西暦・性別の軸（次元）の情報が格納されている\n2・4行目は，それぞれ人口・人口割合・人口性比の観測値の種別と単位が記載されている\n\n\n\nExcelファイルを整形する際には，以下の点を意識しなければならないことがわかります．\n\n列名が複数行にまたがって記載されている際には，各行が軸（次元）・観測値のどちらに対応するかを事前に整理しておく必要がある\n列名の反復を避けるために一部の列名が省略 or セルが結合されていることが多い"
  },
  {
    "objectID": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#readxlでexcelファイルを整形してみよう",
    "href": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#readxlでexcelファイルを整形してみよう",
    "title": "RでExcelファイルを整形する際のTips",
    "section": "readxlでExcelファイルを整形してみよう",
    "text": "readxlでExcelファイルを整形してみよう\nそれでは，実際にExcelファイルを整形してみましょう．以下では，上で紹介した「西暦・地域・性別・年齢別の人口」のExcelファイルを整形するためのコードを示します．\nExcelファイルを読み込むのに役立つパッケージとして，readxlパッケージがあります2． ここでは，readxlパッケージを使ってExcelファイルを整形するため， 事前に，readxlパッケージとtidyverseパッケージをロードしておきます．\n\n# 必要に応じてパッケージをインストールしてください\n# install.packages(\"pak\")\n# pak::pak(\"readxl\")\n# pak::pak(\"tidyverse\")\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n\nデータダウンロード時の使用コードはこちら\nlibrary(fs) \n\nexdir &lt;- \"tips-tidying-excel-data-with-r\"\ndir_create(exdir)\n\ndestfile &lt;- path(exdir, \"population_by_year_sex_age_class\",\n                 ext = \"xlsx\")\nif (!file_exists(destfile)) {\n  curl::curl_download(\"https://www.e-stat.go.jp/stat-search/file-download?statInfId=000001085927&fileKind=0\",\n                      destfile = destfile)\n}\n\n\n\n①列名の読み込み・結合セル等の処理\nExcelファイルの読み込みの最初のステップとして，列名を読み込みます． CSVファイル等のテーブルデータでは，1行目に列名が記載されていることが多く， Rのデータフレームでも列名は文字列ベクトルで表されます．\nしかし，上で示したExcelファイルでは，3列目以降に複数行にまたがって列名が記載されているため， これらをデータフレームの列名として使えるようにするには工夫が必要です．\nそこで，まずreadxlのread_excel()関数を使って列名を読み込みます． read_excel()関数では，col_types引数の指定方法等に違いはあるものの，おおよそreadrのread_csv()関数と同様の書き方でExcelファイルを読み込むことができます3．\n3列目以降の列名では，横方向に軸（次元）や観測値の情報が格納されているため， t()関数で転置してからデータフレームに変換します． さらに，最初の2行（地域・年齢階級）を除外すると以下のようなデータを取得できます．\n\n# 事前にダウンロードしたExcelファイルの保存場所\nfile &lt;- \"tips-tidying-excel-file-with-r/population_by_year_sex_age_class.xlsx\"\nsheet &lt;- \"da03\"\n\ndata_col_names &lt;- read_excel(file,\n                             sheet = sheet, # シート名\n                             skip = 10, # 説明部分をスキップ\n                             n_max = 5, # 列名部分のみを読み込む\n                             col_names = FALSE,\n                             col_types = \"text\",\n                             .name_repair = \"minimal\") |&gt; \n  # 転置してからデータフレームに変換\n  t() |&gt; \n  as_tibble(.name_repair = ~c(\"year\", \"value_type\", \"sex\", \"\", \"value_unit\")) |&gt; \n  select(year, value_type, sex, value_unit) |&gt; \n  \n  # 最初の2行（地域・年齢階級）を除外\n  slice_tail(n = -2) \n\nhead(data_col_names, n = 10)\n\n\n  \n\n\n\ndata_col_namesを見ると，以下のことがわかります．\n\nyear列には，西暦と和暦が混在している\nyear・value_type列では，列名の重複を避けるため一部の列名が省略されNAとなっている\n\nそこで，以下のコードでは主に以下のような処理を行っています．\n\nyear列から西暦の年数のみを抽出\ntidyrのfill()関数を用いてyear・value_type列のNAを埋める\n\n以上の作業により列名を作成するための準備が整いました．\n\ndata_col_names &lt;- data_col_names |&gt;\n  # 西暦の年数のみを抽出\n  mutate(year = year |&gt; \n           str_extract(\"^\\\\d+(?=年$)\") |&gt; \n           as.integer(),\n         \n         # value_unitが\"-\"の場合は空文字に置換\n         value_unit = if_else(value_unit == \"-\",\n                              \"\",\n                              value_unit)) |&gt; \n  \n  # 西暦年とvalue_typeのNAを埋める\n  fill(year, value_type)\n\nhead(data_col_names, n = 10)\n\n\n  \n\n\n\n\n\n②列名の作成・データの読み込み\nそれでは，data_col_namesを使って列名を作成しましょう． ここでは，以下のような手順で列名を作成しました．\n\nまず，value_type・value_unit列を結合し，value_type列を作成\n次に，year・sex・value_type列の順に\"/\"区切りで結合し，col_name列を作成\n最後に，\"region\"・\"age_class\"・col_name列のデータを結合し，列名を作成\n\n以下のコードでは，データフレームの列名の結合にtidyrのunite()関数を使っています． 以上の作業により，データの読み込みに必要となる列名が作成されました．\n\ncol_names &lt;- data_col_names |&gt; \n  unite(\"value_type\", value_type, value_unit,\n        sep = \"\") |&gt; \n  unite(\"col_name\", year, sex, value_type,\n        sep = \"/\") |&gt; \n  pull(col_name)\ncol_names &lt;- c(\"region\", \"age_class\", col_names)\n\nhead(col_names, n = 10)\n\n [1] \"region\"                           \"age_class\"                       \n [3] \"1920/総数/人口（人）\"             \"1920/男/人口（人）\"              \n [5] \"1920/女/人口（人）\"               \"1920/総数/年齢，男女別割合（％）\"\n [7] \"1920/男/年齢，男女別割合（％）\"   \"1920/女/年齢，男女別割合（％）\"  \n [9] \"1920/NA/人口性比\"                 \"1925/総数/人口（人）\"            \n\n\ncol_namesを使ってデータを読み込んでみましょう． read_excel()関数にcol_names = col_namesとして列名を指定することで，先ほど作成した列名を使ってデータを読み込むことができます．\n\ndata &lt;- read_excel(file,\n                   sheet = sheet,\n                   skip = 10 + 5,\n                   col_names = col_names,\n                   col_types = \"text\",\n                   .name_repair = \"minimal\") |&gt;\n  \n  # 末尾の列に重複がみられるため重複箇所を削除 (元データ作成時のミスと思われる)\n  select(all_of(vctrs::vec_unique_loc(col_names)))\n\nhead(data, n = 10)\n\n\n  \n\n\n\n\n\n③tidy dataへの変換\n最後に，データを整形し，tidy data（整然データ）に変換しましょう． tidy dataは通常，縦長のデータになることが多いため，tidyrのpivot_longer()関数が便利です． pivot_longer()関数では，names_sep引数を用いることで，列名に含まれる複数の情報を列方向に展開することができます．\n今回のExcelファイルでは，軸（次元）にあたる地域・年齢階級以外の西暦・性別が列名に含まれているため，names_sep = \"/\"引数を使ってこれらを展開します． さらに，西暦・性別は，それぞれ\"/\"で区切られた部分の1・2番目に格納されているため，names_to引数の1・2番目にそれぞれ\"year\"・\"sex\"を指定します．\nさらに，\"/\"で区切られた部分の3番目にあたる人口（人）・年齢，男女別割合（％）・人口性比は観測値にあたるため， 今回のケースでは，これらを列方向に展開せず列名として残しておきたいです． これは，pivot_longer()関数のnames_to引数の3番目に\".value\"を指定することで実現できます．\nしたがって，以下のようなコードにより，データを縦長データに変換することができます．\n\ndata &lt;- data |&gt; \n  pivot_longer(!c(region, age_class),\n               names_to = c(\"year\", \"sex\", \".value\"),\n               names_sep = \"/\",\n               names_transform = list(sex = \\(x) x |&gt; \n                                        na_if(\"NA\"))) |&gt;   \n  # 人口・人口割合・人口性比の列を数値に変換\n  mutate(across(c(`人口（人）`, `年齢，男女別割合（％）`, 人口性比),\n                \\(x) {\n                  parse_number(x, \n                               na = \"-\")\n                })) |&gt; \n  relocate(year, region, sex, age_class)\n\nhead(data, n = 10)\n\n\n  \n\n\n\ndataを見ると，人口性比の性別sexの列が常にNAとなっていることがわかります． そこで，以下のコードでは，人口（人）・年齢，男女別割合（％）を含むdata_populationと人口性比を含むdata_sex_ratioにdataを分割しています． こうすることで，各データの意味がさらに明確になります．\n\ndata_population &lt;- data |&gt; \n  drop_na(sex) |&gt; \n  select(!人口性比)\n\nhead(data_population, n = 5)\n\n\n  \n\n\ndata_sex_ratio &lt;- data |&gt; \n  filter(is.na(sex)) |&gt; \n  select(!c(sex, `人口（人）`, `年齢，男女別割合（％）`))\n\nhead(data_sex_ratio, n = 5)"
  },
  {
    "objectID": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#tidyxlとunpivotrでexcelファイルを整形してみよう",
    "href": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#tidyxlとunpivotrでexcelファイルを整形してみよう",
    "title": "RでExcelファイルを整形する際のTips",
    "section": "tidyxlとunpivotrでExcelファイルを整形してみよう",
    "text": "tidyxlとunpivotrでExcelファイルを整形してみよう\n次に，tidyxlパッケージとunpivotrパッケージを使ってExcelファイルを整形する方法を紹介します．\nreadxlパッケージでは，列名の作成・データの読み込み・tidy dataへの変換という多くのステップを踏む必要がありました． tidyxlパッケージとunpivotrパッケージでは，Excelファイルのテーブルを「行・列別の値」に展開した縦長データを用いることで， readxlよりも柔軟かつ効率的にデータ整形を行うことができます．\nまずは，必要となるパッケージをロードしておきましょう．\n\n# 必要に応じてパッケージをインストールしてください\n# install.packages(\"pak\")\n# pak::pak(\"tidyxl\")\n# pak::pak(\"unpivotr\")\n# pak::pak(\"tidyverse\")\n\nlibrary(tidyxl)\nlibrary(unpivotr)\nlibrary(tidyverse)\n\n\n①Excelファイルの読み込み\ntidyxlパッケージのxlsx_cells()関数を使ってExcelファイルを読み込みます． この関数により，Excelファイルのセル情報を行・列別に取得することができます．\n\ncells &lt;- xlsx_cells(file,\n                    sheets = sheet)\n\nhead(cells, n = 10)\n\n\n  \n\n\n\n読み込んだcellsに対して，unpivotrパッケージのbehead()関数を適用することで，データに軸（次元）の情報を追加することができます． 具体的には，軸（次元）の情報がデータから見てどの方向directionにあるかを指定することで情報を追加します． 特に，direction = \"up-left\"等を指定することで空白セルや結合セルの対応が可能になり，とても便利です．\nbehead()関数を使うには，事前にテーブル構造を読み解いておく必要があります．データに対する方向directionの詳しい指定方法については，以下が参考になります．\n\nDirections from data cells to headers\n\n以下のコードでは，上で整理したテーブル構造に基づき，readxlで作成したdataと同様の情報を持つdata2を作成しています． readxlを用いた場合と比べて少ないコードでデータの読み込めたことがわかります．\n\ndata2 &lt;- cells |&gt; \n  # 説明部分にあたる最初の10行を除外\n  filter(row &gt; 10) |&gt;\n  \n  # 1列目は地域を表す (データから見てleft側)\n  behead(\"left\", \"region\") |&gt; \n  \n  # 2列目は年齢階級を表す (データから見てleft側)\n  behead(\"left\", \"age_class\") |&gt; \n  \n  # 1行目は西暦を表す (データから見てup-left側)\n  filter(row != 10 + 1 | str_detect(character, \"^\\\\d+年$\")) |&gt; # 和暦を削除\n  behead(\"up-left\", \"year\") |&gt; \n  \n  # 2行目は値の種別を表す (データから見てup-left側)\n  behead(\"up-left\", \"value_type\") |&gt; \n  \n  # 3行目は性別を表す (データから見てup側)\n  behead(\"up\", \"sex\") |&gt;\n  \n  # 5行目は単位を表す (データから見てup側)\n  behead(\"up\", \".\") |&gt; # 4行目の空白の行に適当な名前を付与 (後で削除)\n  behead(\"up\", \"value_unit\") |&gt; \n  \n  select(year, region, age_class, sex, value_type, value_unit, numeric)\n\nhead(data2, n = 5)\n\n\n  \n\n\n\n\n\n②tidy dataへの変換\n新たに作成したdata2は縦長データではあるものの，値の種別value_type（人口，年齢・男女別割合や人口性比）が列方向に展開されており，必ずしもデータ分析に適した形式とは言えません．\nそこで，tidyrのpivot_wider()関数を使って，値の種別value_typeを行方向に展開します． 以下のコードでは，value_type列とvalue_unit列を結合したのち横長データに変換しています． これによりreadxlを使って作成したdataとほぼ同等のデータが得られました．\n繰り返しになるため割愛しますが，readxlを使った場合と同様に，人口（人）・年齢，男女別割合（％）を含むデータと人口性比を含むデータに分割すれば，データ整形は完了です．\n\ndata2 &lt;- data2 |&gt; \n  # readxlを使った場合と同様にvalue_type列を作成\n  mutate(value_unit = if_else(value_unit == \"-\",\n                              \"\",\n                              value_unit)) |&gt;\n  unite(\"value_type\", value_type, value_unit,\n        sep = \"\") |&gt;\n  \n  # 重複箇所を削除\n  slice_head(n = 1,\n             by = c(year, region, age_class, sex, value_type)) |&gt; \n  \n  # 値の種別を行方向に展開して横長データに変換\n  pivot_wider(names_from = value_type,\n              values_from = numeric)\n\nhead(data2, n = 10)"
  },
  {
    "objectID": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#まとめ",
    "href": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#まとめ",
    "title": "RでExcelファイルを整形する際のTips",
    "section": "まとめ",
    "text": "まとめ\nこの記事では，Excelファイルを整形する際のTipsを紹介しました． データ整形のアプローチとして，readxlパッケージを使う方法とtidyxlパッケージとunpivotrパッケージを使う方法を紹介しましたが， いずれの場合でも，データのテーブル構造を読み解くことが重要です．\nこの記事で紹介したデータ整形の流れについて改善点・追加すべき内容・ご質問等があれば，コメント等をいただけると幸いです．"
  },
  {
    "objectID": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#footnotes",
    "href": "posts/2024/12/tips-for-tidying-excel-file-with-r.html#footnotes",
    "title": "RでExcelファイルを整形する際のTips",
    "section": "脚注",
    "text": "脚注\n\n\nこの事例は，Wikipedia日本語版のTidy dataのページでも用いられています．↩︎\nwritexlパッケージも便利です．↩︎\ncellrangerを使うことで，セルの範囲を詳細に指定することもできます．↩︎"
  },
  {
    "objectID": "posts/2025/04/removal-of-observation-noise-from-proportion.html",
    "href": "posts/2025/04/removal-of-observation-noise-from-proportion.html",
    "title": "割合データ（割り算値）から観測ノイズを除去する",
    "section": "",
    "text": "この記事では，以下のような割合データ（割り算値）を扱うときにしばしば生じる少数問題への対処法について説明します．\n少数問題（small number problem）とは，観測値が小さい場合に割合データのばらつきが大きくなることを指します． 以下では，カウントデータが従うことが多いとされるポアソン分布を対象に少数問題の対処法を考えます（二項分布でも同様の議論を展開することができます）． 分母にあたる人口等が\\(N\\)，割合の真値が\\(\\theta\\)のとき，分子にあたるカウントデータ（感染者数等）\\(n\\)は以下のようなポアソン分布に従うと考えられます．\n\\[\nn \\sim \\text{Poisson}(N \\theta)\n\\]\nさらに，このポアソン分布の分散は以下のように表されます．\n\\[\n\\text{Var}[n] = N \\theta\n\\]\n割合（真値）を推定するための素朴な方法として，\\(n\\)を\\(N\\)で割ることが考えられます． しかし，この方法で得られる割合の分散は以下のように，\\(N\\)が小さいときに分散が大きくなるという性質を持っています． こうした性質は，ポアソン分布や二項分布のように分散（観測ノイズ）が期待値と比例する分布に共通するものです．\n\\[\n\\text{Var}\\left[\\frac{n}{N}\\right] = \\frac{\\theta}{N}\n\\]\nこうした観測ノイズの性質は，たとえば難関校合格率の学校別ランキング等を考えた場合に問題となる可能性があります． 総受験者数が大きく異なる以下のような学校Aと学校Bを比べてみましょう．\nこの場合，学校Bでは，合格者数が少し変動しただけで難関校合格率が大きく変化してしまうため，学校Aの難関校合格率と比べて精度が低いと考えられます． このような状況で，難関校合格率ランキングを作ると，難関校合格率の真値は必ずしも高くないがたまたま合格者数が多かった総受験者数の少ない学校等が上位に順位付けされることとなります． こうした現象は，今後の難関校合格率を予想する場合等に問題となりえます．\n以下では，こうした複数地域・店舗・学校等のデータの観測値のみを用いて割合データの平滑化を行う方法について説明します．"
  },
  {
    "objectID": "posts/2025/04/removal-of-observation-noise-from-proportion.html#ベイズの定理を用いた平滑化",
    "href": "posts/2025/04/removal-of-observation-noise-from-proportion.html#ベイズの定理を用いた平滑化",
    "title": "割合データ（割り算値）から観測ノイズを除去する",
    "section": "ベイズの定理を用いた平滑化",
    "text": "ベイズの定理を用いた平滑化\nカウントデータがポアソン分布，割合の真値がポアソン分布の共役事前分布であるガンマ分布に従うと仮定します．\n\\[\n\\begin{align}\n  \\theta &\\sim \\text{Gamma}(\\alpha, \\beta) \\\\\n  n \\mid \\theta &\\sim \\text{Poisson}(N \\theta)\n\\end{align}\n\\]\nすると，ベイズの定理を用いて，\\(\\theta\\)の事後分布，すなわち観測値\\(n\\)が与えられたときの\\(\\theta\\)の分布は以下のようになります．\n\\[\n\\begin{align}\n  P(\\theta \\mid n) &\\propto P(n \\mid \\theta) P(\\theta) \\\\\n  &\\propto (N \\theta)^n \\exp(-N \\theta) \\cdot \\theta^{\\alpha - 1} \\exp(-\\beta\\theta) \\\\\n  &\\propto \\theta^{n + \\alpha - 1} \\exp[-(N + \\beta)\\theta] \\\\\n  \\theta \\mid n &\\sim \\text{Gamma}(n + \\alpha, N + \\beta)\n\\end{align}\n\\]\nそのため，\\(\\theta\\)の事後分布の期待値は以下のような簡単な式で表され，この式を用いることで，観測ノイズを除去（平滑化）することができます．\n\\[\n\\text{E}[\\theta \\mid n] = \\frac{n + \\alpha}{N + \\beta}\n\\]\n一方で，こうした平滑化を行うためには，\\(\\alpha\\)と\\(\\beta\\)の値を決める必要があります． 以下では，事前分布のパラメータ\\(\\alpha\\)と\\(\\beta\\)を観測値から推定する方法について説明します．"
  },
  {
    "objectID": "posts/2025/04/removal-of-observation-noise-from-proportion.html#経験ベイズ法による事前分布のパラメータ推定",
    "href": "posts/2025/04/removal-of-observation-noise-from-proportion.html#経験ベイズ法による事前分布のパラメータ推定",
    "title": "割合データ（割り算値）から観測ノイズを除去する",
    "section": "経験ベイズ法による事前分布のパラメータ推定",
    "text": "経験ベイズ法による事前分布のパラメータ推定\n経験ベイズ法は，観測値から事前分布のパラメータを（経験的に）推定する方法です． 経験ベイズ法における事前分布のパラメータの推定方法として，モーメント法や最尤法を用いる方法が考えられますが，以下では，最尤法を用いた方法について説明します．\n最尤法を行うにあたって，事前に\\(n\\)が従う分布を確かめておきましょう． \\(n\\)は，以下のようなガンマ分布とポアソン分布の混合分布に従うと考えられます． 通常，混合分布を求めるには積分計算が必要となってしまいますが，ガンマ分布とポアソン分布の混合分布は，負の二項分布に従うことが知られています． そのため，あらかじめ以下のように\\(n\\)の確率分布を求めておきます．\n\\[\n\\begin{align}\n  P(n) &= \\int_{0}^{\\infty} P(n \\mid \\theta) P(\\theta) d\\theta \\\\\n  &= \\int_{0}^{\\infty} \\frac{1}{n!} (N \\theta)^n \\exp(-N \\theta) \\cdot \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\theta^{\\alpha - 1} \\exp(-\\beta\\theta) d\\theta \\\\\n  &= \\frac{N^n \\beta^{\\alpha}}{n! \\Gamma(\\alpha)} \\int_{0}^{\\infty} \\theta^{n + \\alpha - 1} \\exp[-(N + \\beta)\\theta] d\\theta \\\\\n  &= \\frac{N^n \\beta^{\\alpha}}{n! \\Gamma(\\alpha)} \\cdot \\frac{\\Gamma(n + \\alpha)}{(N + \\beta)^{n + \\alpha}} \\quad \\text{(ガンマ分布のカーネルの積分は正規化定数の逆数に等しい)} \\\\\n  &= \\frac{\\Gamma(n + \\alpha)}{\\Gamma(n + 1) \\Gamma(\\alpha)} \\left( \\frac{N}{N + \\beta} \\right)^{n} \\left( \\frac{\\beta}{N + \\beta} \\right)^\\alpha \\\\\n  &= \\binom{n + \\alpha - 1}{n} \\left( \\frac{\\frac{\\alpha}{\\beta} N}{\\frac{\\alpha}{\\beta} N + \\alpha} \\right)^{n} \\left( \\frac{\\alpha}{\\frac{\\alpha}{\\beta} N + \\alpha} \\right)^{\\alpha} \\\\\n\\end{align}\n\\]\n以上の式より，\\(n\\)は以下のような負の二項分布に従うことがわかります． ただし，負の二項分布にはいくつかのパラメータ設定方法があるため注意が必要です． ここでのパラメータ設定方法は，stanのNegBinomial2と同様です．\n\\[\nn \\sim \\text{NegBinomial2}(\\mu, \\alpha), \\quad \\mu = \\frac{\\alpha}{\\beta} N\n\\]\nこのように，\\(n\\)は，負の二項分布と呼ばれる一般的な確率分布に従うことがわかるため，Rで提供されている既存の関数を用いて最尤法を行うことができます． 以下では，実験的に作成したテストデータを対象に，R MASSパッケージのglm.nb()関数を用いて最尤法を行い，割合データの平滑化を行ってみます．"
  },
  {
    "objectID": "posts/2025/04/removal-of-observation-noise-from-proportion.html#平滑化の実例",
    "href": "posts/2025/04/removal-of-observation-noise-from-proportion.html#平滑化の実例",
    "title": "割合データ（割り算値）から観測ノイズを除去する",
    "section": "平滑化の実例",
    "text": "平滑化の実例\n\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\n\nテストデータの作成\nまず，サンプルサイズを10,000とし，割合の真値を\\(\\alpha = 5, \\beta = 50\\)のガンマ分布からランダムに生成します．\n\nset.seed(1234)\n\n# サンプルサイズ\nn &lt;- 1e4\n\n# 割合 (真値)\nalpha_true &lt;- 5\nbeta_true &lt;- 50\nrate_true &lt;- rgamma(n, shape = alpha_true, rate = beta_true)\n\n割合の真値rate_trueは以下のような分布を持ちます．\n\ntibble(rate_true = rate_true) |&gt; \n  ggplot(aes(rate_true)) +\n  geom_histogram(binwidth = 1e-2)\n\n\n\n\n\n\n\n\n次に，人口（分母）populationを1から100までの整数からランダムに選びます． さらに，割合の真値rate_trueに基づき，ポアソン分布からカウントデータ（分子）countを生成し，これらの結果をデータフレームdataにまとめます．\n\n# 人口 (分母)\nx &lt;- 1:100\npopulation &lt;- sample(x, n, replace = TRUE)\n\n# カウントデータ (分子)\ncount &lt;- rpois(n, rate_true * population)\n\ndata &lt;- tibble(\n  population = population,\n  rate_true = rate_true,\n  count = count\n)\nhead(data, n = 10)\n\n\n  \n\n\n\n\n\n少数問題の確認\nそれでは，テストデータを用いて，少数問題が発生しているかを確認してみましょう． まずは，割り算を用いた割合の推定値rate_crudeを計算し，populationをX軸・rate_crudeをY軸にとる散布図を描画します． すると，以下のように，populationが小さい場合に，rate_crudeが大きくなる傾向がみられることがわかります． もし，手元のデータでも同様の傾向がみられる場合，少数問題が発生している可能性があります．\n\ndata_crude &lt;- data |&gt; \n  mutate(\n    rate_crude = count / population\n  )\nhead(data_crude, n = 10)\n\n\n  \n\n\ndata_crude |&gt; \n  ggplot(aes(population, rate_crude)) +\n  geom_point()\n\n\n\n\n\n\n\n\n通常のデータでは割合の真値rate_trueはわかりませんが，ここではrate_trueとrate_crudeの関係も確認することができます． 以下のように，rate_trueをX軸・rate_crudeをY軸にとる散布図を描画すると，populationが小さい場合に，rate_crudeが45度線から大きく外れる傾向がみられることがわかります． 以上の結果より，テストデータで少数問題が発生していることが確認できました．\n\ndata_crude |&gt; \n  ggplot(aes(rate_true, rate_crude, color = population)) +\n  geom_point() +\n  geom_abline(color = \"red\") +\n  scale_color_viridis_c(option = \"turbo\") +\n  tune::coord_obs_pred()"
  },
  {
    "objectID": "posts/2025/04/removal-of-observation-noise-from-proportion.html#負の二項分布を用いた平滑化",
    "href": "posts/2025/04/removal-of-observation-noise-from-proportion.html#負の二項分布を用いた平滑化",
    "title": "割合データ（割り算値）から観測ノイズを除去する",
    "section": "負の二項分布を用いた平滑化",
    "text": "負の二項分布を用いた平滑化\nそれでは，負の二項分布を用いて平滑化を行ってみましょう． ここでは，Rで簡単に利用可能なMASSパッケージのglm.nb()関数を用いて，最尤法を行います． glm.nb()により，目的変数が負の二項分布に従うような一般化線形モデルを推定することができます．\n以下のような簡単なコードで負の二項分布のパラメータ推定ができます． ここでは，期待値がpopulationに比例するよう，log(population)をオフセット項に指定しています．\n\nmodel &lt;- MASS::glm.nb(\n  count ~ offset(log(population)),\n  data = data\n)\n\n次に，glm.nb()の出力値を格納したmodelからガンマ分布のパラメータを取得します． ガンマ分布の\\(\\alpha\\)は，model$thetaで取得できます． \\(\\alpha\\)の推定値alpha_estimatedは，真値のalpha_trueとおおよそ一致することがわかります．\n\nalpha_estimated &lt;- model$theta\n\nalpha_true\n\n[1] 5\n\nalpha_estimated\n\n[1] 5.014545\n\n\nまた，ガンマ分布の\\(\\beta\\)は，上に示した\\(\\mu\\)の関係式より以下で求められます． \\(\\mu\\)はサンプルサイズの数だけ出力されますが，\\(\\beta\\)の推定値beta_estimatedが全てほぼ同等の値をもち，さらに，真値のbeta_trueとおおよそ一致することがわかります．\n\\[\n\\beta = \\frac{a N}{\\mu}\n\\]\n\nmu &lt;- predict(model, type = \"response\")\nbeta_estimated &lt;- alpha_estimated * population / mu\n\nbeta_true\n\n[1] 50\n\nall(near(beta_estimated, beta_estimated[[1]]))\n\n[1] TRUE\n\nhead(beta_estimated)\n\n       1        2        3        4        5        6 \n50.07359 50.07359 50.07359 50.07359 50.07359 50.07359 \n\n\nそれでは，最後に求められたガンマ分布の\\(\\alpha\\)と\\(\\beta\\)を使って割合の平滑化を行ってみましょう． 以下に，平滑化に使用する式を再掲します． この式に従って平滑化した割合rate_smoothedを対象に，先ほどと同様のグラフを描画してみます． すると，populationが小さい場合であっても，割合が過大・過小になる傾向がみられないことがわかり，少数問題が解消されていることが確認できました．\n\\[\n\\frac{n + \\alpha}{N + \\beta}\n\\]\n\ndata_smoothed &lt;- data |&gt; \n  mutate(\n    rate_smoothed = (count + alpha_estimated) / (population + beta_estimated)\n  )\nhead(data_smoothed, n = 10)\n\n\n  \n\n\ndata_smoothed |&gt;\n  ggplot(aes(population, rate_smoothed)) +\n  geom_point()\n\n\n\n\n\n\n\ndata_smoothed |&gt; \n  ggplot(aes(rate_true, rate_smoothed, color = population)) +\n  geom_point() +\n  geom_abline(color = \"red\") +\n  scale_color_viridis_c(option = \"turbo\") +\n  tune::coord_obs_pred()"
  },
  {
    "objectID": "posts/2025/04/removal-of-observation-noise-from-proportion.html#おわりに",
    "href": "posts/2025/04/removal-of-observation-noise-from-proportion.html#おわりに",
    "title": "割合データ（割り算値）から観測ノイズを除去する",
    "section": "おわりに",
    "text": "おわりに\nこの記事では，割合データにおける少数問題への対処法として，経験ベイズ法を用いた平滑化手法について説明しました． 分母の人口等が小さい場合に割合データのばらつきが大きくなる現象は，実務でもしばしば問題になると思われます． そうした場合には，何らかのしきい値で分母の小さいデータを除外する方法も考えられますが， この記事で説明した方法を使うと全てのデータを活用できるという利点もあります．\nこの記事で用いたMASSパッケージのglm.nb()は，一般化線形モデル用の関数であるため，分母にあたるoffset(log(population))以外の項も説明変数として含むことが可能です． これにより，地域特性等によって\\(\\beta\\)が異なる状況を再現できる可能性があります．\nまた，汎用性の高いと思われるポアソン分布を対象としましたが，二項分布でも同様の枠組みが利用できます． 二項分布の場合，事前分布としてベータ分布を用いると，平滑化の式は以下のようになります． また，混合分布はベータ・二項分布となるため，既存のRパッケージ（例：bbmle）等を用いれば，比較的簡単にパラメータ推定が可能だと思われます．\n\\[\n\\frac{n + \\alpha}{N + \\alpha + \\beta}\n\\]"
  }
]