{
  "hash": "0e39a7670072fefb019fdb230167b5a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scraping dynamic sites with rvest (without Selenium)\"\nlang: en\ncategories: [rvest, R, Japanese]\ndate: \"2024-02-15\"\nformat:\n  html:\n    df-print: paged\nimage: scraping-dynamic-sites-with-rvest/html-view.png\n---\n\n\nNote: This article is translated from [my Japanese article](https://uchidamizuki.quarto.pub/blog/posts/2024/02/scraping-dynamic-sites-with-rvest.html).\n\n## Web scraping for dynamic sites in R\n\nrvest is a popular package for web scraping in R, bur it could not be used for dynamic sites where the contents changes with the operations on the browser.\n\nTherefore, for scraping dynamic sites in R, you needed to use other packages such as RSelenium with rvest and had the following issues.\n\n-   When using Selenium, it is troublesome to set up the environment (e.g., you need to download the driver in advance, etc.).\n\n-   We couldn't seamlessly apply rvest functions to HTML from other packages.\n\nIn [rvest 1.0.4](https://cran.r-project.org/web/packages/rvest/news/news.html), however, a new [`read_html_live()`](https://rvest.tidyverse.org/reference/read_html_live.html) function has been added to allow dynamic site scraping with rvest alone. By using `read_html_live()`, you can automate browser operations with methods such as `$click()` and `$type()`. Not only that, but you can seamlessly call rvest functions such as `html_elements()` and `html_attr()` with `read_html_live()`.\n\n`read_html_live()` uses [chromote](https://rstudio.github.io/chromote/) package for Google Chrome automation internally. Therefore to use the function, you need to install Google Chrome (browser) and chromote (R package) beforehand.\n\n## Let's use `read_html_live()`\n\nFrom here, let's use `read_html_live()` to perform the same process in [this RSelenium tutorial](https://joshuamccrain.com/tutorials/web_scraping_R_selenium.html).\n\nThis tutorial automates the process of collecting information from local TV stations in [this site](https://www.fcc.gov/media/engineering/dtvmaps). This process requires a search for U.S. zip codes.\n\nBy using `read_html_live()`, the code to access the site in RSelenium can be rewritten in rvest as follows.\n\n``` r\n# RSelenium\n# Source: https://joshuamccrain.com/tutorials/web_scraping_R_selenium.html\nlibrary(RSelenium)\n\nrD <- rsDriver(browser=\"firefox\", port=4545L, verbose=F)\nremDr <- rD[[\"client\"]]\nremDr$navigate(\"https://www.fcc.gov/media/engineering/dtvmaps\")\n```\n\n⏬\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# rvest\nlibrary(rvest)\nlibrary(tidyverse)\n\nhtml <- read_html_live(\"https://www.fcc.gov/media/engineering/dtvmaps\")\n```\n:::\n\n\nYou can use the `$view()` method to see the LiveHTML object. You can also select elements on the site by `Ctrl+Shift+C` and then right-click⏩`Copy`⏩`Copy selector` to copy the CSS selector, and use it as an argument to `$type()` or `$click()`.\n\n``` r\n# rvest\nhtml$view()\n```\n\n![](scraping-dynamic-sites-with-rvest/html-view.png)\n\nNext, enter the zip code in the center form and click the `Go!` button. Here, the code in RSelenium can be rewritten in rvest as follows.\n\n``` r\n# RSelenium\n# Source: https://joshuamccrain.com/tutorials/web_scraping_R_selenium.html\nzip <- \"30308\"\nremDr$findElement(using = \"id\", value = \"startpoint\")$sendKeysToElement(list(zip))\nremDr$findElements(\"id\", \"btnSub\")[[1]]$clickElement()\n```\n\n⏬\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# rvest\nzip <- \"30308\"\nhtml$type(\"#startpoint\", zip)\nhtml$click(\"#btnSub\")\n```\n:::\n\n\nFinally, let's check we got the same data as in the RSelenium tutorial above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# rvest\nhtml |> \n  html_elements(\"table.tbl_mapReception\") |> \n  insistently(chuck)(3) |> \n  html_table() |> \n  select(!c(1, IA)) |> \n  rename_with(str_to_lower) |> \n  rename(ch_num = `ch#`) |> \n  slice_tail(n = -1) |> \n  filter(callsign != \"\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"callsign\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"network\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ch_num\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"band\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"WXIA-TV\",\"2\":\"NBC\",\"3\":\"11\",\"4\":\"Hi-V\"},{\"1\":\"WSB-TV\",\"2\":\"ABC\",\"3\":\"2\",\"4\":\"UHF\"},{\"1\":\"WUVG-DT\",\"2\":\"UNIV\",\"3\":\"34\",\"4\":\"UHF\"},{\"1\":\"WHSG-TV\",\"2\":\"TRIN\",\"3\":\"63\",\"4\":\"UHF\"},{\"1\":\"WAGA-TV\",\"2\":\"FOX\",\"3\":\"5\",\"4\":\"UHF\"},{\"1\":\"WANF\",\"2\":\"CBS\",\"3\":\"46\",\"4\":\"UHF\"},{\"1\":\"WUPA\",\"2\":\"THE\",\"3\":\"69\",\"4\":\"UHF\"},{\"1\":\"WPCH-TV\",\"2\":\"IND\",\"3\":\"17\",\"4\":\"UHF\"},{\"1\":\"WATL\",\"2\":\"MY N\",\"3\":\"36\",\"4\":\"UHF\"},{\"1\":\"WIRE-CD\",\"2\":\"\",\"3\":\"\",\"4\":\"UHF\"},{\"1\":\"WABE-TV\",\"2\":\"PBS\",\"3\":\"30\",\"4\":\"UHF\"},{\"1\":\"WATC-DT\",\"2\":\"ETV\",\"3\":\"57\",\"4\":\"UHF\"},{\"1\":\"WYGA-CD\",\"2\":\"\",\"3\":\"\",\"4\":\"UHF\"},{\"1\":\"WGTV\",\"2\":\"PBS\",\"3\":\"8\",\"4\":\"Hi-V\"},{\"1\":\"WANN-CD\",\"2\":\"\",\"3\":\"\",\"4\":\"UHF\"},{\"1\":\"WPXA-TV\",\"2\":\"ION\",\"3\":\"14\",\"4\":\"UHF\"},{\"1\":\"WKTB-CD\",\"2\":\"\",\"3\":\"\",\"4\":\"UHF\"},{\"1\":\"WSKC-CD\",\"2\":\"\",\"3\":\"\",\"4\":\"UHF\"},{\"1\":\"WNGH-TV\",\"2\":\"PBS\",\"3\":\"18\",\"4\":\"Lo-V\"},{\"1\":\"WJSP-TV\",\"2\":\"PBS\",\"3\":\"28\",\"4\":\"Lo-V\"},{\"1\":\"WCIQ\",\"2\":\"PBS\",\"3\":\"7\",\"4\":\"Hi-V\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Summary\n\nAs above, by using `read_html_live()` that was added in rvest 1.0.4, we found that seamless scraping of static and dynamic sites is possible with rvest alone. In addition, it should be noted that rvest's code was simpler than RSelenium's.\n\nIn addition to rvest, a web scraping package called [selenider](https://ashbythorpe.github.io/selenider/) is also being developed for R. It is expected that the development of such packages will make web scraping in R even more convenient!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}